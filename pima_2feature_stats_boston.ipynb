{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "                \n",
    "         # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add                               3 with p-value 5.0811e-88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-c6da44f0c516>:24: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  new_pval = pd.Series(index=excluded)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([3], dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-641f379b3c22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstepwise_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'resulting features:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c6da44f0c516>\u001b[0m in \u001b[0;36mstepwise_selection\u001b[1;34m(X, y, initial_list, threshold_in, threshold_out, verbose)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m          \u001b[1;31m# backward step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mincluded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# use all coefs except intercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mpvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m         self._validate_read_indexer(\n\u001b[0m\u001b[0;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1640\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([3], dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "result = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target  \n",
       "0       15.3  396.90   4.98    24.0  \n",
       "1       17.8  396.90   9.14    21.6  \n",
       "2       17.8  392.83   4.03    34.7  \n",
       "3       18.7  394.63   2.94    33.4  \n",
       "4       18.7  396.90   5.33    36.2  \n",
       "..       ...     ...    ...     ...  \n",
       "501     21.0  391.99   9.67    22.4  \n",
       "502     21.0  396.90   9.08    20.6  \n",
       "503     21.0  396.90   5.64    23.9  \n",
       "504     21.0  393.45   6.48    22.0  \n",
       "505     21.0  396.90   7.88    11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "X['target'] = y\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats feature selection 외국인 2015년 코드\n",
    "+ https://planspace.org/20150423-forward_selection_with_statsmodels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x236571c5220>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model\n",
    "forward_selected(X, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.735</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   128.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 14 Oct 2020</td> <th>  Prob (F-statistic):</th> <td>5.54e-137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:54:58</td>     <th>  Log-Likelihood:    </th> <td> -1498.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3022.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   494</td>      <th>  BIC:               </th> <td>   3072.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   36.3411</td> <td>    5.067</td> <td>    7.171</td> <td> 0.000</td> <td>   26.385</td> <td>   46.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>     <td>   -0.5226</td> <td>    0.047</td> <td>  -11.019</td> <td> 0.000</td> <td>   -0.616</td> <td>   -0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>        <td>    3.8016</td> <td>    0.406</td> <td>    9.356</td> <td> 0.000</td> <td>    3.003</td> <td>    4.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th>   <td>   -0.9465</td> <td>    0.129</td> <td>   -7.334</td> <td> 0.000</td> <td>   -1.200</td> <td>   -0.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>       <td>   -1.4927</td> <td>    0.186</td> <td>   -8.037</td> <td> 0.000</td> <td>   -1.858</td> <td>   -1.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>       <td>  -17.3760</td> <td>    3.535</td> <td>   -4.915</td> <td> 0.000</td> <td>  -24.322</td> <td>  -10.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>      <td>    2.7187</td> <td>    0.854</td> <td>    3.183</td> <td> 0.002</td> <td>    1.040</td> <td>    4.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>         <td>    0.0093</td> <td>    0.003</td> <td>    3.475</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>        <td>    0.0458</td> <td>    0.014</td> <td>    3.390</td> <td> 0.001</td> <td>    0.019</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>      <td>   -0.1084</td> <td>    0.033</td> <td>   -3.307</td> <td> 0.001</td> <td>   -0.173</td> <td>   -0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>       <td>    0.2996</td> <td>    0.063</td> <td>    4.726</td> <td> 0.000</td> <td>    0.175</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>       <td>   -0.0118</td> <td>    0.003</td> <td>   -3.493</td> <td> 0.001</td> <td>   -0.018</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.430</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 787.785</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.523</td>  <th>  Prob(JB):          </th> <td>8.60e-172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.300</td>  <th>  Cond. No.          </th> <td>1.47e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.47e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.735\n",
       "Method:                 Least Squares   F-statistic:                     128.2\n",
       "Date:                Wed, 14 Oct 2020   Prob (F-statistic):          5.54e-137\n",
       "Time:                        00:54:58   Log-Likelihood:                -1498.9\n",
       "No. Observations:                 506   AIC:                             3022.\n",
       "Df Residuals:                     494   BIC:                             3072.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     36.3411      5.067      7.171      0.000      26.385      46.298\n",
       "LSTAT         -0.5226      0.047    -11.019      0.000      -0.616      -0.429\n",
       "RM             3.8016      0.406      9.356      0.000       3.003       4.600\n",
       "PTRATIO       -0.9465      0.129     -7.334      0.000      -1.200      -0.693\n",
       "DIS           -1.4927      0.186     -8.037      0.000      -1.858      -1.128\n",
       "NOX          -17.3760      3.535     -4.915      0.000     -24.322     -10.430\n",
       "CHAS           2.7187      0.854      3.183      0.002       1.040       4.397\n",
       "B              0.0093      0.003      3.475      0.001       0.004       0.015\n",
       "ZN             0.0458      0.014      3.390      0.001       0.019       0.072\n",
       "CRIM          -0.1084      0.033     -3.307      0.001      -0.173      -0.044\n",
       "RAD            0.2996      0.063      4.726      0.000       0.175       0.424\n",
       "TAX           -0.0118      0.003     -3.493      0.001      -0.018      -0.005\n",
       "==============================================================================\n",
       "Omnibus:                      178.430   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              787.785\n",
       "Skew:                           1.523   Prob(JB):                    8.60e-172\n",
       "Kurtosis:                       8.300   Cond. No.                     1.47e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.47e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = forward_selected(X, 'target')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 github 2020-04-19\n",
    "Best Subset Selection, Forward Stepwise, Backward Stepwise Classes in sk-learn style.  \n",
    "This package is compatible to sklearn. Examples on Pipeline and GridSearchCV are given.  \n",
    "https://github.com/xinhe97/StepwiseSelectionOLS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BackwardStepwiseOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "8\n",
      "[0, 1, 2, 3, 4, 5, 6, 8, 9]\n",
      "7\n",
      "[0, 1, 2, 3, 4, 5, 8, 9]\n",
      "6\n",
      "[0, 1, 2, 3, 4, 8, 9]\n",
      "5\n",
      "[0, 1, 2, 3, 4, 9]\n",
      "4\n",
      "[0, 1, 2, 3, 4]\n",
      "3\n",
      "[0, 1, 2, 3]\n",
      "2\n",
      "[0, 1, 2]\n",
      "1\n",
      "[0, 1]\n",
      "0\n",
      "[0]\n",
      "                                               model   rsq_adj         bic  \\\n",
      "0  <statsmodels.regression.linear_model.Regressio...  0.887225   33.274280   \n",
      "1  <statsmodels.regression.linear_model.Regressio...  0.887454   26.041482   \n",
      "2  <statsmodels.regression.linear_model.Regressio...  0.887638   19.011364   \n",
      "3  <statsmodels.regression.linear_model.Regressio...  0.887793   12.106134   \n",
      "4  <statsmodels.regression.linear_model.Regressio...  0.887932    5.271525   \n",
      "5  <statsmodels.regression.linear_model.Regressio...  0.887450    1.202154   \n",
      "6  <statsmodels.regression.linear_model.Regressio...  0.876483   41.478788   \n",
      "7  <statsmodels.regression.linear_model.Regressio...  0.858160  104.422655   \n",
      "8  <statsmodels.regression.linear_model.Regressio...  0.825279  202.454081   \n",
      "9  <statsmodels.regression.linear_model.Regressio...  0.739448  396.046314   \n",
      "\n",
      "        rsq                predictors_index  \n",
      "0  0.889480  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
      "1  0.889480     [0, 1, 2, 3, 4, 5, 6, 8, 9]  \n",
      "2  0.889435        [0, 1, 2, 3, 4, 5, 8, 9]  \n",
      "3  0.889364           [0, 1, 2, 3, 4, 8, 9]  \n",
      "4  0.889277              [0, 1, 2, 3, 4, 9]  \n",
      "5  0.888575                 [0, 1, 2, 3, 4]  \n",
      "6  0.877471                    [0, 1, 2, 3]  \n",
      "7  0.859011                       [0, 1, 2]  \n",
      "8  0.825978                          [0, 1]  \n",
      "9  0.739969                             [0]  \n",
      "0.27586697953903694\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "\n",
    "# my stepwise selection + sklearn style\n",
    "class BackwardStepwiseOLS(BaseEstimator):\n",
    "\n",
    "    def __init__(self, fK=3):\n",
    "        self.fK = fK                        # number of predictors\n",
    "\n",
    "    def myBic(self, n, mse, k):\n",
    "        if k<=0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return n*np.log(mse)+k*np.log(n)\n",
    "\n",
    "    ################### Criteria ###################\n",
    "    def processSubset(self, X,y,feature_index):\n",
    "        # Fit model on feature_set and calculate rsq_adj\n",
    "        regr = sm.OLS(y,X[:,feature_index]).fit()\n",
    "        rsq_adj = regr.rsquared_adj\n",
    "        bic = self.myBic(X.shape[0], regr.mse_resid, len(feature_index))\n",
    "        rsq = regr.rsquared\n",
    "        return {\"model\":regr, \"rsq_adj\":rsq_adj, \"bic\":bic, \"rsq\":rsq, \"predictors_index\":feature_index}\n",
    "\n",
    "    ################### Backward Stepwise ###################\n",
    "    def Backward(self,predictors_index,X,y):\n",
    "        \n",
    "        results = []\n",
    "        for p in predictors_index:\n",
    "            index_tmp = predictors_index.copy()\n",
    "            index_tmp.remove(p)\n",
    "            new_predictors_index = index_tmp\n",
    "            new_predictors_index.sort()\n",
    "            results.append(self.processSubset(X,y,new_predictors_index))\n",
    "            # Wrap everything up in a nice dataframe\n",
    "        models = pd.DataFrame(results)\n",
    "        # Choose the model with the highest rsq_adj\n",
    "        # best_model = models.loc[models['bic'].idxmin()]\n",
    "        best_model = models.loc[models['rsq'].idxmax()]\n",
    "        # Return the best model, along with model's other  information\n",
    "        return best_model\n",
    "\n",
    "    def BackwardK(self,X_est,y_est, fK):\n",
    "        models_bwd = pd.DataFrame(columns=[\"model\", \"rsq_adj\", \"bic\", \"rsq\", \"predictors_index\"])\n",
    "        predictors_index = list(range(X_est.shape[1]))\n",
    "\n",
    "        if X_est.shape[1]<=fK:\n",
    "            print(\"use all predictors\")\n",
    "            best_model_bwd = self.processSubset(X_est,y_est,predictors_index)[\"model\"]\n",
    "            best_predictors = predictors_index\n",
    "        else:\n",
    "            i = X_est.shape[1]\n",
    "            j = 0\n",
    "            models_bwd.loc[j] = self.processSubset(X_est,y_est,predictors_index)\n",
    "            i = i-1\n",
    "            print(i)\n",
    "            print(predictors_index)\n",
    "\n",
    "            while i >= fK:\n",
    "                j = j+1\n",
    "                models_bwd.loc[j] = self.Backward(predictors_index,X_est,y_est)\n",
    "                predictors_index = models_bwd.loc[j,'predictors_index']\n",
    "                i = i-1\n",
    "                print(i)\n",
    "                print(predictors_index)\n",
    "\n",
    "            print(models_bwd)\n",
    "            best_model_bwd = models_bwd.loc[models_bwd['bic'].idxmin(),'model']\n",
    "            # best_model_bwd = models_bwd.loc[models_bwd['rsq'].idxmax(),'model']\n",
    "            best_predictors = models_bwd.loc[models_bwd['bic'].idxmin(),'predictors_index']\n",
    "            # best_predictors = models_bwd.loc[models_bwd['rsq'].idxmax(),'predictors_index']\n",
    "        return best_model_bwd, best_predictors\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "        # hexin\n",
    "        self.best_model_bwd, self.best_predictors = self.BackwardK(X,y,self.fK)\n",
    "        # hexin\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        # `fit` should always return `self`\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = check_array(X, accept_sparse=True)\n",
    "\n",
    "        # hexin\n",
    "        y_pred = self.best_model_bwd.predict(X[:,self.best_predictors])\n",
    "        # hexin\n",
    "\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"fK\": self.fK}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        return r2_score(y_true, self.predict(X))\n",
    "        # return -mean_squared_error(y_true, self.predict(X))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### DGP ######\n",
    "    # Spare signals\n",
    "    N = 1000\n",
    "    P = 10 # Total number of inputs\n",
    "\n",
    "    N_true_inputs = 5\n",
    "    N_false_inputs = P - N_true_inputs\n",
    "    n_obs = N/2\n",
    "    n_pred = N/2\n",
    "    error_sd = 1\n",
    "\n",
    "    #True inputs have coefficient 1\n",
    "    beta = np.matrix(np.zeros((P,1)))\n",
    "    beta[:N_true_inputs,:] = 1\n",
    "\n",
    "    #Simulate the data\n",
    "    X = np.matrix(np.random.rand(N,P))\n",
    "    epsilon = np.matrix(error_sd*np.random.normal(0,size=(N,1)))\n",
    "    y = X*beta + epsilon\n",
    "\n",
    "    # Pack the data into a dataframe\n",
    "    DF = pd.concat([pd.DataFrame(X),pd.DataFrame(y)],axis=1)\n",
    "    new_names_true = ['x_true_'+str(i) for i in range(1,N_true_inputs+1)]\n",
    "    new_names_false = ['x_false_'+str(i) for i in range(1,N_false_inputs+1)]\n",
    "    names = new_names_true + new_names_false + ['y']\n",
    "    DF.columns = names\n",
    "\n",
    "    # Now we split the data into an estimation and prediction sample. # Randomly draw n_obs observations\n",
    "    train_index = random.sample(range(0,N),np.int(n_obs))\n",
    "    train_index.sort()\n",
    "    DF_estimation = DF.loc[train_index,:]\n",
    "    DF_prediction = DF.drop(index=train_index)\n",
    "\n",
    "    # ###### Algorithm ######\n",
    "    # bwd = BackwardStepwiseOLS(fK=10)\n",
    "    # bwd.fit(DF_estimation.drop('y',1), DF_estimation['y'])\n",
    "    # bwd.predict(DF_prediction.drop('y',1))\n",
    "    # print(bwd.score(DF_prediction.drop('y',1), DF_prediction['y']))\n",
    "\n",
    "    ###### Algorithm ######\n",
    "    bwd = BackwardStepwiseOLS(fK=1)\n",
    "    bwd.fit(DF_estimation.drop('y',1), DF_estimation['y'])\n",
    "    bwd.predict(DF_prediction.drop('y',1))\n",
    "    print(bwd.score(DF_prediction.drop('y',1), DF_prediction['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ForwardStepwiseOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "                                                model   rsq_adj         bic  \\\n",
      "1   <statsmodels.regression.linear_model.Regressio...  0.739448  396.046314   \n",
      "2   <statsmodels.regression.linear_model.Regressio...  0.825279  202.454081   \n",
      "3   <statsmodels.regression.linear_model.Regressio...  0.859588   99.365348   \n",
      "4   <statsmodels.regression.linear_model.Regressio...  0.876018   43.356333   \n",
      "5   <statsmodels.regression.linear_model.Regressio...  0.887450    1.202154   \n",
      "6   <statsmodels.regression.linear_model.Regressio...  0.887932    5.271525   \n",
      "7   <statsmodels.regression.linear_model.Regressio...  0.887793   12.106134   \n",
      "8   <statsmodels.regression.linear_model.Regressio...  0.887638   19.011364   \n",
      "9   <statsmodels.regression.linear_model.Regressio...  0.887454   26.041482   \n",
      "10  <statsmodels.regression.linear_model.Regressio...  0.887225   33.274280   \n",
      "\n",
      "         rsq                predictors_index  \n",
      "1   0.739969                             [0]  \n",
      "2   0.825978                          [0, 1]  \n",
      "3   0.860430                       [0, 1, 4]  \n",
      "4   0.877010                    [0, 1, 3, 4]  \n",
      "5   0.888575                 [0, 1, 2, 3, 4]  \n",
      "6   0.889277              [0, 1, 2, 3, 4, 9]  \n",
      "7   0.889364           [0, 1, 2, 3, 4, 8, 9]  \n",
      "8   0.889435        [0, 1, 2, 3, 4, 5, 8, 9]  \n",
      "9   0.889480     [0, 1, 2, 3, 4, 5, 6, 8, 9]  \n",
      "10  0.889480  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
      "0.25991611391461633\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "\n",
    "# my stepwise selection + sklearn style\n",
    "class ForwardStepwiseOLS(BaseEstimator):\n",
    "\n",
    "    def __init__(self, fK=3):\n",
    "        self.fK = fK                        # number of predictors\n",
    "\n",
    "    def myBic(self, n, mse, k):\n",
    "        if k<=0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return n*np.log(mse)+k*np.log(n)\n",
    "\n",
    "    ################### Criteria ###################\n",
    "    def processSubset(self, X,y,feature_index):\n",
    "        # Fit model on feature_set and calculate rsq_adj\n",
    "        regr = sm.OLS(y,X[:,feature_index]).fit()\n",
    "        rsq_adj = regr.rsquared_adj\n",
    "        bic = self.myBic(X.shape[0], regr.mse_resid, len(feature_index))\n",
    "        rsq = regr.rsquared\n",
    "        return {\"model\":regr, \"rsq_adj\":rsq_adj, \"bic\":bic, \"rsq\":rsq, \"predictors_index\":feature_index}\n",
    "\n",
    "    ################### Forward Stepwise ###################\n",
    "    def forward(self,predictors_index,X,y):\n",
    "        # Pull out predictors we still need to process\n",
    "        remaining_predictors_index = [p for p in range(X.shape[1])\n",
    "                                if p not in predictors_index]\n",
    "\n",
    "        results = []\n",
    "        for p in remaining_predictors_index:\n",
    "            new_predictors_index = predictors_index+[p]\n",
    "            new_predictors_index.sort()\n",
    "            results.append(self.processSubset(X,y,new_predictors_index))\n",
    "            # Wrap everything up in a nice dataframe\n",
    "        models = pd.DataFrame(results)\n",
    "        # Choose the model with the highest rsq_adj\n",
    "        # best_model = models.loc[models['bic'].idxmin()]\n",
    "        best_model = models.loc[models['rsq'].idxmax()]\n",
    "        # Return the best model, along with model's other  information\n",
    "        return best_model\n",
    "\n",
    "    def forwardK(self,X_est,y_est, fK):\n",
    "        models_fwd = pd.DataFrame(columns=[\"model\", \"rsq_adj\", \"bic\", \"rsq\", \"predictors_index\"])\n",
    "        predictors_index = []\n",
    "\n",
    "        M = min(fK,X_est.shape[1])\n",
    "\n",
    "        for i in range(1,M+1):\n",
    "            print(i)\n",
    "            models_fwd.loc[i] = self.forward(predictors_index,X_est,y_est)\n",
    "            predictors_index = models_fwd.loc[i,'predictors_index']\n",
    "\n",
    "        print(models_fwd)\n",
    "        # best_model_fwd = models_fwd.loc[models_fwd['bic'].idxmin(),'model']\n",
    "        best_model_fwd = models_fwd.loc[models_fwd['rsq'].idxmax(),'model']\n",
    "        # best_predictors = models_fwd.loc[models_fwd['bic'].idxmin(),'predictors_index']\n",
    "        best_predictors = models_fwd.loc[models_fwd['rsq'].idxmax(),'predictors_index']\n",
    "        return best_model_fwd, best_predictors\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "        # hexin\n",
    "        self.best_model_fwd, self.best_predictors = self.forwardK(X,y,self.fK)\n",
    "        # hexin\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        # `fit` should always return `self`\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = check_array(X, accept_sparse=True)\n",
    "\n",
    "        # hexin\n",
    "        y_pred = self.best_model_fwd.predict(X[:,self.best_predictors])\n",
    "        # hexin\n",
    "\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"fK\": self.fK}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        return r2_score(y_true, self.predict(X))\n",
    "        # return -mean_squared_error(y_true, self.predict(X))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### DGP ######\n",
    "    # Spare signals\n",
    "    N = 1000\n",
    "    P = 10 # Total number of inputs\n",
    "\n",
    "    N_true_inputs = 5\n",
    "    N_false_inputs = P - N_true_inputs\n",
    "    n_obs = N/2\n",
    "    n_pred = N/2\n",
    "    error_sd = 1\n",
    "\n",
    "    #True inputs have coefficient 1\n",
    "    beta = np.matrix(np.zeros((P,1)))\n",
    "    beta[:N_true_inputs,:] = 1\n",
    "\n",
    "    #Simulate the data\n",
    "    X = np.matrix(np.random.rand(N,P))\n",
    "    epsilon = np.matrix(error_sd*np.random.normal(0,size=(N,1)))\n",
    "    y = X*beta + epsilon\n",
    "\n",
    "    # Pack the data into a dataframe\n",
    "    DF = pd.concat([pd.DataFrame(X),pd.DataFrame(y)],axis=1)\n",
    "    new_names_true = ['x_true_'+str(i) for i in range(1,N_true_inputs+1)]\n",
    "    new_names_false = ['x_false_'+str(i) for i in range(1,N_false_inputs+1)]\n",
    "    names = new_names_true + new_names_false + ['y']\n",
    "    DF.columns = names\n",
    "\n",
    "    # Now we split the data into an estimation and prediction sample. # Randomly draw n_obs observations\n",
    "    train_index = random.sample(range(0,N),np.int(n_obs))\n",
    "    train_index.sort()\n",
    "    DF_estimation = DF.loc[train_index,:]\n",
    "    DF_prediction = DF.drop(index=train_index)\n",
    "\n",
    "    ###### Algorithm ######\n",
    "    fwd = ForwardStepwiseOLS(fK=10)\n",
    "    fwd.fit(DF_estimation.drop('y',1), DF_estimation['y'])\n",
    "    fwd.predict(DF_prediction.drop('y',1))\n",
    "    print(fwd.score(DF_prediction.drop('y',1), DF_prediction['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BestSubsetSelectionOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (0, 1, 2, 3, 4)\n",
      "0.27586697953903694\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "\n",
    "# %%pixie_debugger\n",
    "class BestSubsetSelectionOLS(BaseEstimator):\n",
    "    def __init__ (self, fK=3):\n",
    "        self.fK = fK              #number of predictors\n",
    "        \n",
    "    def myBic(self, n, mse, k):\n",
    "        if k<=0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return n*np.log(mse) + k*np.log(n)\n",
    "        \n",
    "    ############   Criteria   ##################################\n",
    "    def processSubset(self, X,y,feature_set):\n",
    "        regr = sm.OLS(y, X[list(feature_set)]).fit()\n",
    "\n",
    "        rsq_adj = regr.rsquared_adj\n",
    "\n",
    "        bic = self.myBic(X.shape[0], regr.mse_resid, len(feature_set))\n",
    "        rsq = regr.rsquared\n",
    "        return{\"model\": regr, \"rsq_adj\":rsq_adj, \"bic\":bic, \"rsq\":rsq, \"best_predictors\": feature_set}  \n",
    "    \n",
    " ############## bext subset selection######################\n",
    "    def getBest(self, X,y,fK):\n",
    "        results = [] #fill results in a list\n",
    "        #get X variable's all combinations(X.columns,k):\n",
    "        X = pd.DataFrame(X)\n",
    "        for combo in itertools.combinations(X.columns, fK):\n",
    "            results.append(self.processSubset(X,y,combo))\n",
    "        # Wrap everything up in a nice dataframe\n",
    "        models = pd.DataFrame(results)\n",
    "        # Choose the model with the highest rsq_adj\n",
    "        best_model =  models.loc[models[\"rsq\"].idxmax(), 'model']\n",
    "        #Return best_model\n",
    "        best_predictors =  models.loc[models[\"rsq\"].idxmax(), 'best_predictors']\n",
    "        print(fK, best_predictors)\n",
    "        return best_model, best_predictors #later add feature_set\n",
    "    \n",
    "# forwardK should be not applicable to best selection since best only choose a specific fK\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, accept_sparse = True)\n",
    "        \n",
    "        self.best_model, self.best_predictors = self.getBest(X,y, self.fK) #later add return predictors\n",
    "        \n",
    "        # hexin\n",
    "        self.is_fitted_ = True\n",
    "        # print(self.best_predictors)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X, accept_sparse = True)\n",
    "        \n",
    "        # hexin\n",
    "        y_pred = self.best_model.predict(X[:, list(self.best_predictors)]) #later add returning the feature_set\n",
    "        # hexin\n",
    "        \n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        return y_pred\n",
    "    \n",
    "    def get_params(self, deep = True):\n",
    "        return {\"fK\": self.fK}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y_true):\n",
    "        return r2_score(y_true, self.predict(X))\n",
    " \n",
    "if __name__ == '__main__': \n",
    "    N = 1000\n",
    "    P = 10 # Total number of inputs\n",
    "\n",
    "    N_true_inputs = 5\n",
    "    N_false_inputs = P - N_true_inputs\n",
    "    n_obs = N/2\n",
    "    n_pred = N/2\n",
    "    error_sd = 1\n",
    "\n",
    "    # True inputs have coefficient 1\n",
    "    beta = np.matrix(np.zeros((P,1)))\n",
    "    beta[:N_true_inputs, :] = 1\n",
    "\n",
    "    # stimulate the data\n",
    "    X = np.matrix(np.random.rand(N,P))\n",
    "    epsilon = np.matrix(error_sd*np.random.normal(0, size= (N,1)))\n",
    "    y = X*beta + epsilon\n",
    "\n",
    "    # Pack the data into a dataframe\n",
    "    DF = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis = 1)\n",
    "    new_names_true = ['x_true_'+str(i) for i in range(1, N_true_inputs + 1)]\n",
    "    new_names_false = ['x_true_' +str(i) for i in range (1, N_false_inputs +1)]\n",
    "    names = new_names_true + new_names_false + ['y']\n",
    "    DF.columns = names\n",
    "\n",
    "    # Now we split the data into an estimation and prediction sample. # Randomly draw n_obs obervations\n",
    "    train_index = random.sample(range(0,N), np.int(n_obs))\n",
    "    train_index.sort()\n",
    "    DF_estimation = DF.loc[train_index, :]\n",
    "    DF_prediction = DF.drop(index = train_index) \n",
    "\n",
    "    ####### Algorithm ####################\n",
    "    bfit = BestSubsetSelectionOLS(fK=5)\n",
    "    bfit.fit(DF_estimation.drop('y', 1), DF_estimation['y'])\n",
    "    bfit.predict(DF_prediction.drop('y',1))\n",
    "    print(bfit.score(DF_prediction.drop('y',1), DF_prediction['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 (0, 1, 2, 3, 4, 5, 6, 7, 9)\n",
      "{'bst__fK': 9}\n",
      "{'mean_fit_time': array([0.0293222 , 0.08916249, 0.19556732, 0.3231153 , 0.40578017,\n",
      "       0.34366755, 0.19995208, 0.08123059, 0.02187014, 0.00624886]), 'std_fit_time': array([0.00173853, 0.00499126, 0.01116728, 0.00813042, 0.01695689,\n",
      "       0.01711219, 0.00624857, 0.00624897, 0.00765288, 0.00765325]), 'mean_score_time': array([0.00259314, 0.00259356, 0.00059853, 0.        , 0.00312395,\n",
      "       0.        , 0.        , 0.00312419, 0.00624862, 0.00312443]), 'std_score_time': array([0.00048899, 0.00048827, 0.00119705, 0.        , 0.0062479 ,\n",
      "       0.        , 0.        , 0.00624838, 0.00765296, 0.00624886]), 'param_bst__fK': masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'bst__fK': 1}, {'bst__fK': 2}, {'bst__fK': 3}, {'bst__fK': 4}, {'bst__fK': 5}, {'bst__fK': 6}, {'bst__fK': 7}, {'bst__fK': 8}, {'bst__fK': 9}, {'bst__fK': 10}], 'split0_test_score': array([-1.69591129, -0.58807858, -0.23886298, -0.09646673, -0.06255825,\n",
      "       -0.03501665, -0.05795305, -0.04221971, -0.05522714, -0.05610596]), 'split1_test_score': array([-1.46056429, -0.49893961, -0.12386328, -0.06658852, -0.04520363,\n",
      "       -0.05888187, -0.03716572, -0.04026984, -0.01523224, -0.01706446]), 'split2_test_score': array([-2.53901003, -1.08112094, -0.66208732, -0.43426219, -0.2335625 ,\n",
      "       -0.24144199, -0.21206871, -0.15931425, -0.15361509, -0.15501886]), 'split3_test_score': array([-1.73487041e+00, -9.13076617e-01, -2.93520297e-01, -5.10779261e-02,\n",
      "        2.80329567e-02, -1.66885198e-02, -1.10835452e-03,  2.50210297e-02,\n",
      "        2.57989741e-02,  1.71414094e-02]), 'split4_test_score': array([-0.80610655, -0.3314574 , -0.05032567,  0.07251657,  0.16649541,\n",
      "        0.16974217,  0.17785562,  0.18618926,  0.17753493,  0.16911674]), 'mean_test_score': array([-1.64729252, -0.68253463, -0.27373191, -0.11517576, -0.0293592 ,\n",
      "       -0.03645737, -0.02608805, -0.0061187 , -0.00414811, -0.00838623]), 'std_test_score': array([0.55635511, 0.27491997, 0.21203693, 0.16962538, 0.13015767,\n",
      "       0.13071548, 0.12485504, 0.11304025, 0.10860742, 0.10584193]), 'rank_test_score': array([10,  9,  8,  7,  5,  6,  4,  2,  1,  3])}\n"
     ]
    }
   ],
   "source": [
    "# from ForwardStepwiseOLS import *\n",
    "# from BackwardStepwiseOLS import *\n",
    "# from BestSubsetSelectionOLS import *\n",
    "\n",
    "\n",
    "# DGP\n",
    "# Spare signals\n",
    "N = 1000\n",
    "P = 10 # Total number of inputs\n",
    "\n",
    "N_true_inputs = 5\n",
    "N_false_inputs = P - N_true_inputs\n",
    "n_obs = N/2\n",
    "n_pred = N/2\n",
    "error_sd = 1\n",
    "\n",
    "#True inputs have coefficient 1\n",
    "beta = np.matrix(np.zeros((P,1)))\n",
    "beta[:N_true_inputs,:] = 1\n",
    "\n",
    "#Simulate the data\n",
    "X = np.matrix(np.random.rand(N,P))\n",
    "epsilon = np.matrix(error_sd*np.random.normal(0,size=(N,1)))\n",
    "y = X*beta + epsilon\n",
    "y_label = y>1\n",
    "\n",
    "# Pack the data into a dataframe\n",
    "#DF = pd.concat([pd.DataFrame(X),pd.DataFrame(y)],axis=1)\n",
    "DF = pd.concat([pd.DataFrame(X),pd.DataFrame(y_label)],axis=1)\n",
    "new_names_true = ['x_true_'+str(i) for i in range(1,N_true_inputs+1)]\n",
    "new_names_false = ['x_false_'+str(i) for i in range(1,N_false_inputs+1)]\n",
    "names = new_names_true + new_names_false + ['y']\n",
    "DF.columns = names\n",
    "\n",
    "# Now we split the data into an estimation and prediction sample. # Randomly draw n_obs observations\n",
    "train_index = random.sample(range(0,N),np.int(n_obs))\n",
    "train_index.sort()\n",
    "DF_estimation = DF.loc[train_index,:]\n",
    "DF_prediction = DF.drop(index=train_index)\n",
    "\n",
    "###### GridSearchCV ######\n",
    "\n",
    "############\n",
    "# # forward\n",
    "# param_grid_pipe_fwd = {\n",
    "#     'fwd__fK': [1,2,3,4,5,6,7,8,9,10]\n",
    "# }\n",
    "# pipe = Pipeline(steps=[('fwd', ForwardStepwiseOLS())])\n",
    "# search = GridSearchCV(estimator=pipe, cv=5, param_grid=param_grid_pipe_fwd, n_jobs=-1)\n",
    "# search.fit(DF_estimation.drop('y',1), DF_estimation['y'])\n",
    "\n",
    "# print(search.best_params_)\n",
    "# print(search.cv_results_)\n",
    "\n",
    "############\n",
    "# # backward\n",
    "# # the hyperparameter fK: is the least number of features included\n",
    "# param_grid_pipe_bwd = {\n",
    "#     'bwd__fK': [1,2,3,4,5,6,7,8,9,10]\n",
    "# }\n",
    "# pipe = Pipeline(steps=[('bwd', BackwardStepwiseOLS())])\n",
    "# search = GridSearchCV(estimator=pipe, cv=5, param_grid=param_grid_pipe_bwd, n_jobs=-1)\n",
    "# search.fit(DF_estimation.drop('y',1), DF_estimation['y'])\n",
    "\n",
    "# print(search.best_params_)\n",
    "# print(search.cv_results_)\n",
    "\n",
    "###########\n",
    "# best subset\n",
    "# the hyperparameter fK: is the number of features included\n",
    "param_grid_pipe_bst = {\n",
    "    'bst__fK': [1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "pipe = Pipeline(steps=[('bst', BestSubsetSelectionOLS())])\n",
    "search = GridSearchCV(estimator=pipe, cv=5, param_grid=param_grid_pipe_bst, n_jobs=-1)\n",
    "search.fit(DF_estimation.drop('y',1), DF_estimation['y'])\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
