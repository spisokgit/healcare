{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version : 0.23.2\n",
      "python version : 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sys\n",
    "print(f'sklearn version : {sklearn.__version__}')\n",
    "print(f'python version : {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "from IPython.display import HTML, display # jupyter 사진넣기\n",
    "import time                      # excution time 계산\n",
    "from datetime import datetime   # system time \n",
    "import json                     # json save\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "import sklearn\n",
    "from sklearn import tree                  # 결정트리 모형  # Classifier tree\n",
    "from sklearn.inspection import permutation_importance  # feature 중요도\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer, PolynomialFeatures\n",
    "from sklearn.metrics import (accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix,\n",
    "                             precision_recall_curve,roc_curve,mean_squared_error,mean_absolute_error,r2_score)\n",
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier,LinearRegression,SGDRegressor,Ridge,Lasso,ElasticNet\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, SVR, LinearSVC,LinearSVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeRegressor, plot_tree \n",
    "from sklearn.ensemble import (VotingClassifier, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier,\n",
    "                            ExtraTreesClassifier,GradientBoostingClassifier,RandomForestRegressor,GradientBoostingRegressor)\n",
    "from xgboost import XGBClassifier, XGBRegressor     # 사이킷런 wrapper\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor   # 사이킷런 wrapper\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from ngboost import NGBClassifier, NGBRegressor\n",
    "\n",
    "\n",
    "import pickle                         # 모델 저장\n",
    "import matplotlib as mpl              # 한글깨짐\n",
    "import matplotlib.font_manager as fm  # 한글깨짐\n",
    "import matplotlib.font_manager        # 한글깨짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linux \n",
    "font_dirs = ['/home/sch/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf', ]\n",
    "font_files = fm.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = fm.createFontList(font_files)\n",
    "fm.fontManager.ttflist.extend(font_list)\n",
    "[f.name for f in matplotlib.font_manager.fontManager.ttflist if 'Nanum' in f.name][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 대한 성능평가지표개선을 위한 threshold 선택관련 plot\n",
    "def precision_recall_curve_plot(y_test, pos_proba, model_name=None,time_name=None ): # pred_proba positive값\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출.\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba)\n",
    "\n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    \n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary],linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n",
    "\n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n",
    "\n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.title(f\"{data_name} {model_name} Precision and Recall Trade off Curve \")\n",
    "    plt.xlabel('Threshold value)')\n",
    "    plt.ylabel('Precision and Recall value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f'./img/{data_name}_{model_name}_{time_name}_PR.png', bbox_inches='tight')\n",
    "#         plt.close(fig)\n",
    "    plt.show()\n",
    "\n",
    "def roc_curve_plot(y_test, pos_proba, model_name=None,time_name=None): # pred_proba positive값\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    \n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    # 임곗값에 따른 FPR, TPR 값을 반환 받음.\n",
    "    fprs, tprs, thresholds = roc_curve(y_test, pos_proba)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    # ROC Curve를 plot 곡선으로 그림.\n",
    "    plt.plot(fprs, tprs, label='ROC')\n",
    "    # 가운데 대각선 직선을 그림.\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "    # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f'{data_name} {model_name} Model ROC Curve')\n",
    "    plt.xlabel('FPR( 1 - Sensitivity )')\n",
    "    plt.ylabel('TPR( Recall )')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./img/{data_name}_{model_name}_{time_name}_ROC.png', bbox_inches='tight')\n",
    "#         plt.close(fig)\n",
    "    plt.show()\n",
    "\n",
    "# 선형기반 회귀계수 coef_ (lr, Ridge, Lasso, ) # gridsearchCV 에서는 없음\n",
    "def Plot_coef_(model, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    # 리눅스\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    df = pd.Series(model.coef_[0], index=indexs) \n",
    "    df = df.sort_values(ascending=False) # [:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df, y=df.index)\n",
    "    plt.title(f'{data_name} {model_name} coef_ ')\n",
    "#     plt.xlabel(f'{model_params}')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_{model_name}_{time_name}_coef_.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show()  \n",
    "    \n",
    "# 회귀트리(dt, rf, xgb, lgbm ) 이용시 feature_importances_  # gridsearchCV 에서는 없음\n",
    "def Plot_feature_importance(model, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    # 리눅스\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    df = pd.Series(model.feature_importances_, index=indexs) \n",
    "    df = df.sort_values(ascending=False) # [:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df, y=df.index)\n",
    "    plt.title(f'{data_name} {model_name} feature_importance')\n",
    "#     plt.xlabel(f'{model_params}')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_{model_name}_{time_name}_FI.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show()  \n",
    "    \n",
    "    # permutation_importance : sklearn 0.23.2이상\n",
    "def Plot_permutation_importance(importance, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "\n",
    "    df = pd.Series(importance, index=indexs)\n",
    "    df_20 = df.sort_values(ascending=False)#[:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df_20, y=df_20.index)\n",
    "    plt.title(f'{data_name} {model_name} permutation_importance')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_{model_name}_{time_name}_PI.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show() \n",
    "        \n",
    " # 결정트리 모형도 sklearn 0.23.1   (lr, rf 사용못함, CART 가능) : 수행시간이 많이 걸림\n",
    "def Plot_tree_(model,model_name,time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    tree.plot_tree(model, filled=True)\n",
    "    plt.title(f'{data_name} {model_name} tree')\n",
    "    plt.savefig(f\"./imgTree/{data_name}_{model_name}_{time_name}_tree.png\", bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습곡선\n",
    "def Plot_learning_curve(model, model_name, time_name,  X, y, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "            model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'neg_mean_squared_error', return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title(f'{data_name} {model_name} learing curve')\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training error\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation error\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training \")\n",
    "    plt.ylabel(\"neg_mean_squared_error\")\n",
    "    plt.savefig(f\"./imgLearnCurve/{data_name}_{model_name}_{time_name}_LC.png\", bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results_데이터로 GridSearchCV 곡선\n",
    "def plot_grid_curve(model, grid_params, model_name,time_name):\n",
    "    df = pd.DataFrame(model.cv_results_)\n",
    "    results = ['mean_test_score','mean_train_score'] \n",
    "    fig, axes = plt.subplots(1, len(grid_params), figsize = (3*len(grid_params), 4) )  # sharey='row'\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=10)\n",
    "    for idx, (param_name, param_range) in enumerate(grid_params.items()):\n",
    "        grouped_df = df.groupby(f'param_{param_name}')[results].agg({'mean_train_score': 'mean', 'mean_test_score': 'mean'})\n",
    "        previous_group = df.groupby(f'param_{param_name}')[results]\n",
    "        axes[idx].set_xlabel(param_name, fontsize=10)\n",
    "#         axes[idx].set_ylim(0.0, 1.1)\n",
    "        lw = 2 # 선의 굵기\n",
    "        axes[idx].plot(param_range, grouped_df['mean_train_score'], label=\"Training score\", color=\"darkorange\", lw=lw)\n",
    "        axes[idx].plot(param_range, grouped_df['mean_test_score'], label=\"Cross-validation score\",  color=\"navy\", lw=lw)\n",
    "#     handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(f'{data_name} {model_name} Validation curves', fontsize=20)\n",
    "    plt.legend(loc = \"best\")\n",
    "    fig.savefig(f\"./imgGridCurve/{data_name}_{model_name}_{time_name}_GC.png\", bbox_inches='tight')\n",
    "    fig.subplots_adjust(bottom=0.25, top=0.85)  \n",
    "#     plt.close(fig)\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  metric 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clf\n",
    "# 다중분류에서는 confusion_matrix, accuracy, roc_auc 만 나온다\n",
    "def get_clf_eval_poly(y_test, pred, pos_proba=None):\n",
    "    metrics_dict={}\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    metrics_dict['confusion_matrix'] = confusion.tolist() # confusion nd.array  --> list\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    roc_auc = roc_auc_score(y_test, pos_proba) \n",
    "    metrics_dict['auc_score'] = auc_score\n",
    "    print(f'오차 행렬 :\\n {confusion}')\n",
    "    print(f\" 정확도: {accuracy:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    return metrics_dict\n",
    "\n",
    "# 다중분류에서는 confusion_matrix, accuracy만 나온다\n",
    "# 이진분류에서 성능평가지표 종합 함수\n",
    "def get_clf_eval(y_test=None, pred=None, pos_proba=None): # 권철민 저 책자에 누락 pred_proba\n",
    "    metrics_dict={}\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    metrics_dict['confusion_matrix'] = confusion.tolist()\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    precision = precision_score(y_test, pred)  # average binary # multi [None, 'micro', 'macro', 'weighted'].\n",
    "    metrics_dict['precision'] = precision\n",
    "    recall = recall_score(y_test, pred )     \n",
    "    metrics_dict['recall'] = recall\n",
    "    f1 = f1_score(y_test, pred )\n",
    "    metrics_dict['f1'] = f1\n",
    "    roc_auc = roc_auc_score(y_test, pos_proba) # 권철민 저 책자에 오기 pred --> pred_proba  # average='macro'\n",
    "    metrics_dict['roc_auc'] = roc_auc\n",
    "    # print(f'오차 행렬 :\\n {confusion}')\n",
    "    # print(f\" 정확도: {accuracy:.4f}, 정밀도: {precision:.4f}, 재현율: {recall:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    print(f\" 정확도: {accuracy:.4f}, 정밀도: {precision:.4f}, 재현율: {recall:.4f}, f1_score: {f1:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    return metrics_dict\n",
    "\n",
    "## reg \n",
    "# log 값 변환 시 NaN등의 이슈로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산\n",
    "def rmsle(y_test, pred):\n",
    "    log_y = np.log1p(y_test)\n",
    "    log_pred = np.log1p(pred)\n",
    "    squared_error = (log_y - log_pred) ** 2\n",
    "    rmsle = np.sqrt(np.mean(squared_error))\n",
    "    return rmsle\n",
    "\n",
    "# 사이킷런의 mean_square_error() 를 이용하여 np.sqrt로 RMSE 계산\n",
    "def rmse(y_test,pred):\n",
    "    return np.sqrt(mean_squared_error(y_test,pred))\n",
    "\n",
    "# MAE, MSE, RMSE, RMSLE 를 모두 계산 \n",
    "def get_reg_eval(y_test, pred):\n",
    "    metrics_dict={}\n",
    "    mae_val = mean_absolute_error(y_test,pred)\n",
    "    metrics_dict['MAE'] = mae_val\n",
    "    mse_val = mean_squared_error(y_test,pred)\n",
    "    metrics_dict['MSE'] = mse_val\n",
    "    rmse_val = rmse(y_test,pred)\n",
    "    metrics_dict['RMSE'] = rmse_val\n",
    "    rmsle_val = rmsle(y_test,pred)\n",
    "    metrics_dict['RMSLE'] = rmsle_val    \n",
    "    r2 = r2_score(y_test,pred)\n",
    "    metrics_dict['R2'] = r2   \n",
    "    # adjusted_r2 = 1-(1-r2_score(y_test, pred))*((len(X_test)-1)/(len(X_test)-len(X_test[0])-1)))\n",
    "    print(f'MAE: {mae_val:.3F}, MSE: {mse_val:.3F}, RMSE: {rmse_val:.3F}, RMSLE: {rmsle_val:.3f}, R2: {r2:.3f}\\n')\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. model train, metric 호출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 metricPlot  ## 2 metric함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricPlot(model, model_name, time_name, start_model_time, X_train,y_train,X_test, y_test):\n",
    "    linear_model=['LogisticRegression','SGDClassifier','LinearSVC','LinearRegression','LinearSVR','SGDRegressor','Ridge','Lasso','ElasticNet']\n",
    "    pred = model.predict(X_test)\n",
    "    if(est ==\"clf\"):\n",
    "        if model_name in ['LinearSVC']:\n",
    "            pred_proba = model._predict_proba_lr(X_test) \n",
    "        else:\n",
    "            pred_proba = model.predict_proba(X_test) \n",
    "    if yLog : #is_expm1 <= targetlog변환시 복원\n",
    "        y_test = np.expm1(y_test)\n",
    "        pred = np.expm1(pred)  \n",
    "    if (est == 'clf'):\n",
    "        metrics_dict = get_clf_eval(y_test, pred, pred_proba[:,1])\n",
    "    elif (est == 'reg'):\n",
    "        metrics_dict = get_reg_eval(y_test, pred)\n",
    "    spend_metric_time=(time.time() - start_model_time)\n",
    "    print(f'{model_name} metric 수행시간 : {int(spend_metric_time//60)}분 {spend_metric_time%60:.2f}초\\n')\n",
    "    if (plot == 1) :\n",
    "        Plot_learning_curve(model,model_name,time_name,X_train,y_train,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5))\n",
    "        if (est ==\"clf\"):\n",
    "            precision_recall_curve_plot(y_test, pred_proba[:,1], model_name, time_name )\n",
    "            roc_curve_plot(y_test, pred_proba[:,1], model_name, time_name)\n",
    "        if model_name in linear_model:\n",
    "            Plot_coef_(model, X_train.columns, model_name, time_name) # 회귀계수 \n",
    "        else : \n",
    "# [KNeighborsClassifier,DecisionTreeClassifier,RandomForestClassifier,GradientBoostingClassifier,XGBClassifier,LGBMClassifier]:\n",
    "            Plot_feature_importance(model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "        # permutation_importance\n",
    "        if (est == 'clf'):\n",
    "            results = permutation_importance(model, X_test, y_test, scoring='f1')\n",
    "        elif (est == 'reg'):\n",
    "            results = permutation_importance(model, X_test, y_test, scoring='r2')\n",
    "        importance = results.importances_mean\n",
    "        Plot_permutation_importance(importance, X_test.columns, model_name, time_name)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_params()관련 LGBM기준 (XGB유사) # sklearn version별 api 다름\n",
    "# max_depth log2(데이터수) # bostonHouse 506=>8, # bike 10886=>13 # AmesHouse 1460=>10 # project 797만=>22\n",
    "# num_leaves  : 2^(max_depth) 작아야 함  최대 리프노드 갯수 = max_leaf_nodes(dt, rf)\n",
    "# min_child_samples : Leaf node가 되기 위한 최소한 데이터 개체수  = min_samples_leaf  # min_samples_split (dt,rf 분리 최소 데이터)\n",
    "# min_child_weight = min_child_leaf (dt, rf) *gbm은 없음    \n",
    "# subsample (gbm,xgb,lgbm) 데이터 샘플링 비율 *lgbm은 subsample_for_bin 20000, subsample_freq 0\n",
    "# colsample_bytree = max_features (dt, rf, gbm) 개별 트리를 학습할 때마다 무작위로 선택하는 feature 비율을 제어 (feature 많을 경우) * \n",
    "    # XGB : colsample_bylevel, colsample_bynode, colsample_bytree\n",
    "# reg_alpha : L1 규제(제거) # reg_lambda : L2 규제(최소화) \n",
    "# 영혼까지 learning_rate 내리고,  n_estimators(rf,gbm,xgb,lgbm) 올리고\n",
    "# stats.uniform(loc, scale) # subsample[0,1]  loc + scale =1 범위를 넘어가면 안됨\n",
    "def get_params(model_name):  # param_grid 2개항목 이상일때만 plot_grid_curve 가능   # l1_ratio  0 ~ 1 사이값\n",
    "    if (grid == 2) : # RandomSearch    속도느링 LogisticRegression  GradientBoostingClassifier\n",
    "        # clf\n",
    "        if model_name == 'LogisticRegression': # 속도느림 #\"solver\":['lbfgs','auto']\n",
    "#             param_grid = { 'C': stats.loguniform(1e-1, 1e2),'l1_ratio': stats.uniform(0, 1), \"max_iter\":stats.randint(100,500) }\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e2), \"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e2),\"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate':'optimal','penalty':'l2','loss':'hinge','eta0':[0,1],\n",
    "            param_grid = { 'alpha':stats.loguniform(1e-4,1e2),'l1_ratio':stats.uniform(0,1),'average':[True,False],\n",
    "                            'epsilon':stats.loguniform(1e-1, 1e1),'max_iter':stats.randint(100,500)}\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            param_grid = {\"max_depth\": stats.randint(5, 50), \"max_features\": stats.randint(2, 24),\n",
    "                \"min_samples_split\":stats.randint(10,100),\"min_samples_leaf\":stats.randint(10,100),\"criterion\":[\"gini\",\"entropy\"]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            param_grid = {\"max_depth\":stats.randint(5, 50),\"max_features\":stats.randint(2, 24),\"min_samples_split\":stats.randint(10,100),\n",
    "                          \"min_samples_leaf\":stats.randint(10,100),\"bootstrap\":[True,False],\"criterion\":[\"gini\",\"entropy\"]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림#n_estimators=100 learning_rate:0.1 subsample:1 ccp_alpha:0 n_features\n",
    "            param_grid={'max_depth':stats.randint(5,50),'min_samples_leaf':stats.randint(10,500),'max_features':stats.randint(5,24)}\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            param_grid={'n_estimators':stats.randint(200,1000),'learning_rate':stats.uniform(0.01,0.6),'subsample':stats.uniform(0.3,0.7),\n",
    "            \"max_depth\":stats.randint(5,50),'colsample_bytree':stats.uniform(0.3,0.7),'min_child_weight':stats.loguniform(1e-3, 1e3)}\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            param_grid={'n_estimators': stats.randint(200, 1000),'learning_rate': stats.uniform(0.01,0.6),\n",
    "                        'min_child_samples':stats.randint(10,500),'colsample_bytree':stats.uniform(loc=0.4,scale=0.6),\n",
    "                        'num_leaves': stats.randint(32,1000),'subsample':stats.uniform(loc=0.2, scale=0.8),\n",
    "        'min_child_weight': stats.loguniform(1e-3,1e3),'reg_alpha':stats.loguniform(1e-3,1e2),'reg_lambda':stats.loguniform(1e-3,1e3)}\n",
    "\n",
    "        # reg\n",
    "        elif model_name == 'LinearRegression': # normalize True => L2 norm 1\n",
    "            param_grid  = {'copy_X': [True, False], 'fit_intercept': [True,False], 'normalize':[True,False] }\n",
    "        elif model_name == 'LinearSVR': \n",
    "            param_grid = {'C':stats.loguniform(1e-1,1e2),'intercept_scaling':loguniform(1e-1,1e0),\"max_iter\":stats.randint(100,500)}\n",
    "        elif model_name == 'SGDRegressor':\n",
    "            param_grid = {'alpha':stats.loguniform(1e-4,1e2),'l1_ratio':stats.uniform(0,1),'average':[True,False],\n",
    "                            'epsilon':stats.loguniform(1e-1, 1e1),'max_iter':stats.randint(100,500)}\n",
    "        elif model_name == 'Ridge': \n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e2),'fit_intercept': [True,False] } \n",
    "        elif model_name == 'Lasso': \n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e2),'fit_intercept': [True,False] } \n",
    "        elif model_name == 'ElasticNet': # if L1 a, if L2 b  => alpha = a+b, l1_ratio = a/(a+b)  # if l1_ratio=1 => a=1, b=0\n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e2),'l1_ratio': stats.uniform(0,1)} \n",
    "        elif model_name == 'DecisionTreeRegressor':\n",
    "            param_grid = {\"max_depth\": stats.randint(5, 50), \"max_features\": stats.randint(2, 24),\n",
    "              \"min_samples_split\": stats.randint(10, 100), \"min_samples_leaf\": stats.randint(10, 100)}\n",
    "        elif model_name == 'RandomForestRegressor': # cpu 병렬지원(n_jobs=-1, 100% 가동)\n",
    "            param_grid = {\"max_depth\":  stats.randint(5, 50),\"max_features\":stats.randint(2, 24),\"min_samples_split\":stats.randint(10,100),\n",
    "                          \"min_samples_leaf\":stats.randint(10,100)}\n",
    "        elif model_name == 'GradientBoostingRegressor': # param많으면 학습시간이 많이 걸림(순차 시행, cpu 1개만 수행)\n",
    "            param_grid={ 'max_depth' : stats.randint(5, 50),'min_samples_leaf': stats.randint(10,500),'max_features':  stats.randint(5,24)}\n",
    "        elif model_name == 'XGBRegressor': #min_child_samples없음 # num_leaves 없음 # cpu 병렬지원(n_jobs=-1, auto) # 시간소요\n",
    "            param_grid={'n_estimators': stats.randint(200,1000),'learning_rate': stats.uniform(0.01,0.6),'subsample':stats.uniform(0.2,0.8),\n",
    "                \"max_depth\":stats.randint(5,50),'colsample_bytree':stats.uniform(0.2,0.8),'min_child_weight':stats.loguniform(1e-3, 1e3) }\n",
    "        elif model_name == 'LGBMRegressor':  # cpu 병렬지원(n_jobs=-1, 100% 가동) # 매우빠름\n",
    "            param_grid={'n_estimators':stats.randint(200,1000),'learning_rate':stats.uniform(0.01,0.6),'min_child_samples':stats.randint(10,500),\n",
    "                'colsample_bytree':stats.uniform(loc=0.4,scale=0.6),'num_leaves': stats.randint(32,1000),'subsample':stats.uniform(0.2,0.8),\n",
    "                'min_child_weight':stats.loguniform(1e-3,1e3),'reg_alpha':stats.loguniform(1e-3,1e2),'reg_lambda':stats.loguniform(1e-3,1e3)}\n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid\n",
    "        \n",
    "    elif (grid == 3) : #GridSearch     \n",
    "        # clf\n",
    "        if model_name == 'LogisticRegression': # \"solver\":['lbfgs','auto']\n",
    "            param_grid  = {'C': [0.1,0.2,0.3],\"l1_ratio\": [0.4,0.6,0.8],\"max_iter\":[400,500,600]}\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid ={'C': [0.2,0.3,0.4],\"max_iter\":[200,300,400] }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate':'optimal','penalty':'l2','loss':'hinge','eta0':[0.0,1.0]\n",
    "            param_grid ={'alpha': [0.00001,0.001,0.1],\"l1_ratio\":[0.1,0.5,1],'epsilon':[0.1,0.5,1],'max_iter':[100,300,500]} # 빠름\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            param_grid={'max_depth':[25,35,45],'min_samples_leaf':[50,100,150],'max_features':[10,20,30],'min_samples_split':[50,100,150]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            param_grid={'max_depth':[25,35,45],'min_samples_leaf':[10,20,30],'max_features':[5,10,20],'min_samples_split':[40,55,70]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림 #n_estimators=100learning_rate:0.1subsample:1.0 ccp_alpha:0.0 n_features\n",
    "            param_grid={'max_depth':[20,30],'min_samples_leaf':[200,330, 400],'max_features':[10,15]}\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            param_grid={# 'n_estimators':[400,500,600], \n",
    "                        'learning_rate':[0.08],\n",
    "                        'max_depth':[30,40,50],'min_child_weight':[100,500,1000],\n",
    "                        'colsample_bytree':[0.2,0.4,0.6],'subsample':[0.5,0.7,0.9]}\n",
    "                        #'reg_alpha':[0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            param_grid={'n_estimators':[800],\n",
    "                         'learning_rate':[0.3],\n",
    "                        'min_child_samples':[300,500,700],'min_child_weight':[700,900,1000],\n",
    "                        'num_leaves':[200,250,300],'colsample_bytree':[0.5,0.75,1],'subsample':[0.4,0.55,0.7],\n",
    "                         'reg_alpha': [0.057],'reg_lambda':[0.021]}\n",
    "      \n",
    "        # reg\n",
    "        elif model_name == 'LinearRegression':\n",
    "            param_grid  = {'n_jobs' : [-1, 20 ], 'fit_intercept' : [True, False]}\n",
    "        elif model_name == 'LinearSVR':#degree,C,kernal,epsilon,gamma,tol(허용오차)#degree차원 #C커지면곡선->직선 #gamma커지면과적합\n",
    "            param_grid ={'C': [1, 10], 'max_iter' : [100, 1000]}\n",
    "        elif model_name == 'SGDRegressor':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid ={'alpha': [0.0001,0.1],\"l1_ratio\":[0.01,0.5],'epsilon':[0.1,1],'max_iter':[100,500]}\n",
    "        elif model_name == 'Ridge': # L2규제(feature 가중치(베타)최소화)\n",
    "            param_grid ={'alpha': [0.01, 0.1, 1, 10, 100, 500], 'solver':['auto','svd']}\n",
    "        elif model_name == 'Lasso': # L1규제 (feature 가중치 제거)\n",
    "            param_grid ={'alpha': [0.01, 0.1, 1, 10, 100, 500], 'max_iter' : [100, 1000]}\n",
    "        elif model_name == 'ElasticNet': # if L1 a, if L2 b  => alpha = a+b, l1_ratio = a/(a+b)  # if l1_ratio=1 => a=1, b=0\n",
    "            param_grid ={'alpha': [0.1, 1, 10, 100, 500], \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "        elif model_name == 'DecisionTreeRegressor':\n",
    "            param_grid={'max_depth':[15,22,30],'min_samples_leaf':[100,200,300],'min_samples_split':[200,500,1000]}\n",
    "        elif model_name == 'RandomForestRegressor': # cpu 병렬지원(n_jobs=-1, 100% 가동)\n",
    "            param_grid={'max_depth':[15,22,30],'min_samples_leaf':[100,200,300],'min_samples_split':[200,500,1000]}\n",
    "        elif model_name == 'GradientBoostingRegressor': # param많으면 학습시간이 많이 걸림(순차 시행, cpu 1개만 수행)\n",
    "            param_grid={'max_depth':[15,22,30],'min_samples_leaf':[100,200,300],'min_samples_split':[200,500,1000]}\n",
    "        elif model_name == 'XGBRegressor': #min_child_samples없음 # num_leaves 없음 # cpu 병렬지원(n_jobs=-1, auto) # 시간소요\n",
    "            param_grid={'max_depth':[15,22,30],'min_child_weight':[0.1,1,5],\n",
    "                        'colsample_bytree':[0.5,0.75,0.95],'subsample':[0.6,0.8,1]}\n",
    "        elif model_name == 'LGBMRegressor':  # cpu 병렬지원(n_jobs=-1, 100% 가동) # 매우빠름\n",
    "            param_grid={'min_child_samples':[20,60,100],'num_leaves':[32,64,128],\n",
    "                        'colsample_bytree':[0.5,0.75,1],'subsample':[0.6,0.8,1]}\n",
    "                \n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 get_model_train_eval  # 3-3 metricPlot 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 학습/테스트 데이터 셋을 입력하면 학습 -> GridSerchCV 수행 + 성능 평가(metric) 등 반환\n",
    "def get_model_train_eval(model,model_name,time_name,start_model_time,X_train,X_test,y_train,y_test,eval_X=None,eval_y=None):\n",
    "# def get_model_train_eval(model,model_name,time_name,start_model_time,X_train,X_test,y_train,y_test):\n",
    "    early_stopping_modellist = ['XGBClassifier','LGBMClassifier','XGBRegressor','LGBMRegressor'] # SGD 자체적\n",
    "    print('###',model.__class__.__name__,'###')\n",
    "    if (grid == 2) :  # RandomSearchCV\n",
    "        param_grid  = get_params(model_name)\n",
    "        print(f'RandomSearchCV 입력 파라미터:{param_grid}')\n",
    "        if (est == 'clf'): #  분류 scoring : accuracy, f1, neg_log_loss, roc_auc\n",
    "            model=RandomizedSearchCV(model,param_distributions=param_grid,scoring='roc_auc',return_train_score=False, n_iter=10)\n",
    "        elif (est =='reg'):\n",
    "            model=RandomizedSearchCV(model,param_distributions=param_grid,scoring='neg_mean_squared_error',return_train_score=True,n_iter=5)\n",
    "        if (model_name in early_stopping_modellist):\n",
    "            model.fit(X_train, y_train,early_stopping_rounds=20,eval_metric=\"logloss\",eval_set=[(eval_X,eval_y)],verbose=False)\n",
    "        else:\n",
    "            model.fit(X_train, y_train) \n",
    "        best_params = model.best_params_\n",
    "        print(f'RandomSearchCV 최적 파라미터:{ best_params}\\n')\n",
    "        # RandomSearch plot_grid_curve : error (because of param_grid scipy.stats)\n",
    "        model = model.best_estimator_\n",
    "        metrics_dict = metricPlot(model,model_name,time_name,start_model_time,X_train,y_train,X_test,y_test)\n",
    "        return model, best_params, metrics_dict\n",
    "\n",
    "    elif (grid == 3) : # GridSearchCV\n",
    "        param_grid  = get_params(model_name)\n",
    "        print(f'GridSearchCV 입력 파라미터:{param_grid}')\n",
    "        if (est == 'clf'): #  분류 scoring : accuracy, f1, neg_log_loss, roc_auc  # clf cv default stratified k 방식\n",
    "            model=GridSearchCV(model,param_grid=param_grid,scoring='roc_auc',return_train_score=True) # cv=2 최소, default 5\n",
    "        elif (est =='reg'): # reg cv default k 방식\n",
    "            model=GridSearchCV(model,param_grid=param_grid,scoring='neg_mean_squared_error',return_train_score=True) # cv=2 최소, default 5\n",
    "        if (model_name in early_stopping_modellist):\n",
    "            model.fit(X_train, y_train,early_stopping_rounds=20,eval_metric=\"logloss\",eval_set=[(eval_X,eval_y)],verbose=False)\n",
    "        else:\n",
    "            model.fit(X_train, y_train) \n",
    "        best_params = model.best_params_\n",
    "        print(f'GridSearchCV 최적 파라미터:{ best_params}')\n",
    "        if (plot ==1):\n",
    "            plot_grid_curve(model, param_grid, model_name, time_name)\n",
    "        # metric\n",
    "        model = model.best_estimator_\n",
    "        metrics_dict = metricPlot(model,model_name,time_name,start_model_time,X_train,y_train,X_test,y_test)\n",
    "        return model, param_grid, best_params, metrics_dict\n",
    "   \n",
    "    else: # grid = 1, 4\n",
    "        if (model_name in early_stopping_modellist):\n",
    "            model.fit(X_train, y_train,early_stopping_rounds=20,eval_metric=\"logloss\",eval_set=[(eval_X,eval_y)],verbose=False)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        metrics_dict = metricPlot(model,model_name,time_name,start_model_time,X_train,y_train,X_test,y_test)\n",
    "        return model, metrics_dict      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 trainMetricSave 실제 실행 함수 ## 3-2 get_model_train_eval 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMetricSave(model_list,X_train,X_test,y_train,y_test,eval_X=None,eval_y=None):\n",
    "    time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    params_dict={}\n",
    "    for model in model_list:\n",
    "        start_model_time = time.time()\n",
    "        model_name = model.__class__.__name__\n",
    "        param_grid=None\n",
    "        best_params=None\n",
    "        if(grid == 1 or grid == 4): # GridSerchCV 없이 초기 2. ~ 3. model 훈련, metric 리턴받아 처리 \n",
    "            model, metrics_dict = \\\n",
    "                get_model_train_eval(model,model_name,time_name,start_model_time,X_train,X_test,y_train,y_test,eval_X,eval_y)\n",
    "        elif(grid == 2): # RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "            model, best_params, metrics_dict =\\\n",
    "                get_model_train_eval(model,model_name,time_name,start_model_time,X_train,X_test,y_train,y_test,eval_X,eval_y)\n",
    "        elif(grid == 3): # GridSerchCV, RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "            model, param_grid, best_params, metrics_dict =\\\n",
    "                get_model_train_eval(model,model_name,time_name,start_model_time,X_train,X_test,y_train,y_test,eval_X,eval_y)\n",
    "        ## metrics json 저장 \n",
    "        metrics_total_dict = {}\n",
    "        spend_model_time=(time.time() - start_model_time)\n",
    "        metrics_total_dict['Experiment_date_time'] = time_name\n",
    "        metrics_total_dict['data_name'] = data_name\n",
    "        metrics_total_dict['model_name'] = model_name\n",
    "        if(param_grid):        \n",
    "            metrics_total_dict['param_grid'] = param_grid \n",
    "        if(best_params):        \n",
    "            metrics_total_dict['best_params'] = best_params\n",
    "        metrics_total_dict['model_params'] = model.get_params()\n",
    "        metrics_total_dict['metrics'] =metrics_dict\n",
    "        metrics_total_dict['Execution_time'] = spend_model_time\n",
    "        with open(f'./json/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "            json.dump(metrics_total_dict, f, indent=4)            \n",
    "        print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n",
    "        if(best_params):\n",
    "            params = 'random_state=1'\n",
    "            for key, value in best_params.items():\n",
    "                params = params + ',' + str(key) + '=' + str(value)\n",
    "    #             print(f'{model_name} params : {params}\\n')\n",
    "            params_dict[f'{model_name}']=params\n",
    "    print(f'best params : {params_dict}')\n",
    "    spend_time=(time.time() - start_time)\n",
    "    print(f'전체수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data preprocessing, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.필요시 전처리 리턴 => 2. X,y(target)분리 => 3. 필요시 scaling => 4. train,test로 split\n",
    "def get_train_test_dataset(df=None):\n",
    "    y_target = df['target']\n",
    "    X_features = df.drop(['target'], axis=1,inplace=False)\n",
    "    \n",
    "    ##  scaler 필요시\n",
    "    columns=X_features.columns\n",
    "#     scaler = MinMaxScaler()\n",
    "#     scaler = StandardScaler()\n",
    "#     X_features = scaler.fit_transform(X_features) # nd.array 변환됨 df변환 필요\n",
    "    X_features = np.log1p(X_features)\n",
    "    ## 다항 \n",
    "#     X_features = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_features)\n",
    "#     X_features = pd.DataFrame(X_features) # polynomialFeatures 사용시 df변환\n",
    "    X_features = pd.DataFrame(X_features, columns=columns) # df 변환\n",
    "    if(yLog == True):\n",
    "        y_target = np.log1p(y_target)\n",
    "\n",
    "    # oneHotencoder\n",
    "    X_features = pd.get_dummies(X_features)\n",
    "    \n",
    "    ## train_test_split( )으로 학습과 테스트 데이터 분할. \n",
    "    # 분류에서 target 불균형시 stratify=y_target으로 Stratified 기반 분할, # 회귀에서는 stratify사용시 error\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)\n",
    "    # xgb, lgbm early_stopping_rounds 사용시 ==> test 데이터 eval_set 으로 다시 분리 \n",
    "    X_train, eval_X, y_train, eval_y = train_test_split(X_train, y_train, test_size=0.2, random_state=256)\n",
    "    \n",
    "#     return X_train, X_test, y_train, y_test\n",
    "    return X_train, X_test, y_train, y_test, eval_X, eval_y\n",
    "#     return scaler, p_degree, X_train, X_test, y_train, y_test\n",
    "#     return scaler, p_degree, X_train, X_test, y_train, y_test, eval_X, eval_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN  \n",
    "### data 선택 (1 get_train_test_dataset 호출), model 선택후 run  ##  3-1 trainMetricSave 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수행 시간 측정을 위함. 시작 시간 설정.\n",
    "start_time = time.time()\n",
    "\n",
    "data_name = 'project_reg_sampling_1_1_re'\n",
    "\n",
    "est = \"clf\"    # 맞는 df 선택 # pd.get_dummies() oHe 여부, target scaling여부\n",
    "# est = \"reg\"    # 맞는 df 선택 # pd.get_dummies() oHe 여부, target scaling여부\n",
    "\n",
    "# target scaler(회귀)했다면(yLog) metric할 때 is_expm1\n",
    "# yLog=False\n",
    "yLog=True\n",
    "\n",
    "grid = 1      # 초기모델링    # 초기 model_list\n",
    "# grid = 2     # RandomSearchCV 수행    # params 조정\n",
    "# grid = 3     # GridSearchCV 수행     # params 조정\n",
    "# grid = 4    # best params 적용  # 해당 model_list\n",
    "\n",
    "plot = 0    # plot  disable 수행시간때문에\n",
    "# plot = 1  # plot enable \n",
    "\n",
    "def run():    \n",
    "    ## data 데이터 선택 ########################################\n",
    "#     df = dataset()\n",
    "#     df = bike_df\n",
    "#     df = house_df_ohe\n",
    "#     df = pima_df\n",
    "    # project\n",
    "    df = pd.read_csv('./data/NHIS_total_model.csv')\n",
    "#     df = pd.read_csv('./data/NHIS_model_1.csv')        # clf\n",
    "#     df = pd.read_csv('./data/NHIS_model_clf_PI_1.csv')        # clf\n",
    "#     df = pd.read_csv('./data/NHIS_model_reg_2.csv')  # reg\n",
    "#     df = pd.read_csv('./data/NHIS_model_reg_4_corr.csv')  # reg\n",
    "#     df = pd.read_csv('./data/NHIS_model_reg_4_sampling_1_1.csv')  # reg\n",
    "#     df = df.iloc[:1000, :]\n",
    "#     df = sklearn.utils.shuffle(df)\n",
    "\n",
    "    # clf\n",
    "#     target_count = df[df['target'] == 1].shape[0]\n",
    "#     target_ex_count = df[df['target'] != 1].shape[0]\n",
    "#     print(f\"target   count : {target_count} ({ round( target_count/df.shape[0], 4)*100 })%\")\n",
    "#     print(f\"target외 count : {target_ex_count} ({ round( target_ex_count/df.shape[0], 4)*100 })%\")\n",
    "\n",
    "    ## 1. 데이터 전처리(preprocessing), 분리(split) ##########################################\n",
    "#     X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n",
    "#     scaler, p_degree, X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n",
    "    X_train, X_test, y_train, y_test, eval_X, eval_y = get_train_test_dataset(df)\n",
    "    \n",
    "    ## model 선택 ##################################### 'squared_loss’, 'huber’,'epsilon_insensitive’,squared_epsilon_insensitive’\n",
    "    if (est=='clf'):\n",
    "        model_list= [\n",
    "                 LogisticRegression(random_state=1, n_jobs=-1),\n",
    "                 LinearSVC(random_state=1, loss='hinge'),\n",
    "#                  SGDClassifier(random_state=1,n_jobs=-1,loss='modified_huber', early_stopping=True), # xgb,lgbm:fit조기중단가능\n",
    "#                  DecisionTreeClassifier(random_state=1),\n",
    "#                  RandomForestClassifier(random_state=1,n_jobs=-1),\n",
    "#                  ExtraTreeRegressor(random_state=1,n_jobs=-1)\n",
    "#                  GradientBoostingClassifier(random_state=1,validation_fraction=0.1,n_iter_no_change=20,subsample=0.25),\n",
    "#                  XGBClassifier(random_state=1,n_jobs=-1,silent=True,device='gpu'),\n",
    "#                  LGBMClassifier(random_state=1,n_jobs=-1),\n",
    "            \n",
    "                ]\n",
    "    elif (est=='reg'): # LinearSVR, GBM 수행시간 느림\n",
    "        model_list = [\n",
    "                      LinearRegression(n_jobs=-1), \n",
    "#                       LinearSVR(random_state=1,verbose=0), # 수행시간 느림 metric 65분\n",
    "#                       SGDRegressor(random_state=1,early_stopping=True), # yLog=True  error\n",
    "                      Ridge(random_state=1),\n",
    "                      Lasso(random_state=1),\n",
    "                      ElasticNet(random_state=1),\n",
    "                      DecisionTreeRegressor(random_state=1),\n",
    "                      RandomForestRegressor(random_state=1,n_jobs=-1),\n",
    "                      ExtraTreeRegressor(random_state=1,n_jobs=-1),\n",
    "#                       GradientBoostingRegressor(random_state=1,validation_fraction=0.1,n_iter_no_change=20,subsample=0.25), # \n",
    "                      XGBRegressor(random_state=1, n_jobs=-1, silent=True,device='gpu'),\n",
    "                      LGBMRegressor(random_state=1, n_jobs=-1),\n",
    "                      CatBoostClassifier(random_state=1, n_jobs=-1)\n",
    "                      \n",
    "                     ] \n",
    "    \n",
    "#     model_list= [\n",
    "#         LinearRegression(n_jobs=-1,fit_intercept=True), # random_state 없음\n",
    "#         LinearSVR(random_state=1, C=1,max_iter=1000),\n",
    "#         Ridge(random_state=1, alpha=10,solver='auto'),\n",
    "#         Lasso(random_state=1, alpha=100,max_iter=1000),\n",
    "#         ElasticNet(random_state=1, alpha=0.1,l1_ratio=0.9),\n",
    "#         DecisionTreeRegressor(random_state=1, max_depth=10,min_samples_leaf=10,min_samples_split=5),\n",
    "#         RandomForestRegressor(n_jobs=-1, random_state=1, max_depth=20,min_samples_leaf=5,min_samples_split=5),\n",
    "#         GradientBoostingRegressor(random_state=1, max_depth=10,min_samples_leaf=20,min_samples_split=10),\n",
    "#         XGBRegressor(n_jobs=-1, random_state=1, silent=True, max_depth=5,min_child_weight=0.1,subsample=0.8),\n",
    "#         LGBMRegressor( n_jobs=-1,random_state=1,max_depth=64,min_child_samples=20,num_leaves=64,subsample=0.6)\n",
    "#     ] \n",
    "\n",
    "    ##  2. ~ 3. 훈련, metric  ###################################################\n",
    "    trainMetricSave(model_list, X_train, X_test, y_train, y_test, eval_X, eval_y)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list= [\n",
    "    LogisticRegression(random_state=1,n_jobs=-1,C=0.1,l1_ratio=0.1,max_iter=100,solver='lbfgs'),\n",
    "    KNeighborsClassifier(n_jobs=-1,n_neighbors=10,p=1),\n",
    "    LinearSVC(random_state=1, loss='hinge'),\n",
    "    SGDClassifier(random_state=1,n_jobs=-1,alpha=0.0001,epsilon=0.1,eta0=0.0,l1_ratio=0.01,max_iter=100), \n",
    "    DecisionTreeClassifier(random_state=1,max_depth=5,max_features=8,min_samples_leaf=20,min_samples_split=10),\n",
    "    RandomForestClassifier(random_state=1,n_jobs=-1,max_depth=5,max_features=5,min_samples_leaf=15,min_samples_split=10),\n",
    "    GradientBoostingClassifier(random_state=1,max_depth=10,max_features=5,min_samples_leaf=10,min_samples_split=10),\n",
    "    XGBClassifier(random_state=1,n_jobs=-1,silent=True,colsample_bytree=0.5,max_depth=15,min_child_weight=1,reg_alpha=0,reg_lambda=0.5,subsample=1),\n",
    "    LGBMClassifier(random_state=1,n_jobs=-1,colsample_bytree=1,max_depth=128,min_child_samples=5,num_leaves=64,reg_alpha=0,reg_lambda=0.5,subsample=0.8)\n",
    "]\n",
    "\n",
    "model_list= [\n",
    "    LinearRegression(n_jobs=-1,fit_intercept=True), # random_state 없음\n",
    "    LinearSVR(random_state=1, C=1,max_iter=1000),\n",
    "    SGDRegressor(random_state=1),\n",
    "    Ridge(random_state=1, alpha=10,solver='auto'),\n",
    "    Lasso(random_state=1, alpha=100,max_iter=1000),\n",
    "    ElasticNet(random_state=1, alpha=0.1,l1_ratio=0.9),\n",
    "    DecisionTreeRegressor(random_state=1, max_depth=10,min_samples_leaf=10,min_samples_split=5),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=1, max_depth=20,min_samples_leaf=5,min_samples_split=5),\n",
    "    GradientBoostingRegressor(random_state=1, max_depth=10,min_samples_leaf=20,min_samples_split=10),\n",
    "    XGBRegressor(n_jobs=-1, random_state=1, silent=True, max_depth=5,min_child_weight=0.1,subsample=0.8),\n",
    "    LGBMRegressor( n_jobs=-1,random_state=1,max_depth=64,min_child_samples=20,num_leaves=64,subsample=0.6)\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개별 model별 \n",
    "### Clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Base__ccp_alpha': 0.0,\n",
       " 'Base__criterion': 'friedman_mse',\n",
       " 'Base__max_depth': 3,\n",
       " 'Base__max_features': None,\n",
       " 'Base__max_leaf_nodes': None,\n",
       " 'Base__min_impurity_decrease': 0.0,\n",
       " 'Base__min_impurity_split': None,\n",
       " 'Base__min_samples_leaf': 1,\n",
       " 'Base__min_samples_split': 2,\n",
       " 'Base__min_weight_fraction_leaf': 0.0,\n",
       " 'Base__presort': 'deprecated',\n",
       " 'Base__random_state': None,\n",
       " 'Base__splitter': 'best',\n",
       " 'Base': DecisionTreeRegressor(criterion='friedman_mse', max_depth=3),\n",
       " 'Dist': ngboost.distns.categorical.k_categorical.<locals>.Categorical,\n",
       " 'Score': ngboost.scores.LogScore,\n",
       " 'col_sample': 1.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'minibatch_frac': 1.0,\n",
       " 'n_estimators': 500,\n",
       " 'natural_gradient': True,\n",
       " 'random_state': RandomState(MT19937) at 0x1ED77B8C8C8,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': True,\n",
       " 'verbose_eval': 100}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target   count : 521858 (50.0)%\n",
      "target외 count : 521754 (50.0)%\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.5857 val_loss=0.0000 scale=2.0000 norm=3.7418\n",
      "[iter 200] loss=0.5649 val_loss=0.0000 scale=2.0000 norm=3.7782\n",
      "[iter 300] loss=0.5570 val_loss=0.0000 scale=1.0000 norm=1.9037\n",
      "[iter 400] loss=0.5524 val_loss=0.0000 scale=1.0000 norm=1.9166\n",
      " 정확도: 0.7182, 정밀도: 0.6948, 재현율: 0.7782, f1_score: 0.7341, auc_score:0.7919\n",
      "NGBClassifier metric 수행시간 : 36분 26.17초\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7fcaac31ed72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m## plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mPlot_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[0mprecision_recall_curve_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_name\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[0mroc_curve_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-0ce4401cb8df>\u001b[0m in \u001b[0;36mPlot_learning_curve\u001b[1;34m(model, model_name, time_name, X, y, n_jobs, train_sizes)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n\u001b[1;32m----> 5\u001b[1;33m             model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'neg_mean_squared_error', return_times=True)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest_scores_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times)\u001b[0m\n\u001b[0;32m   1281\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m             error_score=error_score, return_times=return_times)\n\u001b[1;32m-> 1283\u001b[1;33m             for train, test in train_test_proportions)\n\u001b[0m\u001b[0;32m   1284\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[0mn_cv_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mn_unique_ticks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# data_name = \"pima\"\n",
    "data_name = \"project_clf_104M_ng\"\n",
    "\n",
    "grid = 1      # 초기모델링    # 초기 model_list, # target scaler했다면 is_expm1=False, True 확인\n",
    "# grid = 2     # RandomSearchCV 수행    # params 조정\n",
    "# grid = 3     # GridSearchCV 수행     # params 조정\n",
    "# grid = 4    # best params 적용  # 해당 model_list\n",
    "\n",
    "# plot = 0    # plot  disable 수행시간때문에\n",
    "plot = 1  # plot enable \n",
    "\n",
    "est = \"clf\" \n",
    "\n",
    "# df = pd.read_csv('data/NHIS_total_model.csv')\n",
    "# df = pima_df\n",
    "df = pd.read_csv('data/NHIS_model_1.csv')\n",
    "# df = df.iloc[:200000, :]\n",
    "# df['target'] = df['식전혈당(공복혈당)'].apply(lambda x: 1 if x >= 126 else 0 )\n",
    "# df.drop('식전혈당(공복혈당)', axis=1, inplace=True)    \n",
    "\n",
    "target_count = df[df['target'] == 1].shape[0]\n",
    "target_ex_count = df[df['target'] != 1].shape[0]\n",
    "print(f\"target   count : {target_count} ({ round( target_count/df.shape[0], 4)*100 })%\")\n",
    "print(f\"target외 count : {target_ex_count} ({ round( target_ex_count/df.shape[0], 4)*100 })%\")\n",
    "\n",
    "y_target = df['target']\n",
    "X_features = df.drop(['target', '기준년도', '시도코드'], axis=1)\n",
    "# X_features = X = df[['Glucose','BloodPressure','BMI','Age']]\n",
    "\n",
    "## scaler시\n",
    "# columns = X_features.columns\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "# X_features = scaler.fit_transform(X_features)\n",
    "# X_features = np.log1p(X_features)\n",
    "# X_features = pd.DataFrame(X_features, columns=columns)\n",
    "\n",
    "## 데이터 학습과 테스트 데이터 분할 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156,stratify=y_target)\n",
    "# xgb,lgbm,sgb early_stopping_rounds 사용시 ==> test 데이터 eval_set 으로 다시 분리  # train, test 분리 0.3 필요\n",
    "X_test, eval_X, y_test, eval_y = train_test_split(X_test, y_test, test_size=0.2, random_state=256, stratify=y_test)\n",
    "\n",
    "## 모델 선택\n",
    "# model = LogisticRegression(random_state=1, n_jobs=-1)  \n",
    "# model = SVC(random_state=1, kernel=\"linear\", C=1, probability=True) # StandardScaler  # \n",
    "# model = SVC(random_state=1,probability=True, kernel='poly', degree=3, coef0=1, C=5) # StandardScaler\n",
    "# model = SVC(random_state=1,probability=True, kernel='rbf', gamma=5, C=0.001) # StandardScaler\n",
    "# model = LinearSVC(random_state=1, loss='hinge')\n",
    "# model = KNeighborsClassifier(n_jobs=-1)\n",
    "# model = SGDClassifier(random_state=1,n_jobs=-1)\n",
    "# model =  SGDClassifier(random_state=1,n_jobs=-1, loss='log')  # log => logit + SGD\n",
    "# model= GaussianNB()\n",
    "# model = tree.DecisionTreeClassifier(random_state=1)\n",
    "# model = DecisionTreeClassifier(random_state=1)\n",
    "# model = RandomForestClassifier(random_state=1,n_jobs=-1)\n",
    "# model = GradientBoostingClassifier(random_state=1),\n",
    "# model = XGBClassifier(random_state=1,colsample_bytree=0.5734329430545178,learning_rate=0.03251204127577167,\n",
    "#                     max_depth=22,min_child_weight=0.002426065522925542,n_estimators=429,subsample=0.9991897949519826,\n",
    "#                     silent=True,n_jobs=-1, device='gpu')\n",
    "# model = LGBMClassifier(random_state=1,colsample_bytree=0.5,learning_rate=0.3,min_child_samples=300,min_child_weight=900,\n",
    "#                        n_estimators=800,num_leaves=200,reg_alpha=0.057,reg_lambda=0.021,subsample=0.4)\n",
    "# model = CatBoostClassifier(verbose=0, n_estimators=1000)\n",
    "model = NGBClassifier(random_state=1) # 정확도: 0.7182, 정밀도: 0.6948, 재현율: 0.7782, f1_score: 0.7341, auc_score:0.7919 # 36분 # plot하지 말것 \n",
    "  \n",
    "## 모델 훈련 및 성능평가(metric)\n",
    "time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = model.__class__.__name__\n",
    "model.fit(X_train, y_train)\n",
    "# model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=[(eval_X, eval_y)], verbose=True)\n",
    "pred = model.predict(X_test)\n",
    "if model_name in ['LinearSVC']:\n",
    "    pred_proba = model._predict_proba_lr(X_test) \n",
    "else:\n",
    "    pred_proba = model.predict_proba(X_test) \n",
    "metrics_dict = get_clf_eval(y_test, pred, pred_proba[:,1])\n",
    "spend_metric_time=(time.time() - start_time)\n",
    "print(f'{model_name} metric 수행시간 : {int(spend_metric_time//60)}분 {spend_metric_time%60:.2f}초\\n')  \n",
    "\n",
    "## model 저장\n",
    "# pickle.dump(model, open(f'./modeling/pima_{model_name}_{time_name}.sav', 'wb'))\n",
    "\n",
    "## plot\n",
    "Plot_learning_curve(model,model_name,time_name,X_train,y_train,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5))\n",
    "precision_recall_curve_plot(y_test, pred_proba[:,1], model_name, time_name )\n",
    "roc_curve_plot(y_test, pred_proba[:,1], model_name, time_name)\n",
    "if model_name in ['LogisticRegression','SGDClassifier','LinearSVC']:\n",
    "    Plot_coef_logit(model, X_train.columns, model_name, time_name) # 회귀계수 \n",
    "else : \n",
    "    Plot_feature_importance(model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "results = permutation_importance(model, X_test, y_test, scoring='f1')\n",
    "importance = results.importances_mean\n",
    "Plot_permutation_importance(importance, X_test.columns, model_name, time_name)\n",
    "    \n",
    "## metrics json 저장 \n",
    "metrics_total_dict = {}\n",
    "spend_time=(time.time() - start_time)\n",
    "metrics_total_dict['Experiment_date_time'] = time_name\n",
    "metrics_total_dict['data_name'] = data_name\n",
    "metrics_total_dict['model_name'] = model_name\n",
    "metrics_total_dict['model_params'] = model.get_params()\n",
    "metrics_total_dict['metrics'] =metrics_dict\n",
    "metrics_total_dict['Execution_time'] = spend_time\n",
    "# with open(f'./json1/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "with open(f'./json1/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "    json.dump(metrics_total_dict, f, indent=4)            \n",
    "print(f'{model_name} 전체 수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Base__ccp_alpha': 0.0,\n",
       " 'Base__criterion': 'friedman_mse',\n",
       " 'Base__max_depth': 3,\n",
       " 'Base__max_features': None,\n",
       " 'Base__max_leaf_nodes': None,\n",
       " 'Base__min_impurity_decrease': 0.0,\n",
       " 'Base__min_impurity_split': None,\n",
       " 'Base__min_samples_leaf': 1,\n",
       " 'Base__min_samples_split': 2,\n",
       " 'Base__min_weight_fraction_leaf': 0.0,\n",
       " 'Base__presort': 'deprecated',\n",
       " 'Base__random_state': None,\n",
       " 'Base__splitter': 'best',\n",
       " 'Base': DecisionTreeRegressor(criterion='friedman_mse', max_depth=3),\n",
       " 'Dist': ngboost.distns.normal.Normal,\n",
       " 'Score': ngboost.scores.LogScore,\n",
       " 'col_sample': 1.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'minibatch_frac': 1.0,\n",
       " 'n_estimators': 500,\n",
       " 'natural_gradient': True,\n",
       " 'random_state': RandomState(MT19937) at 0x1ED77B8C8C8,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': True,\n",
       " 'verbose_eval': 100}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGBRegressor().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=0.2746 val_loss=0.0000 scale=1.0000 norm=0.5614\n",
      "[iter 100] loss=0.1625 val_loss=0.0000 scale=2.0000 norm=1.0922\n",
      "[iter 200] loss=0.1161 val_loss=0.0000 scale=1.0000 norm=0.5640\n",
      "[iter 300] loss=0.0997 val_loss=0.0000 scale=2.0000 norm=1.1425\n",
      "[iter 400] loss=0.0907 val_loss=0.0000 scale=1.0000 norm=0.5749\n",
      "MAE: 27.985, MSE: 1872.801, RMSE: 43.276, RMSLE: 0.273, R2: 0.187\n",
      "\n",
      "NGBRegressor metric 수행시간 : 50분 6.58초\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type DecisionTreeRegressor is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-6e60c9d00b56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;31m# with open(f'./json1/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./json1/{data_name}_{model_name}_{time_name}.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8-sig'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics_total_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Circular reference detected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\anaconda3\\envs\\py37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type DecisionTreeRegressor is not JSON serializable"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# data_name = \"bostonHouse\"\n",
    "# data_name = \"bike\"\n",
    "# data_name = \"project\"\n",
    "data_name = \"project_reg_4_sample_1_1\"\n",
    "\n",
    "grid = 1      # 초기모델링    # 초기 model_list, # target scaler했다면 is_expm1=False, True 확인\n",
    "# grid = 2     # RandomSearchCV 수행    # params 조정\n",
    "# grid = 3     # GridSearchCV 수행     # params 조정\n",
    "# grid = 4    # best params 적용  # 해당 model_list\n",
    "\n",
    "# plot = 0    # plot  disable 수행시간때문에\n",
    "# plot = 1  # plot enable \n",
    "\n",
    "est = \"reg\" \n",
    "yLog = 1\n",
    "# yLog = False\n",
    " \n",
    "## project data\n",
    "# df = pd.read_csv('data/NHIS_total_model.csv')\n",
    "# df = pd.read_csv('./data/NHIS_model_reg_1.csv')  # reg\n",
    "df = pd.read_csv('./data/NHIS_model_reg_4_sampling_1_1.csv')  # reg\n",
    "# df = df.iloc[:1000000, :]\n",
    "# df = sklearn.utils.shuffle(df)\n",
    "\n",
    "y_target = df['target']\n",
    "X_features = df.drop(['target'], axis=1,inplace=False)\n",
    "\n",
    "y_target = np.log1p(y_target)\n",
    "\n",
    "##  scaler 필요시\n",
    "columns=X_features.columns\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_features = scaler.fit_transform(X_features) # nd.array 변환됨 df변환 필요\n",
    "# X_features = np.log1p(X_features)\n",
    "## 다항 \n",
    "# X_features = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_features)\n",
    "# X_features = pd.DataFrame(X_features) # polynomialFeatures 사용시 df 변환\n",
    "X_features = pd.DataFrame(X_features, columns=columns) # df 변환\n",
    "X_features = pd.get_dummies(X_features) # oHe\n",
    "\n",
    "## 데이터 학습과 테스트 데이터 분할  # stratify 회귀에서는 삭제\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)\n",
    "# xgb,lgbm,sgb early_stopping_rounds 사용시 ==> test 데이터 eval_set 으로 다시 분리  # train, test 분리 0.3 필요\n",
    "X_test, eval_X, y_test, eval_y = train_test_split(X_test, y_test, test_size=0.2, random_state=256)\n",
    "\n",
    "\n",
    "## 아래 params에서 n_jobs=-1이 있는 곳은 그대로 추가 # xgb는 silent=True까지 추가\n",
    "# model = LinearRegression(n_jobs=-1,fit_intercept=True)\n",
    "# model = LinearSVR(random_state=1, C=1,max_iter=100)\n",
    "# model = SGDRegressor(random_state=1)\n",
    "# model = SVR(kernel=\"linear\", C=1) # StandardScaler  # \n",
    "# model = SVR(kernel='poly', degree=2, epsilon=0.1, C=100) # StandardScaler\n",
    "# model = SVR(kernel='rbf', gamma=5, C=0.001) # StandardScaler\n",
    "# model = Ridge(random_state=1, alpha=10,solver='auto')\n",
    "# model = Lasso(random_state=1, alpha=0.1,max_iter=100)\n",
    "# model = ElasticNet(random_state=1, alpha=0.1,l1_ratio=0.9)\n",
    "\n",
    "# model = DecisionTreeRegressor(random_state=1, max_depth=13,min_samples_leaf=5,min_samples_split=5)\n",
    "# model = RandomForestRegressor(n_jobs=-1, random_state=1, max_depth=20,min_samples_leaf=5,min_samples_split=5)\n",
    "# model = GradientBoostingRegressor(random_state=1, max_depth=13,min_samples_leaf=10,min_samples_split=50)\n",
    "# model = XGBRegressor(n_jobs=-1, random_state=1, silent=True, max_depth=20,min_child_weight=7,subsample=0.6)\n",
    "# model = LGBMRegressor( n_jobs=-1,random_state=1,max_depth=64,min_child_samples=20,num_leaves=128,subsample=0.6)\n",
    "# model = ExtraTreeRegressor(random_state=1)\n",
    "# model = CatBoostRegressor( n_jobs=-1,random_state=1)\n",
    "model = NGBRegressor(random_state=1) # MAE: 27.985, MSE: 1872.801, RMSE: 43.276, RMSLE: 0.273, R2: 0.187 # 50분\n",
    "  \n",
    "## 모델 훈련 및 성능평가(metric)\n",
    "time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = model.__class__.__name__\n",
    "model.fit(X_train, y_train)\n",
    "# model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=[(eval_X, eval_y)], verbose=True)\n",
    "pred = model.predict(X_test)\n",
    "if yLog : #is_expm1 <= targetlog변환시 복원\n",
    "    y_test = np.expm1(y_test)\n",
    "    pred = np.expm1(pred)  \n",
    "metrics_dict = get_reg_eval(y_test, pred)\n",
    "spend_metric_time=(time.time() - start_time)\n",
    "print(f'{model_name} metric 수행시간 : {int(spend_metric_time//60)}분 {spend_metric_time%60:.2f}초\\n')  \n",
    "\n",
    "## model 저장\n",
    "# pickle.dump(model, open(f'./modeling/{model_name}_{time_name}.sav', 'wb'))\n",
    "\n",
    "## plot\n",
    "# Plot_learning_curve(model,model_name,time_name,X_train,y_train,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5))\n",
    "# if model_name in ['LinearRegression','SGDRegressor','LinearSVR']:\n",
    "#     Plot_coef_logit(model, X_train.columns, model_name, time_name) # 회귀계수 \n",
    "# else : \n",
    "#     Plot_feature_importance(model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "# results = permutation_importance(model, X_test, y_test, scoring='neg_mean_absolute_error')  # r2\n",
    "# importance = results.importances_mean\n",
    "# Plot_permutation_importance(importance, X_test.columns, model_name, time_name)\n",
    "    \n",
    "## metrics json 저장  #  NGBRegressor error : Object of type DecisionTreeRegressor is not JSON serializable\n",
    "metrics_total_dict = {}\n",
    "spend_time=(time.time() - start_time)\n",
    "metrics_total_dict['data_name'] = data_name\n",
    "metrics_total_dict['model_name'] = model_name\n",
    "model_params = model.get_params() # dict\n",
    "metrics_total_dict['model_params'] = model_params\n",
    "# metrics_total_dict['scaler'] = scaler\n",
    "# metrics_total_dict['p_degree'] = p_degree\n",
    "metrics_total_dict['metrics'] =metrics_dict\n",
    "metrics_total_dict['Execution_time'] = spend_time\n",
    "# with open(f'./json1/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "with open(f'./json1/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "    json.dump(metrics_total_dict, f, indent=4)            \n",
    "print(f'수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pima diabetes\n",
    "pima_df = pd.read_csv('data/diabetes.csv')\n",
    "df=pima_df\n",
    "#0 제거 후 nan 으로 변환 # np.nan의 type : float\n",
    "col=['Glucose', 'BloodPressure','SkinThickness','Insulin','BMI' ]\n",
    "for i in col:\n",
    "    df[i].replace(0, np.nan, inplace= True)\n",
    "    # median 중앙값 찾기\n",
    "def median_target(var):   \n",
    "    temp = df[df[var].notnull()]\n",
    "    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n",
    "    return temp\n",
    "# o값 대체 NaN 채워넣기 \n",
    "df.loc[(df['Outcome'] == 0 ) & (df['Glucose'].isnull()), 'Glucose'] = median_target('Glucose')[ median_target('Glucose')['Outcome']==0]['Glucose'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['Glucose'].isnull()), 'Glucose'] = median_target('Glucose')[ median_target('Glucose')['Outcome']==1]['Glucose'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['BloodPressure'].isnull()), 'BloodPressure'] = median_target('BloodPressure')[ median_target('BloodPressure')['Outcome']==0]['BloodPressure'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['BloodPressure'].isnull()), 'BloodPressure'] = median_target('BloodPressure')[ median_target('BloodPressure')['Outcome']==1]['BloodPressure'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['SkinThickness'].isnull()), 'SkinThickness'] = median_target('SkinThickness')[ median_target('SkinThickness')['Outcome']==0]['SkinThickness'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['SkinThickness'].isnull()), 'SkinThickness'] = median_target('SkinThickness')[ median_target('SkinThickness')['Outcome']==1]['SkinThickness'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['Insulin'].isnull()), 'Insulin'] =  median_target('Insulin')[ median_target('Insulin')['Outcome']==0]['Insulin'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['Insulin'].isnull()), 'Insulin'] = median_target('Insulin')[ median_target('Insulin')['Outcome']==1]['Insulin'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['BMI'].isnull()), 'BMI'] = median_target('BMI')[ median_target('BMI')['Outcome']==0]['BMI'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['BMI'].isnull()), 'BMI'] = median_target('BMI')[ median_target('BMI')['Outcome']==1]['BMI'].values[0]\n",
    "# df.isnull().sum()\n",
    "df['target'] = df['Outcome']\n",
    "df.drop('Outcome', axis=1, inplace=True)\n",
    "pima_df=df\n",
    "# pima_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0. data_name, df 선택 ==> target 분리, 전처리영역필요여부\n",
    "1. 초기모델은 grid = 1 선택, 전체 model_list for문, 과적합 learning_curve plot 확인\n",
    "    model_list,  Plot learning_curve\n",
    "    \n",
    "2. GridSearchCV 사용 grid = 2 선택, 전체 model_list for문, parameter별 영향력 확인 grid_curve\n",
    "    model_list,  plot grid_curve\n",
    "    ==> 1항에서 parameter tunning 반복가능\n",
    "    \n",
    "3. grid = 3 best param model 선택, 과적합 learning_curve plot 확인\n",
    "    model_list, Plot learning_curve\n",
    "    \n",
    "4. model과 최적 param 찾았다면, model\n",
    "    model, Plot (coeff_, feature_importance_ )\n",
    "    \n",
    "5. feature engineering : scaler, p_degree(다항식), \n",
    "    scaler log1p 변환시 is_expm1=True\n",
    "    1. 3.항 실행\n",
    "6. xgb, lgbm의 early기능 사용시 eval_X, eval_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./static/json/{model_name}_{time_name}.json', 'r', encoding='UTF-8-sig') as json_file:\n",
    "    #     data = json.load(json_file)\n",
    "    #     print(data)\n",
    "def dictParse(data):\n",
    "    time_name = data['Experiment_date_time']\n",
    "    data_name= data['data_name']\n",
    "    model_name= data['model_name']\n",
    "    model_params = data['model_params']\n",
    "    if 'scaler' in data.keys():\n",
    "        scaler = data['scaler']\n",
    "    if 'p_degree' in data.keys():\n",
    "        p_degree = data['p_degree']\n",
    "    mae_v = data['metrics']['MAE']\n",
    "    mse_v = data['metrics']['MSE']\n",
    "    rmse_v = data['metrics']['RMSE']\n",
    "    rmsle_v = data['metrics']['RMSLE']\n",
    "    Execution_time = data['Execution_time'] \n",
    "    return data_name, model_name, model_params, rmsle_v, rmse_v, mse_v, Execution_time\n",
    "metrics_total_dict = run()\n",
    "data_name, model_name, model_params, rmsle_v, rmse_v, mse_v, Execution_time = dictParse(metrics_total_dict)\n",
    "# model_n, scaler, p_degree, rmsle_v, rmse_v, mse_v, Execution_time = jsonParse(metrics_total_dict)\n",
    "print(f'Experiment_date_time:{time_name}\\n data_name : {data_name}\\n model_name : {model_name}\\n model_params : {model_params}\\n MAE {mae_v}\\n MSE {mse_v}\\n RMSE {rmse_v}\\n  RMSLE {rmsle_v}\\n Execution_time {Execution_time}')\n",
    "# print(f'model name : {model_n}\\n model_params : {model_parma}\\n scaler {scaler}\\n p_degree {p_degree}\\n RMSLE {rmsle_v}\\n RMSE {rmse_v}\\n MAE {mae_v}\\n Execution_time {Execution_time}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data관련\n",
    "\n",
    "def dataset():\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.datasets import load_breast_cancer # xgb 유방암, voting사용\n",
    "    from sklearn.datasets import load_diabetes  # pima아니고 \n",
    "    from sklearn.datasets import load_boston # 회귀\n",
    "   \n",
    "    # 보스턴 데이터 세트 로드\n",
    "    boston = load_boston()\n",
    "    df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "    df['PRICE'] = boston.target\n",
    "#     print(df.info()) # 506\n",
    "    return df\n",
    "# dataset()\n",
    "\n",
    "bike_df = pd.read_csv('data/bike_train.csv')\n",
    "# 문자열을 datetime 타입으로 변경. \n",
    "bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)\n",
    "# datetime 타입에서 년, 월, 일, 시간 추출\n",
    "bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)\n",
    "bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)\n",
    "bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)\n",
    "bike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour)\n",
    "drop_columns = ['datetime','casual','registered'] # registered와 count 거의 동일해서\n",
    "bike_df.drop(drop_columns, axis=1,inplace=True)\n",
    "# print(bike_df.info()) # 10886\n",
    "# df = bike_df\n",
    "\n",
    "# AmesHouse\n",
    "house_df = pd.read_csv('data/house_price.csv')\n",
    "# Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제\n",
    "house_df.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu'], axis=1 , inplace=True)\n",
    "# Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체\n",
    "house_df.fillna(house_df.mean(),inplace=True)\n",
    "null_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0]\n",
    "# print('## Null 피처의 Type :\\n', house_df.dtypes[null_column_count.index])\n",
    "# 문자형(범주형) feature one-hot 인코딩으로 shape 변환됨 (1460, 75) ==> (1460, 271) \n",
    "house_df_ohe = pd.get_dummies(house_df)\n",
    "null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]\n",
    "# print('## Null 피처의 Type :\\n', house_df_ohe.dtypes[null_column_count.index])\n",
    "\n",
    "# AmesHouse scaler2 ==> X_features skew큰것 log1p변환\n",
    "from scipy.stats import skew\n",
    "# object가 아닌 숫자형 피쳐의 컬럼 index 객체 추출.\n",
    "features_index = house_df.dtypes[house_df.dtypes != 'object'].index\n",
    "# house_df에 컬럼 index를 [ ]로 입력하면 해당하는 컬럼 데이터 셋 반환. apply lambda로 skew( )호출 \n",
    "skew_features = house_df[features_index].apply(lambda x : skew(x))\n",
    "# skew 정도가 1 이상인 컬럼들만 추출. \n",
    "skew_features_top = skew_features[skew_features > 1]\n",
    "# print(skew_features_top.sort_values(ascending=False))\n",
    "house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index])\n",
    "house_df_ohe = pd.get_dummies(house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset(df=None):\n",
    "\n",
    "    ## target과 train 데이터 분리\n",
    "    ## clf\n",
    "    # pima\n",
    "#     y_target = df['Outcome']\n",
    "#     X_features = df.drop('Outcome', axis=1)\n",
    "    # customer(bank)\n",
    "#     y_target = df['TARGET']\n",
    "#     X_features = df.drop('TARGET', axis=1)    \n",
    "\n",
    "    # credit\n",
    "#     y_target = df['Class']\n",
    "#     X_features = df.drop('Class', axis=1)    \n",
    "#     scaler = StandardScaler()\n",
    "#     amount_n = scaler.fit_transform(X_features['Amount'].values.reshape(-1, 1))\n",
    "#     amount_n = np.log1p(X_features['Amount'])\n",
    "#     X_features.insert(0, 'Amount_Scaled', amount_n)\n",
    "#     X_features.drop(['Amount'], axis=1, inplace=True)\n",
    "\n",
    "    ## reg\n",
    "    #bostonHouse\n",
    "#     y_target = df['PRICE']\n",
    "#     X_features = df.drop(['PRICE'], axis=1,inplace=False)\n",
    "    # bike\n",
    "#     y_target = df['count']\n",
    "#     X_features = df.drop(['count'],axis=1,inplace=False)\n",
    "    # Ameshouse\n",
    "#     y_target = house_df_ohe['SalePrice']\n",
    "#     X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metric 함수 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score이용한  5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시  RMSE 구함. \n",
    "# scoring=\"neg_mean_squared_error\" # GridSearchCV에서도 동일함\n",
    "def get_model_cv_prediction(model, X_data, y_target):\n",
    "    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "    rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print('##### ',model.__class__.__name__ , ' #####')\n",
    "    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_error_data(y_test, pred, n_tops = 5):\n",
    "    # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. \n",
    "    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\n",
    "    result_df['predicted_count']= np.round(pred)\n",
    "    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])\n",
    "    # 예측값과 실제값이 가장 큰 데이터 순으로 출력. \n",
    "    print(result_df.sort_values('diff', ascending=False)[:n_tops])\n",
    "    \n",
    "# get_top_error_data(y_test,pred,n_tops=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_params()관련 LGBM기준 (XGB유사) # sklearn version별 api 다름\n",
    "# max_depth log2(데이터수) # bostonHouse 506=>8, # bike 10886=>13 # AmesHouse 1460=>10 # project 797만=>22\n",
    "# num_leaves  : 2^(max_depth) 작아야 함  최대 리프노드 갯수 = max_leaf_nodes(dt, rf)\n",
    "# min_child_samples : Leaf node가 되기 위한 최소한 데이터 개체수  = min_samples_leaf  # min_samples_split (dt,rf 분리 최소 데이터)\n",
    "# min_child_weight = min_child_leaf (dt, rf) *gbm은 없음    \n",
    "# subsample (gbm,xgb,lgbm) 데이터 샘플링 비율 *lgbm은 subsample_for_bin 20000, subsample_freq 0\n",
    "# colsample_bytree = max_features (dt, rf, gbm) 개별 트리를 학습할 때마다 무작위로 선택하는 feature 비율을 제어 (feature 많을 경우) * \n",
    "    # XGB : colsample_bylevel, colsample_bynode, colsample_bytree\n",
    "# reg_alpha : L1 규제(제거) # reg_lambda : L2 규제(최소화) \n",
    "# 영혼까지 learning_rate 내리고,  n_estimators(rf,gbm,xgb,lgbm) 올리고\n",
    "def get_params(model_name, data_name):  # param_grid 2개항목 이상일때만 plot_grid_curve 가능\n",
    "    if (grid == 2) : # RandomSearch    속도느링 LogisticRegression  GradientBoostingClassifier\n",
    "        # clf\n",
    "        if model_name == 'LogisticRegression': # 속도느링 #\"solver\":['lbfgs','auto']\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e0),'l1_ratio': stats.uniform(0, 1), \"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e0),\"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid = { 'alpha':stats.loguniform(1e-4,1e0),'l1_ratio':stats.uniform(0,1),'average':[True,False],\n",
    "                            'epsilon':stats.loguniform(1e-1, 1e1),'max_iter':stats.randint(100,500)}\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            if(data_name=='project'):param_grid = {\"max_depth\": stats.randint(5, 50), \"max_features\": stats.randint(2, 24),\n",
    "                \"min_samples_split\": stats.randint(10, 100), \"min_samples_leaf\": stats.randint(10, 100),\"criterion\": [\"gini\", \"entropy\"]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            if(data_name=='project'):param_grid = {\"max_depth\":  stats.randint(5, 50),\"max_features\":stats.randint(2, 24),\n",
    "            \"min_samples_split\":stats.randint(10,100),\"min_samples_leaf\":stats.randint(10,100),\"bootstrap\":[True,False],\"criterion\":[\"gini\",\"entropy\"]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림 #n_estimators=100 learning_rate:0.1 subsample:1.0 ccp_alpha:0.0 n_features\n",
    "            if(data_name=='project'):param_grid={ 'max_depth' : stats.randint(5, 50),'min_samples_leaf': stats.randint(10,500),\n",
    "                                                   'max_features':  stats.randint(5,24) }\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            if(data_name=='project'):param_grid={'n_estimators': stats.randint(200, 1000),'learning_rate': stats.uniform(0.01, 0.6),\n",
    "            'subsample':stats.uniform(0.3, 0.9),\"max_depth\":stats.randint(5,50),'colsample_bytree':stats.uniform(0.5,0.9),'min_child_weight':stats.loguniform(1e-3, 1e3) }\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            if(data_name=='project'):param_grid={'n_estimators': stats.randint(200, 1000),'learning_rate': stats.uniform(0.01, 0.6),\n",
    "            'min_child_samples':stats.randint(10,500),'colsample_bytree':stats.uniform(loc=0.4,scale=0.6),'num_leaves': stats.randint(32,1000),\n",
    "                                    'subsample': stats.uniform(loc=0.2, scale=0.8), 'min_child_weight': stats.loguniform(1e-3, 1e3),\n",
    "                                      'reg_alpha': stats.loguniform(1e-3, 1e2),'reg_lambda': stats.loguniform(1e-3, 1e3)}   \n",
    "\n",
    "        # reg\n",
    "        elif model_name == 'LinearRegression':\n",
    "            param_grid  = {'copy_X': [True, False], 'fit_intercept': [True,False] }\n",
    "        elif model_name == 'LinearSVR': \n",
    "            param_grid = {'C':stats.loguniform(1e-1,1e0),'intercept_scaling':loguniform(1e-1,1e0),\"max_iter\":stats.randint(100,500)}\n",
    "        elif model_name == 'SGDRegressor':\n",
    "            param_grid = {'alpha':stats.loguniform(1e-4,1e0),'l1_ratio':stats.uniform(0,1),'average':[True,False],\n",
    "                            'epsilon':stats.loguniform(1e-1, 1e1),'max_iter':stats.randint(100,500)}\n",
    "        elif model_name == 'Ridge': \n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e0),'fit_intercept': [True,False] } \n",
    "        elif model_name == 'Lasso': \n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e0),'fit_intercept': [True,False] } \n",
    "        elif model_name == 'ElasticNet': # if L1 a, if L2 b  => alpha = a+b, l1_ratio = a/(a+b)  # if l1_ratio=1 => a=1, b=0\n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e0),'l1_ratio': stats.uniform(0,1)} \n",
    "        elif model_name == 'DecisionTreeRegressor':\n",
    "            param_grid = {\"max_depth\": stats.randint(5, 50), \"max_features\": stats.randint(2, 24),\n",
    "              \"min_samples_split\": stats.randint(10, 100), \"min_samples_leaf\": stats.randint(10, 100)}\n",
    "        elif model_name == 'RandomForestRegressor': # cpu 병렬지원(n_jobs=-1, 100% 가동)\n",
    "            param_grid = {\"max_depth\":  stats.randint(5, 50),\"max_features\":stats.randint(2, 24),\"min_samples_split\":stats.randint(10,100),\n",
    "                          \"min_samples_leaf\":stats.randint(10,100)}\n",
    "        elif model_name == 'GradientBoostingRegressor': # param많으면 학습시간이 많이 걸림(순차 시행, cpu 1개만 수행)\n",
    "            param_grid={ 'max_depth' : stats.randint(5, 50),'min_samples_leaf': stats.randint(10,500),'max_features':  stats.randint(5,24)}\n",
    "        elif model_name == 'XGBRegressor': #min_child_samples없음 # num_leaves 없음 # cpu 병렬지원(n_jobs=-1, auto) # 시간소요\n",
    "            param_grid={'n_estimators': stats.randint(200,1000),'learning_rate': stats.uniform(0.01,0.6),'subsample':stats.uniform(0.2,0.8),\n",
    "                \"max_depth\":stats.randint(5,50),'colsample_bytree':stats.uniform(0.5,0.9),'min_child_weight':stats.loguniform(1e-3, 1e3) }\n",
    "        elif model_name == 'LGBMRegressor':  # cpu 병렬지원(n_jobs=-1, 100% 가동) # 매우빠름\n",
    "            param_grid={'n_estimators':stats.randint(200,1000),'learning_rate':stats.uniform(0.01,0.6),'min_child_samples':stats.randint(10,500),\n",
    "                'colsample_bytree':stats.uniform(loc=0.4,scale=0.6),'num_leaves': stats.randint(32,1000),'subsample':stats.uniform(0.2,0.8),\n",
    "                'min_child_weight':stats.loguniform(1e-3,1e3),'reg_alpha':stats.loguniform(1e-3,1e2),'reg_lambda':stats.loguniform(1e-3,1e3)}\n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid\n",
    "        \n",
    "    elif (grid == 3) : #GridSearch     \n",
    "        # clf\n",
    "        if model_name == 'LogisticRegression': # \"solver\":['lbfgs','auto']\n",
    "            param_grid  = {'C': [0.1,0.2,0.3],\"l1_ratio\": [0.4,0.6,0.8],\"max_iter\":[400,500,600]}\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid ={'C': [0.2,0.3,0.4],\"max_iter\":[200,300,400] }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid ={'alpha': [0.0001,0.01,0.1],\"l1_ratio\":[0.1,0.2,0.3],'epsilon':[0.1,1,10],'max_iter':[100,300,500]} # 빠름\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_samples_leaf':[10,100,500],'min_samples_split':[10,15,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_samples_leaf':[10,16,20],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[12,18,24],'min_samples_leaf':[10,50,100],'max_features':[100,500,1000]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[15,20,25],'min_samples_leaf':[50,100,150],'max_features':[4,8,12]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_samples_leaf':[10,100,500],'min_samples_split':[10,15,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_samples_leaf':[10,100,500],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[12,18,24],'min_samples_leaf':[10,50,100],'max_features':[100,500,1000]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[15,20,25],'min_samples_leaf':[20,40,60],'min_samples_split':[20,40,60]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림 #n_estimators=100 learning_rate:0.1 subsample:1.0 ccp_alpha:0.0 n_features\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[10],'min_samples_leaf':[10,15],'min_samples_split':[10,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[16],'min_samples_leaf':[10,100],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[18],'min_samples_leaf':[10,100],'max_features':[10,20]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[5,10,20],'min_samples_leaf':[100,200],'max_features':[5,10,15]}\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_child_weight':[1,3],'colsample_bytree':[0.5,0.75,1],\n",
    "                                  'subsample':[0.8,1],'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_child_weight':[1,3],'colsample_bytree':[0.5,0.75,1],\n",
    "                                  'subsample':[0.8,1],'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[18,36],'min_child_weight':[1,3],'colsample_bytree':[0.5,1]}\n",
    "                                  #'subsample':[0.8,1]} #'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='project'):param_grid={'n_estimators':[400,500,600],'learning_rate':[0.03,0.05,0.07],'max_depth':[6,15,30],\n",
    "             'min_child_weight':[0.1,1,3],'colsample_bytree':[0.5,0.75,0.95],'subsample':[0.5,0.7,0.9],'reg_alpha':[0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[128,160],'min_child_samples':[10,100,500],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,128,512],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[128,512],'min_child_samples':[10,100,500],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,128,512],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[128,512,1024],'min_child_samples':[10,100,1000],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,512,2048],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='project'):param_grid={'n_estimators':[400,500,600],'learning_rate':[0.03,0.05,0.07], 'min_child_samples':[100,150,200],\n",
    "              'colsample_bytree':[0.5,0.75,1],'num_leaves':[50,100,150],'subsample':[0.6,0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "                \n",
    "        # reg\n",
    "        elif model_name == 'LinearRegression':\n",
    "            param_grid  = {'n_jobs' : [-1, 20 ], 'fit_intercept' : [True, False]}\n",
    "        elif model_name == 'LinearSVR': # degree, C, kernal, epsilon, gamma , tol(허용오차) # degree 차원  # C 커지면 곡선 -> 직선 # gamma 커지면 과적합\n",
    "            param_grid ={'C': [1, 10], 'max_iter' : [100, 1000]}\n",
    "        elif model_name == 'SGDRegressor':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid ={'alpha': [0.0001,0.1],\"l1_ratio\":[0.01,0.5],'epsilon':[0.1,1],'max_iter':[100,500]}\n",
    "        elif model_name == 'Ridge': # L2규제(feature 가중치(베타)최소화)\n",
    "            param_grid ={'alpha': [0.01, 0.1, 1, 10, 100, 500], 'solver':['auto','svd']}\n",
    "        elif model_name == 'Lasso': # L1규제 (feature 가중치 제거)\n",
    "            param_grid ={'alpha': [0.01, 0.1, 1, 10, 100, 500], 'max_iter' : [100, 1000]}\n",
    "        elif model_name == 'ElasticNet': # if L1 a, if L2 b  => alpha = a+b, l1_ratio = a/(a+b)  # if l1_ratio=1 => a=1, b=0\n",
    "            param_grid ={'alpha': [0.1, 1, 10, 100, 500], \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "        elif model_name == 'DecisionTreeRegressor':\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[5,8,10],'min_samples_leaf':[4,8,12,18],'min_samples_split':[2,8,16,50]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[8,13,20],'min_samples_leaf':[5,10,30,50,100 ],'min_samples_split':[5,10,30,50,100]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[5,10,20],'min_samples_leaf':[5,10,20,50],'min_samples_split':[5,10,20,50]}            \n",
    "            if (data_name=='project'):param_grid={'max_depth':[15,22,30],'min_samples_leaf':[100,200,300],'min_samples_split':[200,500,1000]}\n",
    "        elif model_name == 'RandomForestRegressor': # cpu 병렬지원(n_jobs=-1, 100% 가동)\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[5,8,10],'min_samples_leaf':[4,8,12,18 ],'min_samples_split':[2,8,16,20,50]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[8,13,20],'min_samples_leaf':[5,10,30,50,100],'min_samples_split':[5,10,30,50,100]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[5,10,20],'min_samples_leaf':[5,10,20,50],'min_samples_split':[5,10,20,50]}\n",
    "        elif model_name == 'GradientBoostingRegressor': # param많으면 학습시간이 많이 걸림(순차 시행, cpu 1개만 수행)\n",
    "            if (data_name=='bostonHouse'):param_grid= {'max_depth':[8],'min_samples_leaf':[8,36],'min_samples_split':[8,36]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[13],'min_samples_leaf':[10,50],'min_samples_split':[10,50]}\n",
    "            if (data_name=='AmesHouse'):param_grid= {'max_depth':[10], 'min_samples_leaf':[10,20],'min_samples_split':[10,20]}\n",
    "        elif model_name == 'XGBRegressor': #min_child_samples없음 # num_leaves 없음 # cpu 병렬지원(n_jobs=-1, auto) # 시간소요\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[5,8,10],'min_child_weight':[0.1,1,5],'subsample':[0.6,1]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[5,13,20],'min_child_weight':[0.1,1,5],'subsample':[0.6,1]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[5,10,20],'min_child_weight':[0.1,1,5],'subsample':[0.6,0.8,1]}\n",
    "        elif model_name == 'LGBMRegressor':  # cpu 병렬지원(n_jobs=-1, 100% 가동) # 매우빠름\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[128,160],'min_child_samples':[20,60,100],'num_leaves':[32,64,128],'subsample':[0.6,0.8,1]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[64,128,256,512],'min_child_samples':[20,60,100],'num_leaves':[32,64,128],'subsample':[0.6,0.8,1]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[64,128,256],'min_child_samples':[20,60,100],'num_leaves':[32,64,128],'subsample':[0.6,0.8,1]}\n",
    "                \n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀모델을 위한 scaler # 표준정규분포(Standard), 최대값/최소값(MinMax), 로그변환(Log)\n",
    "# p_degree는 다항식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. \n",
    "def get_scaled_data(input_data=None, scaler=None, p_degree=None):\n",
    "    if scaler == 'Standard':\n",
    "        scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    elif scaler == 'MinMax':\n",
    "        scaled_data = MinMaxScaler().fit_transform(input_data)\n",
    "    elif scaler == 'Log':\n",
    "        scaled_data = np.log1p(input_data)\n",
    "    else:\n",
    "        scaled_data = input_data\n",
    "    if p_degree != None:\n",
    "        scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data)\n",
    "    return scaled_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
