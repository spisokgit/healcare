{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version : 0.23.2\n",
      "python version : 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sys\n",
    "print(f'sklearn version : {sklearn.__version__}')\n",
    "print(f'python version : {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "from IPython.display import HTML, display # jupyter 사진넣기\n",
    "import time                      # excution time 계산\n",
    "from datetime import datetime   # system time \n",
    "import json                     # json save\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import export_graphviz  # 결정트리 모형\n",
    "# import graphviz                         # 결정트리 모형\n",
    "from sklearn import tree                  # 결정트리 모형  # Classifier tree\n",
    "from sklearn.inspection import permutation_importance  # feature 중요도\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, precision_recall_curve, roc_curve)\n",
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import (VotingClassifier, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier,\n",
    "                                ExtraTreesClassifier, GradientBoostingClassifier)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import xgboost as xgb                # python wrapper\n",
    "from xgboost import XGBClassifier    # sklearn wrapper\n",
    "from lightgbm import LGBMClassifier  # sklearn wrapper\n",
    "\n",
    "import pickle                         # 모델 저장\n",
    "import matplotlib as mpl              # 한글깨짐\n",
    "import matplotlib.font_manager as fm  # 한글깨짐\n",
    "import matplotlib.font_manager        # 한글깨짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linux \n",
    "font_dirs = ['/home/sch/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf', ]\n",
    "font_files = fm.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = fm.createFontList(font_files)\n",
    "fm.fontManager.ttflist.extend(font_list)\n",
    "[f.name for f in matplotlib.font_manager.fontManager.ttflist if 'Nanum' in f.name][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 대한 성능평가지표개선을 위한 threshold 선택관련 plot\n",
    "def precision_recall_curve_plot(y_test, pos_proba, data_name=None, model_name=None,time_name=None ): # pred_proba positive값\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출.\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba)\n",
    "\n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    \n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary],linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n",
    "\n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n",
    "\n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.title(f\"Precision and Recall Trade off Curve {data_name} {model_name}\")\n",
    "    plt.xlabel('Threshold value)')\n",
    "    plt.ylabel('Precision and Recall value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f'./img/{data_name}_p_r_{model_name}_{time_name}.png', bbox_inches='tight')\n",
    "#         plt.close(fig)\n",
    "    plt.show()\n",
    "\n",
    "def roc_curve_plot(y_test, pos_proba, data_name, model_name=None,time_name=None): # pred_proba positive값\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    \n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    # 임곗값에 따른 FPR, TPR 값을 반환 받음.\n",
    "    fprs, tprs, thresholds = roc_curve(y_test, pos_proba)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    # ROC Curve를 plot 곡선으로 그림.\n",
    "    plt.plot(fprs, tprs, label='ROC')\n",
    "    # 가운데 대각선 직선을 그림.\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "    # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f'Model ROC Curve {data_name} {model_name}')\n",
    "    plt.xlabel('FPR( 1 - Sensitivity )')\n",
    "    plt.ylabel('TPR( Recall )')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./img/{data_name}_roc_{model_name}_{time_name}.png', bbox_inches='tight')\n",
    "#         plt.close(fig)\n",
    "    plt.show()\n",
    "\n",
    " # 결정트리 모형도 sklearn 0.23.1\n",
    "def Plot_tree_(data_name, model,model_name,time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    tree.plot_tree(model, filled=True)\n",
    "    plt.savefig(f\"./imgTree/{data_name}_tree_{model_name}_{time_name}.png\", bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    plt.show()\n",
    "    \n",
    " # 결정트리에서 feature_importances_ # gridcv에는 없음\n",
    "def Plot_feature_importance(data_name, model, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "\n",
    "    df = pd.Series(model.feature_importances_, index=indexs)\n",
    "    df_20 = df.sort_values(ascending=False)#[:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df_20, y=df_20.index)\n",
    "    plt.title(f'{model_name} feature_importance')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_{model_name}_{time_name}_FI.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show()    \n",
    "    \n",
    "# LogisticRegression에서 사용 coef_\n",
    "def Plot_coef_logit(data_name, model, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "#     importance = model.coef_[0]\n",
    "#     for i,v in enumerate(importance):\n",
    "#         print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "    df = pd.Series(model.coef_[0], index=indexs)\n",
    "    df_20 = df.sort_values(ascending=False)#[:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df_20, y=df_20.index)\n",
    "#     plt.bar([ x for x in range(len(importance))], importance)\n",
    "    plt.title(f'{model_name} 회귀계수(coef_)')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_{model_name}_{time_name}_Coef.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show() \n",
    "    \n",
    "def Plot_permutation_importance(data_name, importance, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "\n",
    "    df = pd.Series(importance, index=indexs)\n",
    "    df_20 = df.sort_values(ascending=False)#[:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df_20, y=df_20.index)\n",
    "    plt.title(f'{model_name} permutation_importance')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_{model_name}_{time_name}_PI.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습곡선\n",
    "def Plot_learning_curve(data_name, model, model_name, time_name, X, y, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "            model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title(f'{data_name} {model_name} learning curve')\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training \")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(f\"./imgLearnCurve/{data_name}_l_c_{model_name}_{time_name}.png\", bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results_데이터로 GridSearchCV 곡선\n",
    "def plot_grid_curve(model, grid_params, data_name, model_name,time_name):\n",
    "    df = pd.DataFrame(model.cv_results_)\n",
    "    results = ['mean_test_score','mean_train_score'] \n",
    "    fig, axes = plt.subplots(1, len(grid_params), figsize = (3*len(grid_params), 4) )  # sharey='row'\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=10)\n",
    "    for idx, (param_name, param_range) in enumerate(grid_params.items()):\n",
    "        grouped_df = df.groupby(f'param_{param_name}')[results].agg({'mean_train_score': 'mean', 'mean_test_score': 'mean'})\n",
    "        previous_group = df.groupby(f'param_{param_name}')[results]\n",
    "        axes[idx].set_xlabel(param_name, fontsize=10)\n",
    "#         axes[idx].set_ylim(0.0, 1.1)\n",
    "        lw = 2 # 선의 굵기\n",
    "        axes[idx].plot(param_range, grouped_df['mean_train_score'], label=\"Training score\", color=\"darkorange\", lw=lw)\n",
    "        axes[idx].plot(param_range, grouped_df['mean_test_score'], label=\"Cross-validation score\",  color=\"navy\", lw=lw)\n",
    "#     handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(f'{data_name} {model_name} Validation curves', fontsize=20)\n",
    "    plt.legend(loc = \"best\")\n",
    "    fig.savefig(f\"./imgGridCurve/{data_name}_g_v_{model_name}_{time_name}.png\", bbox_inches='tight')\n",
    "    fig.subplots_adjust(bottom=0.25, top=0.85)  \n",
    "#     plt.close(fig)\n",
    "    plt.tight_layout()\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  metric 함수\n",
    "#### 2-1 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중분류에서는 confusion_matrix, accuracy, roc_auc 만 나온다\n",
    "def get_clf_eval_poly(y_test, pred, pos_proba=None):\n",
    "    metrics_dict={}\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    metrics_dict['confusion_matrix'] = confusion.tolist() # confusion nd.array  --> list\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    roc_auc = roc_auc_score(y_test, pos_proba) \n",
    "    metrics_dict['auc_score'] = auc_score\n",
    "    print(f'오차 행렬 :\\n {confusion}')\n",
    "    print(f\" 정확도: {accuracy:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중분류에서는 confusion_matrix, accuracy만 나온다\n",
    "# 이진분류에서 성능평가지표 종합 함수\n",
    "def get_clf_eval(y_test=None, pred=None, pos_proba=None): # 권철민 저 책자에 누락 pred_proba\n",
    "    metrics_dict={}\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    metrics_dict['confusion_matrix'] = confusion.tolist()\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    precision = precision_score(y_test , pred)\n",
    "    metrics_dict['precision'] = precision\n",
    "    recall = recall_score(y_test , pred )     \n",
    "    metrics_dict['recall'] = recall\n",
    "    f1 = f1_score(y_test, pred )\n",
    "    metrics_dict['f1'] = f1\n",
    "    roc_auc = roc_auc_score(y_test, pos_proba) # 권철민 저 책자에 오기 pred --> pred_proba  # average='macro'\n",
    "    metrics_dict['roc_auc'] = roc_auc\n",
    "    # print(f'오차 행렬 :\\n {confusion}')\n",
    "    # print(f\" 정확도: {accuracy:.4f}, 정밀도: {precision:.4f}, 재현율: {recall:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    print(f\" 정확도: {accuracy:.4f}, 정밀도: {precision:.4f}, 재현율: {recall:.4f}, f1_score: {f1:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. model train, metric호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_params()관련 LGBM기준 (XGB유사) # sklearn version별 api 다름\n",
    "# max_depth log2(데이터수)  # pima 768 => 10  # customer 76020 => 16 # credit 284807 => 18 # project 797만=>22\n",
    "# num_leaves  : 2^(max_depth) 작아야 함  최대 리프노드 갯수 = max_leaf_nodes(dt, rf)\n",
    "# min_child_samples : Leaf node가 되기 위한 최소한 데이터 개체수  = min_samples_leaf  # min_samples_split (dt,rf 분리 최소 데이터)\n",
    "# min_child_weight = min_child_leaf (dt, rf) *gbm은 없음    \n",
    "# subsample (gbm,xgb,lgbm) 데이터 샘플링 비율 *lgbm은 subsample_for_bin 20000, subsample_freq 0\n",
    "# colsample_bytree = max_features (dt, rf, gbm) 개별 트리를 학습할 때마다 무작위로 선택하는 feature 비율을 제어 (feature 많을 경우) * \n",
    "    # XGB : colsample_bylevel, colsample_bynode, colsample_bytree  # pima 8 # customer 371 # credit 31  # project 25\n",
    "# reg_alpha : L1 규제(제거) # reg_lambda : L2 규제(최소화) \n",
    "# 영혼까지 learning_rate 내리고,  n_estimators(rf,gbm,xgb,lgbm) 올리고\n",
    "def get_params(model_name, data_name):  # param_grid 2개항목 이상일때만 plot_grid_curve 가능\n",
    "    if (grid == 2) : # RandomSearch    속도느링 LogisticRegression  GradientBoostingClassifier\n",
    "        if model_name == 'LogisticRegression': # 속도느링 #\"solver\":['lbfgs','auto']\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e0),'l1_ratio': stats.uniform(0, 1), \"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e0),\"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid = { 'alpha':stats.loguniform(1e-4,1e0),'l1_ratio':stats.uniform(0,1),'average':[True,False],\n",
    "                            'epsilon':stats.loguniform(1e-1, 1e1),'max_iter':stats.randint[100,500]}\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            if(data_name=='project'):param_grid = {\"max_depth\": stats.randint(5, 50), \"max_features\": stats.randint(2, 24),\n",
    "                \"min_samples_split\": stats.randint(10, 100), \"min_samples_leaf\": stats.randint(10, 100),\"criterion\": [\"gini\", \"entropy\"]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            if(data_name=='project'):param_grid = {\"max_depth\":  stats.randint(5, 50),\"max_features\":stats.randint(2, 24),\n",
    "            \"min_samples_split\":stats.randint(10,100),\"min_samples_leaf\":stats.randint(10,100),\"bootstrap\":[True,False],\"criterion\":[\"gini\",\"entropy\"]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림 #n_estimators=100 learning_rate:0.1 subsample:1.0 ccp_alpha:0.0 n_features\n",
    "            if(data_name=='project'):param_grid={ 'max_depth' : stats.randint(5, 50),'min_samples_leaf': stats.randint(10,500),\n",
    "                                                   'max_features':  stats.randint(5,24) }\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            if(data_name=='project'):param_grid={'n_estimators': stats.randint(200, 1000),'learning_rate': stats.uniform(0.01, 0.6),\n",
    "            'subsample':stats.uniform(0.3, 0.9),\"max_depth\":stats.randint(5,50),'colsample_bytree':stats.uniform(0.5,0.9),'min_child_weight':stats.loguniform(1e-3, 1e3) }\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            if(data_name=='project'):param_grid={'n_estimators': stats.randint(200, 1000),'learning_rate': stats.uniform(0.01, 0.6),\n",
    "            'min_child_samples':stats.randint(10,500),'colsample_bytree':stats.uniform(loc=0.4,scale=0.6),'num_leaves': stats.randint(32,1000),\n",
    "                                    'subsample': stats.uniform(loc=0.2, scale=0.8), 'min_child_weight': stats.loguniform(1e-3, 1e3),\n",
    "                                      'reg_alpha': stats.loguniform(1e-3, 1e2),'reg_lambda': stats.loguniform(1e-3, 1e3)}\n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid\n",
    "\n",
    "    elif (grid == 3) : #GridSearch\n",
    "        if model_name == 'LogisticRegression': # \"solver\":['lbfgs','auto']\n",
    "            param_grid  = {'C': [0.1,0.2,0.3],\"l1_ratio\": [0.4,0.6,0.8],\"max_iter\":[400,500,600]}\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid ={'C': [0.2,0.3,0.4],\"max_iter\":[200,300,400] }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid ={'alpha': [0.0001,0.01,0.1],\"l1_ratio\":[0.1,0.2,0.3],'epsilon':[0.1,1,10],'max_iter':[100,300,500]} # 빠름\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_samples_leaf':[10,100,500],'min_samples_split':[10,15,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_samples_leaf':[10,16,20],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[12,18,24],'min_samples_leaf':[10,50,100],'max_features':[100,500,1000]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[15,20,25],'min_samples_leaf':[50,100,150],'max_features':[4,8,12]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_samples_leaf':[10,100,500],'min_samples_split':[10,15,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_samples_leaf':[10,100,500],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[12,18,24],'min_samples_leaf':[10,50,100],'max_features':[100,500,1000]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[15,20,25],'min_samples_leaf':[20,40,60],'min_samples_split':[20,40,60]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림 #n_estimators=100 learning_rate:0.1 subsample:1.0 ccp_alpha:0.0 n_features\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[10],'min_samples_leaf':[10,15],'min_samples_split':[10,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[16],'min_samples_leaf':[10,100],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[18],'min_samples_leaf':[10,100],'max_features':[10,20]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[5,10,20],'min_samples_leaf':[100,200],'max_features':[5,10,15]}\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_child_weight':[1,3],'colsample_bytree':[0.5,0.75,1],\n",
    "                                  'subsample':[0.8,1],'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_child_weight':[1,3],'colsample_bytree':[0.5,0.75,1],\n",
    "                                  'subsample':[0.8,1],'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[18,36],'min_child_weight':[1,3],'colsample_bytree':[0.5,1]}\n",
    "                                  #'subsample':[0.8,1]} #'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='project'):param_grid={'n_estimators':[400,500,600],'learning_rate':[0.03,0.05,0.07],'max_depth':[6,15,30],\n",
    "             'min_child_weight':[0.1,1,3],'colsample_bytree':[0.5,0.75,0.95],'subsample':[0.5,0.7,0.9],'reg_alpha':[0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[128,160],'min_child_samples':[10,100,500],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,128,512],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[128,512],'min_child_samples':[10,100,500],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,128,512],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[128,512,1024],'min_child_samples':[10,100,1000],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,512,2048],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='project'):param_grid={'n_estimators':[400,500,600],'learning_rate':[0.03,0.05,0.07], 'min_child_samples':[100,150,200],\n",
    "              'colsample_bytree':[0.5,0.75,1],'num_leaves':[50,100,150],'subsample':[0.6,0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "                \n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 학습/테스트 데이터 셋을 입력하면 학습 -> GridSerchCV 수행 + 성능 평가(metric) 등 반환\n",
    "def get_model_train_eval(model,model_name,data_name,time_name,X_train,X_test,y_train,y_test,eval_X=None,eval_y=None,is_expm1=False):\n",
    "# def get_model_train_eval(model,model_name,data_name,time_name,X_train,X_test,y_train,y_test,is_expm1=False):\n",
    "    print('###',model.__class__.__name__,'###')\n",
    "    if (grid == 2) :  # RandomSearchCV\n",
    "        param_grid  = get_params(model_name, data_name)\n",
    "        print(f'RandomSearchCV 입력 파라미터:{param_grid}')\n",
    "        # 회귀, 분류에 따라 scoring 변경 # 분류 : accuracy, f1, neg_log_loss, roc_auc\n",
    "        model=RandomizedSearchCV(model,param_distributions=param_grid,scoring='roc_auc',return_train_score=True,n_iter=5)\n",
    "        model.fit(X_train, y_train) # 랜덤포레스트는 early_stopping_rounds없음 * xgb, lgbm, sgb만 있음\n",
    "#         model.fit(X_train, y_train,early_stopping_rounds=10,eval_metric=\"logloss\",eval_set=[(eval_X,eval_y)],verbose=True)\n",
    "        best_params = model.best_params_\n",
    "        print(f'RandomSearchCV 최적 파라미터:{ best_params}\\n')\n",
    "        # grid_curve plot \n",
    "#         plot_grid_curve(model, param_grid, data_name, model_name, time_name)\n",
    "        # best_estimator_\n",
    "        model = model.best_estimator_\n",
    "        pred = model.predict(X_test)\n",
    "        if model_name == 'LinearSVC':\n",
    "            pred_proba = model._predict_proba_lr(X_test) \n",
    "        else:\n",
    "            pred_proba = model.predict_proba(X_test)   \n",
    "        if is_expm1 : #is_expm1 <= log변환시 복원\n",
    "            y_test = np.expm1(y_test)\n",
    "            pred = np.expm1(pred)  \n",
    "        metrics_dict = get_clf_eval(y_test, pred, pred_proba[:,1])\n",
    "        spend_metric_time=(time.time() - start_model_time)\n",
    "        print(f'{model_name} metric 수행시간 : {int(spend_metric_time//60)}분 {spend_metric_time%60:.2f}초\\n')\n",
    "        if (plot == 1) :\n",
    "            Plot_learning_curve(data_name,model,model_name,time_name,X_train,y_train,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5))\n",
    "            precision_recall_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name )\n",
    "            roc_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name)\n",
    "            if model_name in ['LogisticRegression','SGDClassifier','LinearSVC']:\n",
    "                Plot_coef_logit(data_name, model, X_train.columns, model_name, time_name) # 회귀계수 \n",
    "            else : \n",
    "    # [KNeighborsClassifier,DecisionTreeClassifier,RandomForestClassifier,GradientBoostingClassifier,XGBClassifier,LGBMClassifier]:\n",
    "                Plot_feature_importance(data_name, model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "            # permutation_importance\n",
    "            results = permutation_importance(model, X_test, y_test, scoring='f1')\n",
    "            importance = results.importances_mean\n",
    "            Plot_permutation_importance(data_name, importance, X_train.columns, model_name, time_name)\n",
    "        return model, best_params, metrics_dict\n",
    "\n",
    "    elif (grid == 3) : # GridSearchCV\n",
    "        param_grid  = get_params(model_name, data_name)\n",
    "        print(f'GridSearchCV 입력 파라미터:{param_grid}')\n",
    "        # 회귀, 분류에 따라 scoring 변경 # 분류 : accuracy, f1, neg_log_loss, roc_auc\n",
    "        model = GridSearchCV(model, param_grid=param_grid, scoring='roc_auc', return_train_score=True) # cv=2 최소, default 5\n",
    "#         gridcv = GridSearchCV(model, param_grid=param_grid, scoring=None, return_train_score=True)\n",
    "#         model.fit(X_train, y_train) # 랜덤포레스트는 early_stopping_rounds없음 * xgb, lgbm, sgb만 있음\n",
    "        model.fit(X_train, y_train,early_stopping_rounds=10,eval_metric=\"logloss\",eval_set=[(eval_X,eval_y)],verbose=True)\n",
    "        best_params = model.best_params_\n",
    "        print(f'GridSearchCV 최적 파라미터:{ best_params}')\n",
    "                                                 \n",
    "        plot_grid_curve(model, param_grid, data_name, model_name, time_name)\n",
    "        # best_estimator_\n",
    "        model = model.best_estimator_\n",
    "        pred = model.predict(X_test)\n",
    "        if model_name == 'LinearSVC':\n",
    "            pred_proba = model._predict_proba_lr(X_test) \n",
    "        else:\n",
    "            pred_proba = model.predict_proba(X_test)   # LinearSVC 없음\n",
    "        if is_expm1 : #is_expm1 <= log변환시 복원\n",
    "            y_test = np.expm1(y_test)\n",
    "            pred = np.expm1(pred)  \n",
    "        metrics_dict = get_clf_eval(y_test, pred, pred_proba[:,1])\n",
    "        spend_metric_time=(time.time() - start_model_time)\n",
    "        print(f'{model_name} metric 수행시간 : {int(spend_metric_time//60)}분 {spend_metric_time%60:.2f}초\\n')  \n",
    "        if (plot == 1) :\n",
    "            Plot_learning_curve(data_name,model,model_name,time_name,X_train,y_train,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5))\n",
    "            precision_recall_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name )\n",
    "            roc_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name)\n",
    "            if model_name in ['LogisticRegression','SGDClassifier','LinearSVC']:\n",
    "                Plot_coef_logit(data_name, model, X_train.columns, model_name, time_name) # 회귀계수 \n",
    "            else : \n",
    "        # [KNeighborsClassifier,DecisionTreeClassifier,RandomForestClassifier,GradientBoostingClassifier,XGBClassifier,LGBMClassifier]\n",
    "                Plot_feature_importance(data_name, model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "            results = permutation_importance(model, X_test, y_test, scoring='f1')\n",
    "            importance = results.importances_mean\n",
    "            Plot_permutation_importance(data_name, importance, X_train.columns, model_name, time_name)\n",
    "        return model, param_grid, best_params, metrics_dict\n",
    "\n",
    "    else: # grid = 1, 4\n",
    "        model.fit(X_train, y_train)\n",
    "    # early_stopping_rounds 기능은 xgboot LGBM ==> eval_metric: auc(bin only) \"logloss\":Negative lig-likehood multiclass \"mlogloss\",error,merror\n",
    "#     model.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=[(X_train, y_train), (X_test, y_test)], verbose=True)\n",
    "#     model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=[(eval_X, eval_y)], verbose=True)\n",
    "        ##########################################    \n",
    "        # model 예측(에러) 확률    # 평가지표(metrics)을 위한 negitive, positive 확률로 변환\n",
    "        pred = model.predict(X_test)\n",
    "        if model_name == 'LinearSVC':\n",
    "            pred_proba = model._predict_proba_lr(X_test) \n",
    "        else:\n",
    "            pred_proba = model.predict_proba(X_test)   # LinearSVC 없음\n",
    "        if is_expm1 : #target log1p 변환시 is_expm1 <= log변환시 복원\n",
    "            y_train = np.expm1(y_train)\n",
    "            y_test = np.expm1(y_test)\n",
    "            pred = np.expm1(pred)  \n",
    "        metrics_dict = get_clf_eval(y_test, pred, pred_proba[:,1])\n",
    "        spend_metric_time=(time.time() - start_model_time)\n",
    "        print(f'{model_name} metric 수행시간 : {int(spend_metric_time//60)}분 {spend_metric_time%60:.2f}초\\n')  \n",
    "        if (plot == 1) :\n",
    "            Plot_learning_curve(data_name,model,model_name,time_name,X_train,y_train,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5))\n",
    "            precision_recall_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name )\n",
    "            roc_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name)\n",
    "            if model_name in ['LogisticRegression','SGDClassifier','LinearSVC']:\n",
    "                Plot_coef_logit(data_name, model, X_train.columns, model_name, time_name) # 회귀계수 \n",
    "            else : \n",
    "    # [KNeighborsClassifier,DecisionTreeClassifier,RandomForestClassifier,GradientBoostingClassifier,XGBClassifier,LGBMClassifier]:\n",
    "                Plot_feature_importance(data_name, model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "            results = permutation_importance(model, X_test, y_test, scoring='f1')\n",
    "            importance = results.importances_mean\n",
    "            Plot_permutation_importance(data_name, importance, X_train.columns, model_name, time_name)\n",
    "        return model, metrics_dict        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data preprocessing, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 데이터 가공\n",
    "def get_preprocessed_df(df=None):\n",
    "    ## \n",
    "    df_copy = df.copy()\n",
    "#     df_copy['target'] = df_copy['식전혈당(공복혈당)'].apply(lambda x: 1 if x >= 126 else ( 2 if 100 <= x < 126 else 3))\n",
    "#     df_copy['target'] = df_copy['식전혈당(공복혈당)'].apply(lambda x: 1 if x >= 126 else 0 )\n",
    "#     df_copy.drop('식전혈당(공복혈당)', axis=1, inplace=True)    \n",
    "#     df_copy['연령'] =df_copy['연령대코드(5세단위)'].apply( lambda x: x*5 + 17 )\n",
    "#     df_copy.drop('연령대코드(5세단위)', axis=1, inplace=True)\n",
    "#     target_count = df_copy[ df_copy['target'] == 1 ].shape[0]\n",
    "#     not_target_count = df_copy[ df_copy['target'] != 1 ].shape[0]\n",
    "#     frac=round( (target_count/not_target_count), 3)\n",
    "#     df_target = df_copy[ df_copy['target'] == 1 ]\n",
    "#     df_X =  df_copy[ df_copy['target'] != 1 ].sample(frac=frac, random_state=1)\n",
    "#     df_copy = pd.concat ([df_target, df_X]).sort_values(by=['기준년도'])\n",
    "#     df_copy.drop(['기준년도','시도코드'], axis=1, inplace=True)    \n",
    "#     df_copy = sklearn.utils.shuffle(df)\n",
    "#     print(df_copy[df_copy['target'] == 1].shape[0])\n",
    "#     print(df_copy[df_copy['target'] != 1].shape[0]) \n",
    "    ## 범주형 drop # 범주형 feature 10개\n",
    "#     category_features = ['성별코드','청력(좌)','청력(우)', '요단백', '흡연상태', '음주여부', '구강검진수검여부']\n",
    "#     df_copy.drop(category_features, axis=1, inplace=True)\n",
    "    df_copy = pd.get_dummies(df_copy)\n",
    "    return df_copy\n",
    "\n",
    "# 1.필요시 전처리 리턴 => 2. X,y(target)분리 => 3. 필요시 scaling => 4. train,test로 split\n",
    "def get_train_test_dataset(df=None):\n",
    "    # 전체 df 전처리 필요시\n",
    "    df = get_preprocessed_df(df)\n",
    "    \n",
    "    ## target과 train 데이터 분리\n",
    "    # pima\n",
    "#     y_target = df['Outcome']\n",
    "#     X_features = df.drop('Outcome', axis=1)\n",
    "    # customer(bank)\n",
    "#     y_target = df['TARGET']\n",
    "#     X_features = df.drop('TARGET', axis=1)    \n",
    "\n",
    "    # credit\n",
    "#     y_target = df['Class']\n",
    "#     X_features = df.drop('Class', axis=1)    \n",
    "#     scaler = StandardScaler()\n",
    "#     amount_n = scaler.fit_transform(X_features['Amount'].values.reshape(-1, 1))\n",
    "#     amount_n = np.log1p(X_features['Amount'])\n",
    "#     X_features.insert(0, 'Amount_Scaled', amount_n)\n",
    "#     X_features.drop(['Amount'], axis=1, inplace=True)\n",
    "    \n",
    "#     X_features = scaler.fit_transform(X_features)\n",
    "#     X_features = np.log1p(X_features)\n",
    "\n",
    "    #project\n",
    "    y_target = df['target']\n",
    "    X_features = df.drop('target', axis=1)\n",
    "    columns = X_features.columns\n",
    "#     scaler = MinMaxScaler()\n",
    "#     scaler = StandardScaler()\n",
    "#     X_features = scaler.fit_transform(X_features)\n",
    "    X_features = np.log1p(X_features)\n",
    "    X_features = pd.DataFrame(X_features, columns=columns)\n",
    "    \n",
    "    ## train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156,stratify=y_target)\n",
    "    # xgb,lgbm,sgb early_stopping_rounds 사용시 ==> test 데이터 eval_set 으로 다시 분리  # train, test 분리 0.3 필요\n",
    "#     X_test, eval_X, y_test, eval_y = train_test_split(X_test, y_test, test_size=0.3, random_state=256, stratify=y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "#     return X_train, X_test, y_train, y_test, eval_X, eval_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. data 및 model selection, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수행 시간 측정을 위함. 시작 시간 설정.\n",
    "start_time = time.time()\n",
    "\n",
    "# data_name = \"breast_cancer\"\n",
    "# data_name = \"pima\"\n",
    "# data_name = \"customer\"\n",
    "# data_name = \"credit\"\n",
    "data_name = \"project104(1_1)LogOhe\"\n",
    "\n",
    "grid = 1      # 초기모델링    # 초기 model_list, # target scaler했다면 is_expm1=False, True 확인\n",
    "# grid = 2     # RandomSearchCV 수행    # params 조정\n",
    "# grid = 3     # GridSearchCV 수행     # params 조정\n",
    "# grid = 4    # best params 적용  # 해당 model_list\n",
    "\n",
    "plot = 0    # plot  disable 수행시간때문에\n",
    "# plot = 1  # plot enable \n",
    "\n",
    "def run():    \n",
    "    ## data 데이터 선택 ########################################    \n",
    "#     df = dataset()\n",
    "#     df = pima_df\n",
    "#     df = cust_df\n",
    "#     df = credit_df\n",
    "    # project\n",
    "#     df = pd.read_csv('./data/NHIS_total_model.csv')\n",
    "    df = pd.read_csv('./data/NHIS_model_1.csv')\n",
    "#     df = df.iloc[:1000, :]\n",
    "    target_count = df[df['target'] == 1].shape[0]\n",
    "    target_ex_count = df[df['target'] != 1].shape[0]\n",
    "    print(f\"target   count : {target_count} ({ round( target_count/df.shape[0], 4)*100 })%\")\n",
    "    print(f\"target외 count : {target_ex_count} ({ round( target_ex_count/df.shape[0], 4)*100 })%\")\n",
    "\n",
    "   ## 1. 데이터 전처리(preprocessing), 분리(split) ##########################################\n",
    "   # 랜덤 포레스트\n",
    "#     X_train, X_test, y_train, y_test = get_human_dataset( )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n",
    "#     X_train, X_test, y_train, y_test, eval_X, eval_y = get_train_test_dataset(df)\n",
    " \n",
    "    ## model 선택 ###########################  LinearSVC() 개별에서\n",
    "      # KNeighborsClassifier(n_jobs=-1), learning_curve loop\n",
    "      # [ SVC(random_state=1, kernel=\"linear\", C=1, probability=True)] # 시간 많이 소요\n",
    "    model_list= [LogisticRegression(random_state=1, n_jobs=-1),\n",
    "                 LinearSVC(random_state=1, loss='hinge'),\n",
    "                 SGDClassifier(random_state=1,n_jobs=-1, loss='modified_huber'), \n",
    "                 DecisionTreeClassifier(random_state=1),\n",
    "                 RandomForestClassifier(random_state=1,n_jobs=-1),\n",
    "        GradientBoostingClassifier(random_state=1,validation_fraction=0.1,n_iter_no_change=20,subsample=0.25), # xgb,lgbm:fit조기중단가능\n",
    "                 XGBClassifier(random_state=1,n_jobs=-1,silent=True,device='gpu'),\n",
    "                 LGBMClassifier(random_state=1,n_jobs=-1) ]\n",
    "\n",
    "#     model_list= [\n",
    "#         LogisticRegression(random_state=1,n_jobs=-1,C=0.1,l1_ratio=0.1,max_iter=100,solver='lbfgs'),\n",
    "#         KNeighborsClassifier(n_jobs=-1,n_neighbors=10,p=1),\n",
    "#         SGDClassifier(random_state=1,n_jobs=-1,loss='modified_huber',alpha=0.0001,epsilon=0.1,eta0=0.0,l1_ratio=0.01,max_iter=100), \n",
    "#         DecisionTreeClassifier(random_state=1,max_depth=5,max_features=8,min_samples_leaf=20,min_samples_split=10),\n",
    "#         RandomForestClassifier(random_state=1,n_jobs=-1,max_depth=5,max_features=5,min_samples_leaf=15,min_samples_split=10),\n",
    "#         GradientBoostingClassifier(random_state=1,max_depth=10,max_features=5,min_samples_leaf=10,min_samples_split=10),\n",
    "#         XGBClassifier(random_state=1,n_jobs=-1,silent=True,colsample_bytree=0.5,max_depth=15,min_child_weight=1,reg_alpha=0,reg_lambda=0.5,subsample=1),\n",
    "#         LGBMClassifier(random_state=1,n_jobs=-1,colsample_bytree=1,max_depth=128,min_child_samples=5,num_leaves=64,reg_alpha=0,reg_lambda=0.5,subsample=0.8)\n",
    "#     ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMetricSave(model_list)\n",
    "    time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    for model in model_list:\n",
    "        start_model_time = time.time()\n",
    "        model_name = model.__class__.__name__\n",
    "        if(grid == 1 or grid == 4): # GridSerchCV 없이 초기 2. ~ 3. model 훈련, metric 리턴받아 처리 \n",
    "            model, metrics_dict = \\\n",
    "                get_model_train_eval(model,model_name,data_name,time_name,X_train,X_test,y_train,y_test)\n",
    "        elif(grid == 2): # RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "            model, best_params, metrics_dict =\\\n",
    "                        get_model_train_eval(model, model_name,data_name,time_name, X_train, X_test, y_train, y_test)\n",
    "        elif(grid == 3): # GridSerchCV, RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "            model, param_grid, best_params, metrics_dict =\\\n",
    "                        get_model_train_eval(model, model_name,data_name,time_name, X_train, X_test, y_train, y_test)\n",
    "        ## metrics json 저장 \n",
    "        metrics_total_dict = {}\n",
    "        spend_model_time=(time.time() - start_model_time)\n",
    "        metrics_total_dict['Experiment_date_time'] = time_name\n",
    "        metrics_total_dict['data_name'] = data_name\n",
    "        metrics_total_dict['model_name'] = model_name\n",
    "        if(param_grid):        \n",
    "            metrics_total_dict['param_grid'] = param_grid \n",
    "        if(best_params):        \n",
    "            metrics_total_dict['best_params'] = best_params\n",
    "        metrics_total_dict['model_params'] = model.get_params()\n",
    "        metrics_total_dict['metrics'] =metrics_dict\n",
    "        metrics_total_dict['Execution_time'] = spend_model_time\n",
    "        with open(f'./json/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "            json.dump(metrics_total_dict, f, indent=4)            \n",
    "        print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n",
    "        if(best_params):\n",
    "            params = 'random_state=1'\n",
    "            for key, value in best_params.items():\n",
    "                params = params + ',' + str(key) + '=' + str(value)\n",
    "    #             print(f'{model_name} params : {params}\\n')\n",
    "            params_dict[f'{model_name}']=params\n",
    "    spend_time=(time.time() - start_time)\n",
    "    print(f'전체수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')  \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target   count : 521858 (50.0)%\n",
      "target외 count : 521754 (50.0)%\n",
      "### LogisticRegression ###\n",
      " 정확도: 0.7138, 정밀도: 0.7038, 재현율: 0.7385, f1_score: 0.7208, auc_score:0.7853\n",
      "LogisticRegression metric 수행시간 : 1분 10.39초\n",
      "\n",
      "LogisticRegression 수행시간 : 1분 6.57초\n",
      "\n",
      "### LinearSVC ###\n",
      " 정확도: 0.7144, 정밀도: 0.7060, 재현율: 0.7351, f1_score: 0.7203, auc_score:0.7857\n",
      "LinearSVC metric 수행시간 : 5분 0.17초\n",
      "\n",
      "LinearSVC 수행시간 : 3분 49.77초\n",
      "\n",
      "### SGDClassifier ###\n",
      " 정확도: 0.7106, 정밀도: 0.7164, 재현율: 0.6973, f1_score: 0.7067, auc_score:0.7844\n",
      "SGDClassifier metric 수행시간 : 5분 16.19초\n",
      "\n",
      "SGDClassifier 수행시간 : 0분 16.02초\n",
      "\n",
      "### DecisionTreeClassifier ###\n",
      " 정확도: 0.6334, 정밀도: 0.6344, 재현율: 0.6297, f1_score: 0.6320, auc_score:0.6334\n",
      "DecisionTreeClassifier metric 수행시간 : 5분 35.90초\n",
      "\n",
      "DecisionTreeClassifier 수행시간 : 0분 19.71초\n",
      "\n",
      "### RandomForestClassifier ###\n",
      " 정확도: 0.7289, 정밀도: 0.7101, 재현율: 0.7738, f1_score: 0.7405, auc_score:0.8001\n",
      "RandomForestClassifier metric 수행시간 : 6분 8.86초\n",
      "\n",
      "RandomForestClassifier 수행시간 : 0분 32.96초\n",
      "\n",
      "### GradientBoostingClassifier ###\n",
      " 정확도: 0.7287, 정밀도: 0.7073, 재현율: 0.7805, f1_score: 0.7421, auc_score:0.8019\n",
      "GradientBoostingClassifier metric 수행시간 : 7분 45.05초\n",
      "\n",
      "GradientBoostingClassifier 수행시간 : 1분 36.18초\n",
      "\n",
      "### XGBClassifier ###\n",
      " 정확도: 0.7278, 정밀도: 0.7054, 재현율: 0.7827, f1_score: 0.7420, auc_score:0.8014\n",
      "XGBClassifier metric 수행시간 : 7분 53.24초\n",
      "\n",
      "XGBClassifier 수행시간 : 0분 8.20초\n",
      "\n",
      "### LGBMClassifier ###\n",
      " 정확도: 0.7363, 정밀도: 0.7163, 재현율: 0.7828, f1_score: 0.7481, auc_score:0.8095\n",
      "LGBMClassifier metric 수행시간 : 7분 57.48초\n",
      "\n",
      "LGBMClassifier 수행시간 : 0분 4.23초\n",
      "\n",
      "전체수행시간 : 7분 57.48초\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    ##  2. data split ~ 3. 훈련, metric  ###################################################\n",
    "    if (grid == 1): # GridSerchCV 없이 초기 2. ~ 3. model 훈련, metric 리턴받아 처리 ####\n",
    "        time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        for model in model_list:\n",
    "            start_model_time = time.time()\n",
    "            model_name = model.__class__.__name__\n",
    "            model, metrics_dict = \\\n",
    "                get_model_train_eval(model,model_name,data_name,time_name,X_train,X_test,y_train,y_test)\n",
    "            ## metrics json 저장 \n",
    "            metrics_total_dict = {}\n",
    "            model_params = model.get_params() # dict\n",
    "            spend_model_time=(time.time() - start_model_time)\n",
    "            metrics_total_dict['Experiment_date_time'] = time_name\n",
    "            metrics_total_dict['data_name'] = data_name\n",
    "            metrics_total_dict['model_name'] = model_name\n",
    "            metrics_total_dict['model_params'] = model_params\n",
    "    #         metrics_total_dict['scaler'] = scaler\n",
    "    #         metrics_total_dict['p_degree'] = p_degree\n",
    "            metrics_total_dict['metrics'] =metrics_dict\n",
    "    #         metric_dict['best_params'] = best_params\n",
    "            metrics_total_dict['Execution_time'] = spend_model_time\n",
    "            with open(f'./json1/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "                json.dump(metrics_total_dict, f, indent=4)            \n",
    "            print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n",
    "        spend_time=(time.time() - start_time)\n",
    "        print(f'전체수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')  \n",
    "        \n",
    "    elif (grid == 2): # RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "        time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        params_dict={}\n",
    "        for model in model_list:\n",
    "            start_model_time = time.time()\n",
    "            model_name = model.__class__.__name__\n",
    "            model, best_params, metrics_dict =\\\n",
    "                        get_model_train_eval(model, model_name,data_name,time_name, X_train, X_test, y_train, y_test)\n",
    "            ## metrics json 저장 \n",
    "            metrics_total_dict = {}\n",
    "            spend_model_time=(time.time() - start_model_time)\n",
    "            metrics_total_dict['Experiment_date_time'] = time_name\n",
    "            metrics_total_dict['data_name'] = data_name\n",
    "            metrics_total_dict['model_name'] = model_name\n",
    "#             metrics_total_dict['param_grid'] = param_grid   # gridsearchcv get_params() ==> json 저장에러\n",
    "#             metrics_total_dict['best_params'] = best_params # gridsearchcv get_params()  ==> json 저장에러\n",
    "            metrics_total_dict['model_params'] = model.get_params()\n",
    "    #         metrics_total_dict['scaler'] = scaler\n",
    "    #         metrics_total_dict['p_degree'] = p_degree\n",
    "            metrics_total_dict['metrics'] =metrics_dict\n",
    "            metrics_total_dict['Execution_time'] = spend_model_time\n",
    "            with open(f'./json2RandomSearch/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "                json.dump(metrics_total_dict, f, indent=4)            \n",
    "            print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n",
    "            # best_params에 대한 model별 params \n",
    "            params = 'random_state=1'\n",
    "            for key, value in best_params.items():\n",
    "                params = params + ',' + str(key) + '=' + str(value)\n",
    "    #             print(f'{model_name} params : {params}\\n')\n",
    "            params_dict[f'{model_name}']=params\n",
    "        spend_time=(time.time() - start_time)\n",
    "        print(f'전체 수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')\n",
    "        print(params_dict)\n",
    "       \n",
    "    elif (grid == 3 ): # GridSerchCV, RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "        time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        params_dict={}\n",
    "        for model in model_list:\n",
    "            start_model_time = time.time()\n",
    "            model_name = model.__class__.__name__\n",
    "            model, param_grid, best_params, metrics_dict =\\\n",
    "                        get_model_train_eval(model, model_name,data_name,time_name, X_train, X_test, y_train, y_test)\n",
    "            ## metrics json 저장 \n",
    "            metrics_total_dict = {}\n",
    "            spend_model_time=(time.time() - start_model_time)\n",
    "            metrics_total_dict['Experiment_date_time'] = time_name\n",
    "            metrics_total_dict['data_name'] = data_name\n",
    "            metrics_total_dict['model_name'] = model_name\n",
    "            metrics_total_dict['param_grid'] = param_grid   # gridsearchcv get_params() ==> json 저장에러\n",
    "            metrics_total_dict['best_params'] = best_params # gridsearchcv get_params()  ==> json 저장에러\n",
    "            metrics_total_dict['model_params'] = model.get_params()\n",
    "    #         metrics_total_dict['scaler'] = scaler\n",
    "    #         metrics_total_dict['p_degree'] = p_degree\n",
    "            metrics_total_dict['metrics'] =metrics_dict\n",
    "            metrics_total_dict['Execution_time'] = spend_model_time\n",
    "            with open(f'./json3GridSearch/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "                json.dump(metrics_total_dict, f, indent=4)            \n",
    "            print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n",
    "            # best_params에 대한 model별 params \n",
    "            params = 'random_state=1'\n",
    "            for key, value in best_params.items():\n",
    "                params = params + ',' + str(key) + '=' + str(value)\n",
    "    #             print(f'{model_name} params : {params}\\n')\n",
    "            params_dict[f'{model_name}']=params\n",
    "        spend_time=(time.time() - start_time)\n",
    "        print(f'전체 수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')\n",
    "        print(params_dict)\n",
    "\n",
    "    else: # grid=4 # GridSearchCV best_params에 대한 model별 수기적용하여, 2. ~ 3. model 훈련, metric저장, 과적합여부 plot\n",
    "        time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        for model in model_list:\n",
    "            start_model_time = time.time()\n",
    "            model_name = model.__class__.__name__\n",
    "            model, metrics_dict = \\\n",
    "                    get_model_train_eval(model, model_name, data_name, time_name, X_train, X_test, y_train, y_test)\n",
    "        ## metrics json 저장 \n",
    "            metrics_total_dict = {}\n",
    "            spend_model_time=(time.time() - start_model_time)\n",
    "            metrics_total_dict['Experiment_date_time'] = time_name\n",
    "            metrics_total_dict['data_name'] = data_name\n",
    "            metrics_total_dict['model_name'] = model_name\n",
    "            model_params = model.get_params() # dict\n",
    "            metrics_total_dict['model_params'] = model_params\n",
    "    #         metrics_total_dict['scaler'] = scaler\n",
    "    #         metrics_total_dict['p_degree'] = p_degree\n",
    "            metrics_total_dict['metrics'] =metrics_dict\n",
    "    #         metric_dict['best_params'] = best_params\n",
    "            metrics_total_dict['Execution_time'] = spend_model_time\n",
    "            with open(f'./json4/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "                json.dump(metrics_total_dict, f, indent=4)            \n",
    "            print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n",
    "        spend_time=(time.time() - start_time)\n",
    "        print(f'전체 수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')        \n",
    "#     return metrics_total_dict  \n",
    "run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list= [\n",
    "    LogisticRegression(random_state=1,n_jobs=-1,C=0.1,l1_ratio=0.1,max_iter=100,solver='lbfgs'),\n",
    "    KNeighborsClassifier(n_jobs=-1,n_neighbors=10,p=1),\n",
    "    SGDClassifier(random_state=1,n_jobs=-1,alpha=0.0001,epsilon=0.1,eta0=0.0,l1_ratio=0.01,max_iter=100), \n",
    "    DecisionTreeClassifier(random_state=1,max_depth=5,max_features=8,min_samples_leaf=20,min_samples_split=10),\n",
    "    RandomForestClassifier(random_state=1,n_jobs=-1,max_depth=5,max_features=5,min_samples_leaf=15,min_samples_split=10),\n",
    "    GradientBoostingClassifier(random_state=1,max_depth=10,max_features=5,min_samples_leaf=10,min_samples_split=10),\n",
    "    XGBClassifier(random_state=1,n_jobs=-1,silent=True,colsample_bytree=0.5,max_depth=15,min_child_weight=1,reg_alpha=0,reg_lambda=0.5,subsample=1),\n",
    "    LGBMClassifier(random_state=1,n_jobs=-1,colsample_bytree=1,max_depth=128,min_child_samples=5,num_leaves=64,reg_alpha=0,reg_lambda=0.5,subsample=0.8)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개별 model별 (1. (data, run),  4.(data split) 항목만 사용치 않고, 2.metric ~ 3. 훈련 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# data_name = \"breast_cancer\"\n",
    "# data_name = \"pima\"\n",
    "# data_name = \"cust\"\n",
    "# data_name = \"credit\"\n",
    "data_name = \"project\"\n",
    "\n",
    "# grid = 1\n",
    "# grid = 2\n",
    "grid = 3\n",
    "# grid = 4\n",
    "\n",
    "# df = dataset()\n",
    "# df = pima_df\n",
    "# df = cust_df\n",
    "# df = credit_df\n",
    "\n",
    "\n",
    "# df = pd.read_csv('data/NHIS_total_model.csv')\n",
    "# df['target'] = df['식전혈당(공복혈당)'].apply(lambda x: 1 if x >= 126 else 0 )\n",
    "# df['target'] = df['식전혈당(공복혈당)'].apply(lambda x: 1 if x >= 126 else ( 2 if 100 <= x < 126 else 3))\n",
    "# df.drop('식전혈당(공복혈당)', axis=1, inplace=True)    \n",
    "# df['연령'] =df['연령대코드(5세단위)'].apply( lambda x: x*5 + 17 )\n",
    "# df.drop('연령대코드(5세단위)', axis=1, inplace=True)\n",
    "\n",
    "# target_count = df_copy[ df_copy['target'] == 1 ].shape[0]\n",
    "# not_target_count = df_copy[ df_copy['target'] != 1 ].shape[0]\n",
    "# frac=round( (target_count/not_target_count), 3)\n",
    "\n",
    "# df_target = df_copy[ df_copy['target'] == 1 ]\n",
    "# df_X =  df_copy[ df_copy['target'] != 1 ].sample(frac=frac, random_state=1)\n",
    "# df = pd.concat ([df_target, df_X]).sort_values(by=['기준년도'])\n",
    "# df.drop(['기준년도','시도코드'], axis=1, inplace=True)    \n",
    "# df = sklearn.utils.shuffle(df)\n",
    "# print(df[df['target'] == 1].shape[0])\n",
    "# print(df[df['target'] != 1].shape[0]) \n",
    "\n",
    "df = pd.read_csv('data/NHIS_model_1.csv')\n",
    "# df = df.iloc[:200000, :]\n",
    "target_count = df[df['target'] == 1].shape[0]\n",
    "target_ex_count = df[df['target'] != 1].shape[0]\n",
    "print(f\"target   count : {target_count} ({ round( target_count/df.shape[0], 4)*100 })%\")\n",
    "print(f\"target외 count : {target_ex_count} ({ round( target_ex_count/df.shape[0], 4)*100 })%\")\n",
    "\n",
    "# 범주형 feature 10개\n",
    "category_features = ['성별코드','청력(좌)','청력(우)', '요단백', '흡연상태', '음주여부', '구강검진수검여부']\n",
    "df.drop(category_features, axis=1, inplace=True)\n",
    "y_target = df['target']\n",
    "X_features = df.drop('target', axis=1)\n",
    "\n",
    "columns = X_features.columns\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "X_features = np.log1p(X_features)\n",
    "# X_features = scaler.fit_transform(X_features)\n",
    "X_features = pd.DataFrame(X_features, columns=columns)\n",
    "\n",
    "## train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=156,stratify=y_target)\n",
    "# xgb,lgbm,sgb early_stopping_rounds 사용시 ==> test 데이터 eval_set 으로 다시 분리  # train, test 분리 0.3 필요\n",
    "X_test, eval_X, y_test, eval_y = train_test_split(X_test, y_test, test_size=0.3, random_state=256, stratify=y_test)\n",
    "   \n",
    "# LinearSVC 규제에 편향이 포함 scaler = StandardScaler 사용, loss=hinge(loss는 2개, squared-hinge)\n",
    "## 아래 params에서 n_jobs=-1이 있는 곳은 그대로 추가 # xgb는 silent=True까지 추가\n",
    "# model = LogisticRegression(random_state=1, n_jobs=-1)   # tree 사용안됨\n",
    "# model = SVC(random_state=1, kernel=\"linear\", C=1, probability=True)\n",
    "# model = LinearSVC(random_state=1, loss='hinge')\n",
    "# model = KNeighborsClassifier(n_jobs=-1)\n",
    "# model = SGDClassifier(random_state=1,n_jobs=-1)\n",
    "# model= GaussianNB()\n",
    "# model = tree.DecisionTreeClassifier(random_state=1)\n",
    "# model = DecisionTreeClassifier(random_state=1)\n",
    "# model = RandomForestClassifier(random_state=1,n_jobs=-1)\n",
    "# model = GradientBoostingClassifier(random_state=1),\n",
    "model = XGBClassifier(random_state=1,silent=True,n_jobs=-1, device='gpu')\n",
    "# model = LGBMClassifier(random_state=1,n_jobs=-1, device='gpu')\n",
    "\n",
    "## SGDClassifier\n",
    "#  clf : 'hinge'(default but error),'log','modified_huber','squared_hinge','perceptron',\n",
    "    # hinge, squared_hinge는 probability return하지 않고 log, modified_huber 리턴함\n",
    "# reg:'squared_loss','huber', 'epsilon_insensitive' or'squared_epsilon_insensitive'\n",
    "#     model =  SGDClassifier(random_state=1,n_jobs=-1, loss='log')  # log => logit + SGD\n",
    "    \n",
    "time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = model.__class__.__name__\n",
    "\n",
    "# grid = 1, 4    \n",
    "# model,metrics_dict = get_model_train_eval_(model,model_name,data_name,time_name, X_train, X_test, y_train, y_test)\n",
    "# grid = 2 (RandomSearchCV)\n",
    "# model,best_params,metrics_dict=get_model_train_eval(model, model_name,data_name,time_name, X_train, X_test, y_train, y_test)\n",
    "model,best_params,metrics_dict=get_model_train_eval(model,model_name,data_name,time_name,X_train,X_test,y_train,y_test,eval_X,eval_y)\n",
    "# grid =3 (GridSearchCV)\n",
    "# model,param_grid,best_params,metrics_dict=get_model_train_eval(model,model_name,data_name,time_name,X_train,X_test,y_train,y_test)\n",
    "# model,best_params,metrics_dict=get_model_train_eval(model,model_name,data_name,time_name,X_train,X_test,y_train,y_test,eval_X,eval_y)\n",
    "\n",
    "## model 저장\n",
    "# pickle.dump(model, open(f'./modeling/model_{model_name}_{time_name}.sav', 'wb'))\n",
    "\n",
    "# plot\n",
    "# Plot_learning_curve(data_name, model, model_name, time_name,  X_train, y_train, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "# precision_recall_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name )\n",
    "# roc_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name)\n",
    "# Plot_tree_(data_name, model, model_name, time_name) # LogisticRegression, RandomForestClassifier 사용못함 # 수행시간 많이 걸림\n",
    "# Plot_feature_importance(data_name, model, X_train.columns, model_name,time_name ) # LogisticRegression 사용못함\n",
    "# Plot_coef_logit(data_name, model, X_train.columns, model_name,time_name ) # LogisticRegression\n",
    "\n",
    "## metrics json 저장 \n",
    "metrics_total_dict = {}\n",
    "spend_model_time=(time.time() - start_time)\n",
    "metrics_total_dict['Experiment_date_time'] = time_name\n",
    "metrics_total_dict['data_name'] = data_name\n",
    "metrics_total_dict['model_name'] = model_name\n",
    "metrics_total_dict['param_grid'] = param_grid   # gridsearchcv get_params() ==> json 저장에러\n",
    "metrics_total_dict['best_params'] = best_params # gridsearchcv get_params()  ==> json 저장에러\n",
    "metrics_total_dict['model_params'] = model.get_params()\n",
    "# metrics_total_dict['scaler'] = scaler\n",
    "# metrics_total_dict['p_degree'] = p_degree\n",
    "metrics_total_dict['metrics'] =metrics_dict\n",
    "metrics_total_dict['Execution_time'] = spend_model_time\n",
    "# with open(f'./json1/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "# with open(f'./json2RandomSearch/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "with open(f'./json3GridSearch/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "# with open(f'./json4/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "    json.dump(metrics_total_dict, f, indent=4)            \n",
    "print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0. data_name, df 선택 ==> target 분리, 전처리영역필요여부\n",
    "1. 초기모델은 grid = 1 선택, 전체 model_list for문, 과적합 learning_curve plot 확인\n",
    "    model_list,  Plot learning_curve\n",
    "    \n",
    "2. feature engineering : scaler, p_degree(다항식), \n",
    "    scaler log1p 변환시 is_expm1=True\n",
    "    1. 3.항 실행    \n",
    "    \n",
    "3. GridSearchCV 사용 grid = 2 선택, 전체 model_list for문, parameter별 영향력 확인 grid_curve\n",
    "    model_list,  plot grid_curve\n",
    "    ==> 1항에서 parameter tunning 반복가능\n",
    "    \n",
    "4. grid = 3 best param model 선택, 과적합 learning_curve plot 확인\n",
    "    model_list, Plot learning_curve\n",
    "    \n",
    "5. model과 최적 param 찾았다면, model\n",
    "    model, Plot (coeff_, feature_importance_ )\n",
    "    \n",
    "6. xgb, lgbm의 early기능 사용시 eval_X, eval_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 로드\n",
    "* get_train_test_dataset()에 의해 분리되지 않으면  \n",
    "* get_preprocessed_df()활용\n",
    "* get_human_dataset() 새로만들어서 리턴\n",
    "\n",
    "###  데이터 분리, 전처리가 필요한가요?  get_train_test_dataset()   \n",
    "\n",
    "### 모델 선택과  get_model_train_eval \n",
    "* 모델, param알고 있나요?  GridsearchCV 사용하여 최적 param찾으실 것인가요?\n",
    "*  ==>  그냥 fit 선택, GridsearchCV 사용여부 결정 \n",
    "    + 모델이 이진분류라면 custom_threshold 선택\n",
    "* return 5개 custom_threshold, model, pred_proba, best_params, metric_lst \n",
    "* GridsearchCV나오고 best_param_에 의해 새롭게 입력되었으면 run함수의 모델에 최적의 param입력하고 \n",
    "    - GridsearchCV 주석처리,  return값 best_params 확인\n",
    "* GridsearchCV 사용하나요?\n",
    "    + param 선택하였나요?\n",
    "    + cv하신것인가요?\n",
    "* run에서도 맞는 return 일치\n",
    "\n",
    "### 모델 metric\n",
    "* 이진분류이면 get_clf_eval(y_test, pred, pred_proba[:,1]) <=========== 6개(c,a,p,r,f1,auc)모두 \n",
    "* 다항분류이면 get_clf_eval_poly(y_test, pred,pred_proba[:,1]) <==== confusion_matrix, accuracy, auc만 지원\n",
    "\n",
    "### 그리기\n",
    "* 이진트리 graphviz, boundary GridsearchCV 전후 사진이 필요한가요? (LogisticRegression 지원하지 않음)\n",
    "    + 전후 저장장소의 파일이름을 달리 하세요 \n",
    "* Plot 데이터 전처리 전후, 이진분류시 precision-recall curve\n",
    "* tree\n",
    "* feature_importances_\n",
    "    + Plot1 이진분류\n",
    "    + Plot_tree : sklearn 0.23.1\n",
    "    + Plot_feature_importance_xgb_lgb ===> xgboost, gbm 맞게 import\n",
    "### save\n",
    "* metric json\n",
    "* feature_importance image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictParse(data):\n",
    "    # with open(f'./static/json/{model_name}_{time_name}.json', 'r', encoding='UTF-8-sig') as json_file:\n",
    "    #     data = json.load(json_file)\n",
    "    #     print(data)\n",
    "    if 'Experiment_date_time' in data.keys():\n",
    "        time_name = data['Experiment_date_time']\n",
    "    if 'model' in data.keys():\n",
    "        model_n = data['model']\n",
    "    if 'metrics' in data.keys():\n",
    "        if 'confusion_matrix' in data['metrics'].keys():\n",
    "            confusion = data['metrics']['confusion_matrix']\n",
    "        if 'accuracy' in data['metrics'].keys():\n",
    "            accuracy = data['metrics']['accuracy']\n",
    "        if 'precision' in data['metrics'].keys():\n",
    "            precision = data['metrics']['precision']\n",
    "        if 'recall' in data['metrics'].keys():\n",
    "            recall = data['metrics']['recall']    \n",
    "        if 'f1' in data['metrics'].keys():\n",
    "            f1_score = data['metrics']['f1']  \n",
    "        if 'roc_auc' in data['metrics'].keys():\n",
    "            auc_score = data['metrics']['roc_auc']  \n",
    "    if 'Execution_time' in data.keys():\n",
    "        Execution_time = data['Execution_time'] \n",
    "    # print(f'confusion_matrix{confusion}\\n accuracy{accuracy}\\n precision{precision}\\n recall{recall}\\n f1_score{f1_score}\\n auc_score{auc_score}\\n Execution_time{Execution_time}')\n",
    "    return model_n, confusion, accuracy, precision, recall, f1_score, auc_score, Execution_time\n",
    "\n",
    "# custom_threshold, metrics_total_dict = run()\n",
    "metrics_total_dict = run()\n",
    "model_n, confusion, accuracy, precision, recall, f1_score, auc_score, Execution_time = dictParse(metrics_total_dict)\n",
    "print(f'model name {model_n}\\n confusion_matrix{confusion}\\n accuracy{accuracy}\\n precision{precision}\\n recall{recall}\\n f1_score{f1_score}\\n auc_score{auc_score}\\n Execution_time{Execution_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.datasets import load_breast_cancer # xgb 유방암, voting사용\n",
    "    from sklearn.datasets import load_diabetes  # pima아니고 \n",
    "    from sklearn.datasets import load_boston # 회귀\n",
    "    # built_in data 선택\n",
    "    dataset = load_iris()\n",
    "#     dataset = load_breast_cancer()    \n",
    "    X_features= dataset.data\n",
    "    y_label = dataset.target\n",
    "    df = pd.DataFrame(data=X_features, columns=dataset.feature_names)\n",
    "    df['target']= y_label\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer 은행고객 만족도 # xgb, gbm\n",
    "cust_df = pd.read_csv('data/train_santander.csv', encoding='latin-1')\n",
    "cust_df['var3'].replace(-999999,2, inplace=True)\n",
    "# cust_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit 신용카드 280,000중 사기 500건 : 0.178%\n",
    "# 단계 1.default  2.'Time' feature remove  3.scaler=standard 'Amount' 4. scaler=lg1p 'Amount' + outlier 'V14'  5. GridSerchCV\n",
    "def get_outlier_index(df = None, column=None, weight_tup=None):\n",
    "    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. \n",
    "    series = df[df['Class']==1][column] # 사기데이터\n",
    "#     print(f'\\nseries.name : {column}   weight : {weight_tup}')\n",
    "    weight, lower_adj=weight_tup\n",
    "    quantile_25 = np.percentile(series.values, 25)\n",
    "#     print(f'quantile_25 : {quantile_25}')\n",
    "    quantile_75 = np.percentile(series.values, 75)\n",
    "#     print(f'quantile_75 : {quantile_75}')\n",
    "    # IQR을 구하고, IQR에 weight를 곱하여 최대값과 최소값 지점 구함. \n",
    "    iqr = quantile_75 - quantile_25\n",
    "#     print(f'iqr : {iqr}')\n",
    "    iqr_weight = iqr * weight\n",
    "#     print(f'iqr_weight : {iqr_weight}')\n",
    "    # lower 조정계수 반영\n",
    "    lowest_val = quantile_25 - iqr_weight + lower_adj\n",
    "#     print(f'lowest_val : {lowest_val}')\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "#     print(f'highest_val : {highest_val}')\n",
    "    # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환. \n",
    "    outlier_index = series[(series < lowest_val) | (series > highest_val)].index\n",
    "#     print(f'outlier_index length : {len(outlier_index)}\\n')\n",
    "    return outlier_index\n",
    "def remove_outlier(df):\n",
    "    outlier_drop_indexs=[]\n",
    "    outlier_index = get_outlier_index(df = credit_df, column='V14', weight_tup=(1.5, 0))  \n",
    "    if (outlier_index.values.tolist() != []):\n",
    "        outlier_drop_indexs.extend(outlier_index.values.tolist())\n",
    "    print(f'len(outlier_drop_indexs) : {len(outlier_drop_indexs)}\\n')\n",
    "    df.drop(outlier_drop_indexs, axis=0, inplace=True)\n",
    "    return df\n",
    "credit_df = pd.read_csv('data/creditcard.csv')\n",
    "# print(credit_df.info())\n",
    "credit_df.drop('Time', axis=1, inplace=True)\n",
    "credit_df = remove_outlier(credit_df)\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pima diabetes\n",
    "pima_df = pd.read_csv('data/diabetes.csv')\n",
    "df=pima_df\n",
    "#0 제거 후 nan 으로 변환 # np.nan의 type : float\n",
    "col=['Glucose', 'BloodPressure','SkinThickness','Insulin','BMI' ]\n",
    "for i in col:\n",
    "    df[i].replace(0, np.nan, inplace= True)\n",
    "    # median 중앙값 찾기\n",
    "def median_target(var):   \n",
    "    temp = df[df[var].notnull()]\n",
    "    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n",
    "    return temp\n",
    "# o값 대체 NaN 채워넣기 \n",
    "df.loc[(df['Outcome'] == 0 ) & (df['Glucose'].isnull()), 'Glucose'] = median_target('Glucose')[ median_target('Glucose')['Outcome']==0]['Glucose'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['Glucose'].isnull()), 'Glucose'] = median_target('Glucose')[ median_target('Glucose')['Outcome']==1]['Glucose'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['BloodPressure'].isnull()), 'BloodPressure'] = median_target('BloodPressure')[ median_target('BloodPressure')['Outcome']==0]['BloodPressure'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['BloodPressure'].isnull()), 'BloodPressure'] = median_target('BloodPressure')[ median_target('BloodPressure')['Outcome']==1]['BloodPressure'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['SkinThickness'].isnull()), 'SkinThickness'] = median_target('SkinThickness')[ median_target('SkinThickness')['Outcome']==0]['SkinThickness'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['SkinThickness'].isnull()), 'SkinThickness'] = median_target('SkinThickness')[ median_target('SkinThickness')['Outcome']==1]['SkinThickness'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['Insulin'].isnull()), 'Insulin'] =  median_target('Insulin')[ median_target('Insulin')['Outcome']==0]['Insulin'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['Insulin'].isnull()), 'Insulin'] = median_target('Insulin')[ median_target('Insulin')['Outcome']==1]['Insulin'].values[0]\n",
    "df.loc[(df['Outcome'] == 0 ) & (df['BMI'].isnull()), 'BMI'] = median_target('BMI')[ median_target('BMI')['Outcome']==0]['BMI'].values[0]\n",
    "df.loc[(df['Outcome'] == 1 ) & (df['BMI'].isnull()), 'BMI'] = median_target('BMI')[ median_target('BMI')['Outcome']==1]['BMI'].values[0]\n",
    "# df.isnull().sum()\n",
    "pima_df=df\n",
    "# pima_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임계값에 따른 성능평가지표 산출\n",
    "def get_eval_by_threshold(y_test , pred_proba, thresholds):\n",
    "    metrics_lst=[]\n",
    "    for custom_threshold in thresholds:\n",
    "        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba) \n",
    "        custom_predict = binarizer.transform(pred_proba) # predict 수행한 것과 동일\n",
    "#         print('임곗값:', custom_threshold)\n",
    "        metrics_dict = get_clf_eval(y_test , custom_predict, pred_proba)\n",
    "        # print(metrics_dict)\n",
    "        metrics_lst.append(metrics_dict)\n",
    "    return metrics_lst\n",
    "\n",
    "     ###################### 사용자임계값 사용시 binary classification 최적의 임계값 선택 ####################################\n",
    "#     thresholds =np.arange(0.1, 0.5, 0.02).tolist()\n",
    "#     metrics_lst = get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds )\n",
    "#     # # print(metrics_lst)\n",
    "\n",
    "#     f1_lst=[]\n",
    "#     for metric_dict in metrics_lst:\n",
    "#     #     # print(f'metric_dict : {metric_dict}')\n",
    "#         if 'f1' in metric_dict:\n",
    "#             f1_lst.append(metric_dict['f1'])     \n",
    "#     # # print(f'f1_lst: {f1_lst}')\n",
    "#     custom_threshold = thresholds[f1_lst.index(max(f1_lst))]\n",
    "#     print(f'best threshold : {custom_threshold}')\n",
    "\n",
    "#     # # 최적 임계값으로 설정한 Binarizer 생성 # 원래 default 임계값 0.5\n",
    "#     binarizer = Binarizer(threshold=custom_threshold)\n",
    "#     pred_custom_th = binarizer.fit_transform(pred_proba[:, 1].reshape(-1,1)) \n",
    "\n",
    "    ##### return #############################################       \n",
    "    # 사용자임계값 사용시 : 이진분류일때 pred_custom_th\n",
    "#     return custom_threshold, model, pred_proba, get_clf_eval(y_test, pred_custom_th, pred_proba[:,1]) # 이진분류일때\n",
    "#     return custom_threshold, model, pred_proba,  best_params, get_clf_eval(y_test, pred_custom_th, pred_proba[:,1])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
