{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version : 0.23.2\n",
      "python version : 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sys\n",
    "print(f'sklearn version : {sklearn.__version__}')\n",
    "print(f'python version : {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "from IPython.display import HTML, display # jupyter 사진넣기\n",
    "import time                      # excution time 계산\n",
    "from datetime import datetime   # system time \n",
    "import json                     # json save\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree                  # 결정트리 모형  # Classifier tree\n",
    "from sklearn.inspection import permutation_importance  # feature 중요도\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer, PolynomialFeatures\n",
    "from sklearn.metrics import (accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix,\n",
    "                             precision_recall_curve,roc_curve,mean_squared_error,mean_absolute_error,r2_score)\n",
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier,LinearRegression,SGDRegressor,Ridge,Lasso,ElasticNet\n",
    "from sklearn.svm import SVC, LinearSVC,LinearSVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree \n",
    "from sklearn.ensemble import (VotingClassifier, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier,\n",
    "                            ExtraTreesClassifier,GradientBoostingClassifier,RandomForestRegressor,GradientBoostingRegressor)\n",
    "from xgboost import XGBClassifier, XGBRegressor     # 사이킷런 wrapper\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor   # 사이킷런 wrapper\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pickle                         # 모델 저장\n",
    "import matplotlib as mpl              # 한글깨짐\n",
    "import matplotlib.font_manager as fm  # 한글깨짐\n",
    "import matplotlib.font_manager        # 한글깨짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linux \n",
    "font_dirs = ['/home/sch/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf', ]\n",
    "font_files = fm.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = fm.createFontList(font_files)\n",
    "fm.fontManager.ttflist.extend(font_list)\n",
    "[f.name for f in matplotlib.font_manager.fontManager.ttflist if 'Nanum' in f.name][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형기반 회귀계수 coef_ (lr, Ridge, Lasso, ) # gridsearchCV 에서는 없음\n",
    "def Plot_coef_(data_name, model, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    # 리눅스\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    df = pd.Series(model.coef_, index=indexs) \n",
    "    df = df.sort_values(ascending=False) # [:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df, y=df.index)\n",
    "    plt.title(f'{data_name} {model_name} coef_ ')\n",
    "#     plt.xlabel(f'{model_params}')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_coef_{model_name}_{time_name}.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show()  \n",
    "    \n",
    "# 회귀트리(dt, rf, xgb, lgbm ) 이용시 feature_importances_  # gridsearchCV 에서는 없음\n",
    "def Plot_feature_importance(data_name, model, indexs, model_name, time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "    # 리눅스\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    df = pd.Series(model.feature_importances_, index=indexs) \n",
    "    df = df.sort_values(ascending=False) # [:20]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=df, y=df.index)\n",
    "    plt.title(f'{data_name} {model_name} feature_importance')\n",
    "#     plt.xlabel(f'{model_params}')\n",
    "    plt.savefig(f\"./imgSel/{data_name}_f_i_{model_name}_{time_name}.png\", bbox_inches='tight') # dpi=100\n",
    "#     plt.close(fig)\n",
    "    plt.show()  \n",
    "    \n",
    " # 결정트리 모형도 sklearn 0.23.1   (lr, rf 사용못함, CART 가능) : 수행시간이 많이 걸림\n",
    "def Plot_tree_(data_name, model,model_name,time_name):\n",
    "    # 한글깨짐 윈도우(window)\n",
    "    font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # 폴더구분자 python / 임\n",
    "    mpl.rc('font', family=font_name) # ahronbd.ttf\n",
    "#     mpl.rcParams['font.family'] = 'NanumGothic'\n",
    "#     mpl.rc('axes', unicode_minus=False)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    tree.plot_tree(model, filled=True)\n",
    "    plt.title(f'{data_name} {model_name} tree')\n",
    "    plt.savefig(f\"./imgTree/{data_name}_tree_{model_name}_{time_name}.png\", bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습곡선\n",
    "def Plot_learning_curve(data_name, model, model_name, time_name,  X, y, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "            model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'neg_mean_squared_error', return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title(f'{data_name} {model_name} learing curve')\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training error\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation error\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training \")\n",
    "    plt.ylabel(\"neg_mean_squared_error\")\n",
    "    plt.savefig(f\"./imgLearnCurve/{data_name}_l_c_{model_name}_{time_name}.png\", bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results_데이터로 GridSearchCV 곡선\n",
    "def plot_grid_curve(model, grid_params, data_name, model_name,time_name):\n",
    "    df = pd.DataFrame(model.cv_results_)\n",
    "    results = ['mean_test_score','mean_train_score'] \n",
    "    fig, axes = plt.subplots(1, len(grid_params), figsize = (3*len(grid_params), 4) )  # sharey='row'\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=10)\n",
    "    for idx, (param_name, param_range) in enumerate(grid_params.items()):\n",
    "        grouped_df = df.groupby(f'param_{param_name}')[results].agg({'mean_train_score': 'mean', 'mean_test_score': 'mean'})\n",
    "        previous_group = df.groupby(f'param_{param_name}')[results]\n",
    "        axes[idx].set_xlabel(param_name, fontsize=10)\n",
    "#         axes[idx].set_ylim(0.0, 1.1)\n",
    "        lw = 2 # 선의 굵기\n",
    "        axes[idx].plot(param_range, grouped_df['mean_train_score'], label=\"Training score\", color=\"darkorange\", lw=lw)\n",
    "        axes[idx].plot(param_range, grouped_df['mean_test_score'], label=\"Cross-validation score\",  color=\"navy\", lw=lw)\n",
    "#     handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(f'{data_name} {model_name} Validation curves', fontsize=20)\n",
    "    plt.legend(loc = \"best\")\n",
    "    fig.savefig(f\"./imgGridCurve/{data_name}_g_v_{model_name}_{time_name}.png\", bbox_inches='tight')\n",
    "    fig.subplots_adjust(bottom=0.25, top=0.85)  \n",
    "#     plt.close(fig)\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  metric 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clf\n",
    "# 다중분류에서는 confusion_matrix, accuracy, roc_auc 만 나온다\n",
    "def get_clf_eval_poly(y_test, pred, pos_proba=None):\n",
    "    metrics_dict={}\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    metrics_dict['confusion_matrix'] = confusion.tolist() # confusion nd.array  --> list\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    roc_auc = roc_auc_score(y_test, pos_proba) \n",
    "    metrics_dict['auc_score'] = auc_score\n",
    "    print(f'오차 행렬 :\\n {confusion}')\n",
    "    print(f\" 정확도: {accuracy:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    return metrics_dict\n",
    "\n",
    "# 다중분류에서는 confusion_matrix, accuracy만 나온다\n",
    "# 이진분류에서 성능평가지표 종합 함수\n",
    "def get_clf_eval(y_test=None, pred=None, pos_proba=None): # 권철민 저 책자에 누락 pred_proba\n",
    "    metrics_dict={}\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    metrics_dict['confusion_matrix'] = confusion.tolist()\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    precision = precision_score(y_test , pred)  # average binary # multi [None, 'micro', 'macro', 'weighted'].\n",
    "    metrics_dict['precision'] = precision\n",
    "    recall = recall_score(y_test , pred )     \n",
    "    metrics_dict['recall'] = recall\n",
    "    f1 = f1_score(y_test, pred )\n",
    "    metrics_dict['f1'] = f1\n",
    "    roc_auc = roc_auc_score(y_test, pos_proba) # 권철민 저 책자에 오기 pred --> pred_proba  # average='macro'\n",
    "    metrics_dict['roc_auc'] = roc_auc\n",
    "    # print(f'오차 행렬 :\\n {confusion}')\n",
    "    # print(f\" 정확도: {accuracy:.4f}, 정밀도: {precision:.4f}, 재현율: {recall:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    print(f\" 정확도: {accuracy:.4f}, 정밀도: {precision:.4f}, 재현율: {recall:.4f}, f1_score: {f1:.4f}, auc_score:{roc_auc:.4f}\")\n",
    "    return metrics_dict\n",
    "\n",
    "## reg \n",
    "# log 값 변환 시 NaN등의 이슈로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산\n",
    "def rmsle(y_test, pred):\n",
    "    log_y = np.log1p(y_test)\n",
    "    log_pred = np.log1p(pred)\n",
    "    squared_error = (log_y - log_pred) ** 2\n",
    "    rmsle = np.sqrt(np.mean(squared_error))\n",
    "    return rmsle\n",
    "\n",
    "# 사이킷런의 mean_square_error() 를 이용하여 np.sqrt로 RMSE 계산\n",
    "def rmse(y_test,pred):\n",
    "    return np.sqrt(mean_squared_error(y_test,pred))\n",
    "\n",
    "# MAE, MSE, RMSE, RMSLE 를 모두 계산 \n",
    "def get_reg_eval(y_test, pred):\n",
    "    metrics_dict={}\n",
    "    mae_val = mean_absolute_error(y_test,pred)\n",
    "    metrics_dict['MAE'] = mae_val\n",
    "    mse_val = mean_squared_error(y_test,pred)\n",
    "    metrics_dict['MSE'] = mse_val\n",
    "    rmse_val = rmse(y_test,pred)\n",
    "    metrics_dict['RMSE'] = rmse_val\n",
    "    rmsle_val = rmsle(y_test,pred)\n",
    "    metrics_dict['RMSLE'] = rmsle_val    \n",
    "    r2 = r2_score(y_test,pred)\n",
    "    metrics_dict['R2'] = r2    \n",
    "    print(f'MAE: {mae_val:.3F}, MSE: {mse_val:.3F}, RMSE: {rmse_val:.3F}, RMSLE: {rmsle_val:.3f}, R2: {r2:.3f}\\n')\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. model train, metric호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_params()관련 LGBM기준 (XGB유사) # sklearn version별 api 다름\n",
    "# max_depth log2(데이터수) # bostonHouse 506=>8, # bike 10886=>13 # AmesHouse 1460=>10 # project 797만=>22\n",
    "# num_leaves  : 2^(max_depth) 작아야 함  최대 리프노드 갯수 = max_leaf_nodes(dt, rf)\n",
    "# min_child_samples : Leaf node가 되기 위한 최소한 데이터 개체수  = min_samples_leaf  # min_samples_split (dt,rf 분리 최소 데이터)\n",
    "# min_child_weight = min_child_leaf (dt, rf) *gbm은 없음    \n",
    "# subsample (gbm,xgb,lgbm) 데이터 샘플링 비율 *lgbm은 subsample_for_bin 20000, subsample_freq 0\n",
    "# colsample_bytree = max_features (dt, rf, gbm) 개별 트리를 학습할 때마다 무작위로 선택하는 feature 비율을 제어 (feature 많을 경우) * \n",
    "    # XGB : colsample_bylevel, colsample_bynode, colsample_bytree\n",
    "# reg_alpha : L1 규제(제거) # reg_lambda : L2 규제(최소화) \n",
    "# 영혼까지 learning_rate 내리고,  n_estimators(rf,gbm,xgb,lgbm) 올리고\n",
    "def get_params(model_name, data_name):  # param_grid 2개항목 이상일때만 plot_grid_curve 가능\n",
    "    if (grid == 2) : # RandomSearch    속도느링 LogisticRegression  GradientBoostingClassifier\n",
    "        # clf\n",
    "        if model_name == 'LogisticRegression': # 속도느링 #\"solver\":['lbfgs','auto']\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e0),'l1_ratio': stats.uniform(0, 1), \"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid = { 'C': stats.loguniform(1e-1, 1e0),\"max_iter\":stats.randint(100,500) }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid = { 'alpha':stats.loguniform(1e-4,1e0),'l1_ratio':stats.uniform(0,1),'average':[True,False],\n",
    "                            'epsilon':stats.loguniform(1e-1, 1e1),'max_iter':stats.randint[100,500]}\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            if(data_name=='project'):param_grid = {\"max_depth\": stats.randint(5, 50), \"max_features\": stats.randint(2, 24),\n",
    "                \"min_samples_split\": stats.randint(10, 100), \"min_samples_leaf\": stats.randint(10, 100),\"criterion\": [\"gini\", \"entropy\"]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            if(data_name=='project'):param_grid = {\"max_depth\":  stats.randint(5, 50),\"max_features\":stats.randint(2, 24),\n",
    "            \"min_samples_split\":stats.randint(10,100),\"min_samples_leaf\":stats.randint(10,100),\"bootstrap\":[True,False],\"criterion\":[\"gini\",\"entropy\"]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림 #n_estimators=100 learning_rate:0.1 subsample:1.0 ccp_alpha:0.0 n_features\n",
    "            if(data_name=='project'):param_grid={ 'max_depth' : stats.randint(5, 50),'min_samples_leaf': stats.randint(10,500),\n",
    "                                                   'max_features':  stats.randint(5,24) }\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            if(data_name=='project'):param_grid={'n_estimators': stats.randint(200, 1000),'learning_rate': stats.uniform(0.01, 0.6),\n",
    "            'subsample':stats.uniform(0.3, 0.9),\"max_depth\":stats.randint(5,50),'colsample_bytree':stats.uniform(0.5,0.9),'min_child_weight':stats.loguniform(1e-3, 1e3) }\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            if(data_name=='project'):param_grid={'n_estimators': stats.randint(200, 1000),'learning_rate': stats.uniform(0.01, 0.6),\n",
    "            'min_child_samples':stats.randint(10,500),'colsample_bytree':stats.uniform(loc=0.4,scale=0.6),'num_leaves': stats.randint(32,1000),\n",
    "                                    'subsample': stats.uniform(loc=0.2, scale=0.8), 'min_child_weight': stats.loguniform(1e-3, 1e3),\n",
    "                                      'reg_alpha': stats.loguniform(1e-3, 1e2),'reg_lambda': stats.loguniform(1e-3, 1e3)}   \n",
    "\n",
    "        # reg\n",
    "        elif model_name == 'LinearRegression':\n",
    "            param_grid  = {'copy_X': [True, False], 'fit_intercept': [True,False] }\n",
    "        elif model_name == 'LinearSVR': \n",
    "            param_grid = {'C':stats.loguniform(1e-1,1e0),'intercept_scaling':loguniform(1e-1,1e0),\"max_iter\":stats.randint(100,500)}\n",
    "        elif model_name == 'SGDRegressor':\n",
    "            param_grid = {'alpha':stats.loguniform(1e-4,1e0),'l1_ratio':stats.uniform(0,1),'average':[True,False],\n",
    "                            'epsilon':stats.loguniform(1e-1, 1e1),'max_iter':stats.randint[100,500]}\n",
    "        elif model_name == 'Ridge': \n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e0),'fit_intercept': [True,False] } \n",
    "        elif model_name == 'Lasso': \n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e0),'fit_intercept': [True,False] } \n",
    "        elif model_name == 'ElasticNet': # if L1 a, if L2 b  => alpha = a+b, l1_ratio = a/(a+b)  # if l1_ratio=1 => a=1, b=0\n",
    "            param_grid = {'alpha': stats.loguniform(1e-4,1e0),'l1_ratio': stats.uniform(0,1)} \n",
    "        elif model_name == 'DecisionTreeRegressor':\n",
    "            param_grid = {\"max_depth\": stats.randint(5, 50), \"max_features\": stats.randint(2, 24),\n",
    "              \"min_samples_split\": stats.randint(10, 100), \"min_samples_leaf\": stats.randint(10, 100),\"criterion\": [\"gini\", \"entropy\"]}\n",
    "        elif model_name == 'RandomForestRegressor': # cpu 병렬지원(n_jobs=-1, 100% 가동)\n",
    "            param_grid = {\"max_depth\":  stats.randint(5, 50),\"max_features\":stats.randint(2, 24),\"min_samples_split\":stats.randint(10,100),\n",
    "                          \"min_samples_leaf\":stats.randint(10,100),\"bootstrap\":[True,False],\"criterion\":[\"gini\",\"entropy\"]}\n",
    "        elif model_name == 'GradientBoostingRegressor': # param많으면 학습시간이 많이 걸림(순차 시행, cpu 1개만 수행)\n",
    "            param_grid={ 'max_depth' : stats.randint(5, 50),'min_samples_leaf': stats.randint(10,500),'max_features':  stats.randint(5,24)}\n",
    "        elif model_name == 'XGBRegressor': #min_child_samples없음 # num_leaves 없음 # cpu 병렬지원(n_jobs=-1, auto) # 시간소요\n",
    "            param_grid={'n_estimators': stats.randint(200,1000),'learning_rate': stats.uniform(0.01,0.6),'subsample':stats.uniform(0.3,0.9),\n",
    "                \"max_depth\":stats.randint(5,50),'colsample_bytree':stats.uniform(0.5,0.9),'min_child_weight':stats.loguniform(1e-3, 1e3) }\n",
    "        elif model_name == 'LGBMRegressor':  # cpu 병렬지원(n_jobs=-1, 100% 가동) # 매우빠름\n",
    "            param_grid={'n_estimators':stats.randint(200,1000),'learning_rate':stats.uniform(0.01,0.6),'min_child_samples':stats.randint(10,500),\n",
    "                'colsample_bytree':stats.uniform(loc=0.4,scale=0.6),'num_leaves': stats.randint(32,1000),'subsample':stats.uniform(loc=0.2,scale=0.8),\n",
    "                'min_child_weight':stats.loguniform(1e-3,1e3),'reg_alpha':stats.loguniform(1e-3,1e2),'reg_lambda':stats.loguniform(1e-3,1e3)}\n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid\n",
    "        \n",
    "    elif (grid == 3) : #GridSearch     \n",
    "        # clf\n",
    "        if model_name == 'LogisticRegression': # \"solver\":['lbfgs','auto']\n",
    "            param_grid  = {'C': [0.1,0.2,0.3],\"l1_ratio\": [0.4,0.6,0.8],\"max_iter\":[400,500,600]}\n",
    "        elif model_name == 'LinearSVC': # 'loss' ['hinge',''] # C 커지면 곡선 -> 직선 # tol(허용오차)\n",
    "            param_grid ={'C': [0.2,0.3,0.4],\"max_iter\":[200,300,400] }\n",
    "        elif model_name == 'KNeighborsClassifier':#'algorithm':'auto' # p:1 manhattan_distance, 2 euclidean_distance \n",
    "            param_grid ={'n_neighbors':[5,10],'p':[1,2]}\n",
    "        elif model_name == 'SGDClassifier':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid ={'alpha': [0.0001,0.01,0.1],\"l1_ratio\":[0.1,0.2,0.3],'epsilon':[0.1,1,10],'max_iter':[100,300,500]} # 빠름\n",
    "        elif model_name == 'DecisionTreeClassifier': \n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_samples_leaf':[10,100,500],'min_samples_split':[10,15,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_samples_leaf':[10,16,20],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[12,18,24],'min_samples_leaf':[10,50,100],'max_features':[100,500,1000]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[15,20,25],'min_samples_leaf':[50,100,150],'max_features':[4,8,12]}\n",
    "        elif model_name == 'RandomForestClassifier': # n_estimators=100\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_samples_leaf':[10,100,500],'min_samples_split':[10,15,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_samples_leaf':[10,100,500],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[12,18,24],'min_samples_leaf':[10,50,100],'max_features':[100,500,1000]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[15,20,25],'min_samples_leaf':[20,40,60],'min_samples_split':[20,40,60]}\n",
    "        elif model_name == 'GradientBoostingClassifier':#속도느림 #n_estimators=100 learning_rate:0.1 subsample:1.0 ccp_alpha:0.0 n_features\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[10],'min_samples_leaf':[10,15],'min_samples_split':[10,20],'max_features':[3,5,8]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[16],'min_samples_leaf':[10,100],'max_features':[100,200,300]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[18],'min_samples_leaf':[10,100],'max_features':[10,20]}\n",
    "            if(data_name=='project'):param_grid={'max_depth':[5,10,20],'min_samples_leaf':[100,200],'max_features':[5,10,15]}\n",
    "        elif model_name == 'XGBClassifier': #min_child_samples, num_leaves 없음 #n_estimators=100 learning_rate:0.1 \n",
    "            if(data_name=='pima'):param_grid={'max_depth':[5,10,15],'min_child_weight':[1,3],'colsample_bytree':[0.5,0.75,1],\n",
    "                                  'subsample':[0.8,1],'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[12,16,20],'min_child_weight':[1,3],'colsample_bytree':[0.5,0.75,1],\n",
    "                                  'subsample':[0.8,1],'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[18,36],'min_child_weight':[1,3],'colsample_bytree':[0.5,1]}\n",
    "                                  #'subsample':[0.8,1]} #'reg_alpha': [0,0.5],'reg_lambda':[0.5,1]}\n",
    "            if(data_name=='project'):param_grid={'n_estimators':[400,500,600],'learning_rate':[0.03,0.05,0.07],'max_depth':[6,15,30],\n",
    "             'min_child_weight':[0.1,1,3],'colsample_bytree':[0.5,0.75,0.95],'subsample':[0.5,0.7,0.9],'reg_alpha':[0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "        elif model_name == 'LGBMClassifier':#n_estimators=100 learning_rate:0.1\n",
    "            if(data_name=='pima'):param_grid={'max_depth':[128,160],'min_child_samples':[10,100,500],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,128,512],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='customer'):param_grid={'max_depth':[128,512],'min_child_samples':[10,100,500],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,128,512],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='credit'):param_grid={'max_depth':[128,512,1024],'min_child_samples':[10,100,1000],'colsample_bytree': [0.5,0.75,1],\n",
    "                                  'num_leaves':[32,512,2048],'subsample':[0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "            if(data_name=='project'):param_grid={'n_estimators':[400,500,600],'learning_rate':[0.03,0.05,0.07], 'min_child_samples':[100,150,200],\n",
    "              'colsample_bytree':[0.5,0.75,1],'num_leaves':[50,100,150],'subsample':[0.6,0.8,1],'reg_alpha': [0,0.5,1],'reg_lambda':[0,0.5,1]}\n",
    "                \n",
    "        # reg\n",
    "        elif model_name == 'LinearRegression':\n",
    "            param_grid  = {'n_jobs' : [-1, 20 ], 'fit_intercept' : [True, False]}\n",
    "        elif model_name == 'LinearSVR': # degree, C, kernal, epsilon, gamma , tol(허용오차) # degree 차원  # C 커지면 곡선 -> 직선 # gamma 커지면 과적합\n",
    "            param_grid ={'C': [1, 10], 'max_iter' : [100, 1000]}\n",
    "        elif model_name == 'SGDRegressor':#early_stopping:False,'learning_rate': 'optimal','penalty': 'l2','loss': 'hinge','eta0':[0.0,1.0],\n",
    "            param_grid ={'alpha': [0.0001,0.1],\"l1_ratio\":[0.01,0.5],'epsilon':[0.1,1],'max_iter':[100,500]}\n",
    "        elif model_name == 'Ridge': # L2규제(feature 가중치(베타)최소화)\n",
    "            param_grid ={'alpha': [0.01, 0.1, 1, 10, 100, 500], 'solver':['auto','svd']}\n",
    "        elif model_name == 'Lasso': # L1규제 (feature 가중치 제거)\n",
    "            param_grid ={'alpha': [0.01, 0.1, 1, 10, 100, 500], 'max_iter' : [100, 1000]}\n",
    "        elif model_name == 'ElasticNet': # if L1 a, if L2 b  => alpha = a+b, l1_ratio = a/(a+b)  # if l1_ratio=1 => a=1, b=0\n",
    "            param_grid ={'alpha': [0.1, 1, 10, 100, 500], \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "        elif model_name == 'DecisionTreeRegressor':\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[5,8,10],'min_samples_leaf':[4,8,12,18],'min_samples_split':[2,8,16,50]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[8,13,20],'min_samples_leaf':[5,10,30,50,100 ],'min_samples_split':[5,10,30,50,100]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[5,10,20],'min_samples_leaf':[5,10,20,50],'min_samples_split':[5,10,20,50]}            \n",
    "            if (data_name=='project'):param_grid={'max_depth':[15,22,30],'min_samples_leaf':[100,200,300],'min_samples_split':[200,500,1000]}\n",
    "        elif model_name == 'RandomForestRegressor': # cpu 병렬지원(n_jobs=-1, 100% 가동)\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[5,8,10],'min_samples_leaf':[4,8,12,18 ],'min_samples_split':[2,8,16,20,50]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[8,13,20],'min_samples_leaf':[5,10,30,50,100],'min_samples_split':[5,10,30,50,100]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[5,10,20],'min_samples_leaf':[5,10,20,50],'min_samples_split':[5,10,20,50]}\n",
    "        elif model_name == 'GradientBoostingRegressor': # param많으면 학습시간이 많이 걸림(순차 시행, cpu 1개만 수행)\n",
    "            if (data_name=='bostonHouse'):param_grid= {'max_depth':[8],'min_samples_leaf':[8,36],'min_samples_split':[8,36]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[13],'min_samples_leaf':[10,50],'min_samples_split':[10,50]}\n",
    "            if (data_name=='AmesHouse'):param_grid= {'max_depth':[10], 'min_samples_leaf':[10,20],'min_samples_split':[10,20]}\n",
    "        elif model_name == 'XGBRegressor': #min_child_samples없음 # num_leaves 없음 # cpu 병렬지원(n_jobs=-1, auto) # 시간소요\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[5,8,10],'min_child_weight':[0.1,1,5],'subsample':[0.6,1]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[5,13,20],'min_child_weight':[0.1,1,5],'subsample':[0.6,1]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[5,10,20],'min_child_weight':[0.1,1,5],'subsample':[0.6,0.8,1]}\n",
    "        elif model_name == 'LGBMRegressor':  # cpu 병렬지원(n_jobs=-1, 100% 가동) # 매우빠름\n",
    "            if (data_name=='bostonHouse'):param_grid={'max_depth':[128,160],'min_child_samples':[20,60,100],'num_leaves':[32,64,128],'subsample':[0.6,0.8,1]}\n",
    "            if (data_name=='bike'):param_grid={'max_depth':[64,128,256,512],'min_child_samples':[20,60,100],'num_leaves':[32,64,128],'subsample':[0.6,0.8,1]}\n",
    "            if (data_name=='AmesHouse'):param_grid={'max_depth':[64,128,256],'min_child_samples':[20,60,100],'num_leaves':[32,64,128],'subsample':[0.6,0.8,1]}\n",
    "                \n",
    "        else:\n",
    "            param_grid = None\n",
    "        return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricPlot(model, model_name, data_name, time_name, start_model_time, X_train,y_train,X_test, y_test,is_expm1=False):\n",
    "# def metricPlot(model,model_name,data_name,time_name,start_model_time,X_train,y_train,X_test,y_test,is_expm1=True): #is_expm1 <= log변환시 복원\n",
    "    pred = model.predict(X_test)\n",
    "    if(est ==\"clf\"):\n",
    "        if model_name in ['LinearSVC']:\n",
    "            pred_proba = model._predict_proba_lr(X_test) \n",
    "        else:\n",
    "            pred_proba = model.predict_proba(X_test) \n",
    "    if is_expm1 : #is_expm1 <= log변환시 복원\n",
    "        y_test = np.expm1(y_test)\n",
    "        pred = np.expm1(pred)  \n",
    "    if (est == 'clf'):\n",
    "        metrics_dict = get_clf_eval(y_test, pred, pred_proba[:,1])\n",
    "    elif (est == 'reg'):\n",
    "        metrics_dict = get_reg_eval(y_test, pred)\n",
    "    spend_metric_time=(time.time() - start_model_time)\n",
    "    print(f'{model_name} metric 수행시간 : {int(spend_metric_time//60)}분 {spend_metric_time%60:.2f}초\\n')\n",
    "    if (plot == 1) :\n",
    "        Plot_learning_curve(data_name,model,model_name,time_name,X_train,y_train,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5))\n",
    "        precision_recall_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name )\n",
    "        roc_curve_plot(y_test, pred_proba[:,1], data_name, model_name, time_name)\n",
    "        if model_name in linear_model:\n",
    "            Plot_coef_logit(data_name, model, X_train.columns, model_name, time_name) # 회귀계수 \n",
    "        else : \n",
    "# [KNeighborsClassifier,DecisionTreeClassifier,RandomForestClassifier,GradientBoostingClassifier,XGBClassifier,LGBMClassifier]:\n",
    "            Plot_feature_importance(data_name, model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "        # permutation_importance\n",
    "        results = permutation_importance(model, X_test, y_test, scoring='f1')\n",
    "        importance = results.importances_mean\n",
    "        Plot_permutation_importance(data_name, importance, X_train.columns, model_name, time_name)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 학습/테스트 데이터 셋을 입력하면 학습 -> GridSerchCV 수행 + 성능 평가(metric) 등 반환\n",
    "# def get_model_train_eval(model,model_name,data_name,time_name,start_model_time,X_train,X_test,y_train,y_test,eval_X=None,eval_y=None,is_expm1=True):\n",
    "def get_model_train_eval(model,model_name,data_name,time_name,start_model_time,X_train,X_test,y_train,y_test):\n",
    "    print('###',model.__class__.__name__,'###')\n",
    "    linear_model = ['LogisticRegression','SGDClassifier','LinearSVC','LinearRegression','SGDRegressor','Ridge','Lasso','ElasticNet']\n",
    "    if (grid == 2) :  # RandomSearchCV\n",
    "        param_grid  = get_params(model_name, data_name)\n",
    "        print(f'RandomSearchCV 입력 파라미터:{param_grid}')\n",
    "        if (est == 'clf'): #  분류 scoring : accuracy, f1, neg_log_loss, roc_auc\n",
    "            model=RandomizedSearchCV(model,param_distributions=param_grid,scoring='roc_auc',return_train_score=True,n_iter=5)\n",
    "        elif (est =='reg'):\n",
    "            model=RandomizedSearchCV(model,param_distributions=param_grid,scoring='neg_mean_squared_error',return_train_score=True,n_iter=5)\n",
    "        model.fit(X_train, y_train) # 랜덤포레스트는 early_stopping_rounds없음 * xgb, lgbm, sgb만 있음\n",
    "#         model.fit(X_train, y_train,early_stopping_rounds=10,eval_metric=\"logloss\",eval_set=[(eval_X,eval_y)],verbose=True)\n",
    "        best_params = model.best_params_\n",
    "        print(f'RandomSearchCV 최적 파라미터:{ best_params}\\n')\n",
    "        # grid_curve plot \n",
    "#         plot_grid_curve(model, param_grid, data_name, model_name, time_name)\n",
    "        # best_estimator_\n",
    "        model = model.best_estimator_\n",
    "        metrics_dict = metricPlot(model,model_name,data_name,time_name,start_model_time,X_train,y_train,X_test,y_test)\n",
    "        return model, best_params, metrics_dict\n",
    "\n",
    "    elif (grid == 3) : # GridSearchCV\n",
    "        param_grid  = get_params(model_name, data_name)\n",
    "        print(f'GridSearchCV 입력 파라미터:{param_grid}')\n",
    "        if (est == 'clf'): #  분류 scoring : accuracy, f1, neg_log_loss, roc_auc\n",
    "            model=RandomizedSearchCV(model,param_distributions=param_grid,scoring='roc_auc',return_train_score=True,n_iter=5)\n",
    "        elif (est =='reg'):\n",
    "            model=RandomizedSearchCV(model,param_distributions=param_grid,scoring='neg_mean_squared_error',return_train_score=True,n_iter=5)\n",
    "        model = GridSearchCV(model, param_grid=param_grid, scoring='neg_mean_squared_error', return_train_score=True) # cv=2 최소, default 5\n",
    "#         gridcv = GridSearchCV(model, param_grid=param_grid, scoring=None, return_train_score=True)\n",
    "#         model.fit(X_train, y_train) # 랜덤포레스트는 early_stopping_rounds없음 * xgb, lgbm, sgb만 있음\n",
    "        model.fit(X_train, y_train,early_stopping_rounds=10,eval_metric=\"logloss\",eval_set=[(eval_X,eval_y)],verbose=True)\n",
    "        best_params = model.best_params_\n",
    "        print(f'GridSearchCV 최적 파라미터:{ best_params}')\n",
    "        if (plot ==1):\n",
    "            plot_grid_curve(model, param_grid, data_name, model_name, time_name)\n",
    "        # metric\n",
    "        model = model.best_estimator_\n",
    "        metrics_dict = metricPlot(model,model_name,data_name,time_name,start_model_time,X_train,y_train,X_test,y_test)\n",
    "        return model, param_grid, best_params, metrics_dict\n",
    "   \n",
    "    else: # grid = 1, 4\n",
    "        model.fit(X_train, y_train)\n",
    "    # early_stopping_rounds 기능은 xgboot LGBM ==> eval_metric: auc(bin only) \"logloss\":Negative lig-likehood multiclass \"mlogloss\",error,merror\n",
    "#     model.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=[(X_train, y_train), (X_test, y_test)], verbose=True)\n",
    "#     model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=[(eval_X, eval_y)], verbose=True)\n",
    "        ##########################################    \n",
    "        # model 예측(에러) 확률    # 평가지표(metrics)을 위한 negitive, positive 확률로 변환\n",
    "        metrics_dict = metricPlot(model,model_name,data_name,time_name,start_model_time,X_train,y_train,X_test,y_test)\n",
    "        return model, metrics_dict      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data preprocessing, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 데이터 가공\n",
    "def get_preprocessed_df(df=None):\n",
    "    ## \n",
    "    df_copy = df.copy()\n",
    "#     df_copy['target'] = df_copy['식전혈당(공복혈당)'].apply(lambda x: 1 if x >= 126 else ( 2 if 100 <= x < 126 else 3))\n",
    "#     df_copy['target'] = df_copy['식전혈당(공복혈당)'].apply(lambda x: 1 if x >= 126 else 0 )\n",
    "#     df_copy.drop('식전혈당(공복혈당)', axis=1, inplace=True)    \n",
    "#     df_copy['연령'] =df_copy['연령대코드(5세단위)'].apply( lambda x: x*5 + 17 )\n",
    "#     df_copy.drop('연령대코드(5세단위)', axis=1, inplace=True)\n",
    "#     target_count = df_copy[ df_copy['target'] == 1 ].shape[0]\n",
    "#     not_target_count = df_copy[ df_copy['target'] != 1 ].shape[0]\n",
    "#     frac=round( (target_count/not_target_count), 3)\n",
    "#     df_target = df_copy[ df_copy['target'] == 1 ]\n",
    "#     df_X =  df_copy[ df_copy['target'] != 1 ].sample(frac=frac, random_state=1)\n",
    "#     df_copy = pd.concat ([df_target, df_X]).sort_values(by=['기준년도'])\n",
    "#     df_copy.drop(['기준년도','시도코드'], axis=1, inplace=True)    \n",
    "#     df_copy = sklearn.utils.shuffle(df)\n",
    "#     print(df_copy[df_copy['target'] == 1].shape[0])\n",
    "#     print(df_copy[df_copy['target'] != 1].shape[0]) \n",
    "    ## 범주형 drop # 범주형 feature 10개\n",
    "#     category_features = ['성별코드','청력(좌)','청력(우)', '요단백', '흡연상태', '음주여부', '구강검진수검여부']\n",
    "#     df_copy.drop(category_features, axis=1, inplace=True)\n",
    "#     df_copy = pd.get_dummies(df_copy)\n",
    "    return df_copy\n",
    "\n",
    "# 선형회귀모델을 위한 scaler # 표준정규분포(Standard), 최대값/최소값(MinMax), 로그변환(Log)\n",
    "# p_degree는 다항식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. \n",
    "def get_scaled_data(input_data=None, scaler=None, p_degree=None):\n",
    "    if scaler == 'Standard':\n",
    "        scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    elif scaler == 'MinMax':\n",
    "        scaled_data = MinMaxScaler().fit_transform(input_data)\n",
    "    elif scaler == 'Log':\n",
    "        scaled_data = np.log1p(input_data)\n",
    "    else:\n",
    "        scaled_data = input_data\n",
    "    if p_degree != None:\n",
    "        \n",
    "        \n",
    "        \n",
    "    return scaled_data\n",
    "\n",
    "# 1.필요시 전처리 리턴 => 2. X,y(target)분리 => 3. 필요시 scaling => 4. train,test로 split\n",
    "def get_train_test_dataset(df=None):\n",
    "    ## 전체 df 전처리 필요시\n",
    "    df = get_preprocessed_df(df)\n",
    "    y_target = df['target']\n",
    "    X_features = df.drop(['target'], axis=1,inplace=False)\n",
    "\n",
    "    ## target과 train 데이터 분리\n",
    "    #bostonHouse\n",
    "#     y_target = df['PRICE']\n",
    "#     X_features = df.drop(['PRICE'], axis=1,inplace=False)\n",
    "    # bike\n",
    "#     y_target = df['count']\n",
    "#     X_features = df.drop(['count'],axis=1,inplace=False)\n",
    "    # Ameshouse\n",
    "#     y_target = house_df_ohe['SalePrice']\n",
    "#     X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)\n",
    "    \n",
    "    ## 선형회귀모델을 위한 scaler 필요시\n",
    "#     scaler='Standard'\n",
    "#     p_degree=None \n",
    "#     X_features = get_scaled_data(X_features, scaler=scaler, p_degree=p_degree) # p_degree는 다향식 특성을 추가할 때 적용\n",
    "#     y_target = np.log1p(y_target)\n",
    "    \n",
    "    ## train_test_split( )으로 학습과 테스트 데이터 분할. target 불균형시 stratify=y_target으로 Stratified 기반 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)\n",
    "    # xgb, lgbm early_stopping_rounds 사용시 ==> test 데이터 eval_set 으로 다시 분리  # train, test 분리 0.3 필요\n",
    "#     X_test, eval_X, y_test, eval_y = train_test_split(X_test, y_test, test_size=0.3, random_state=256, stratify=y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "#     return scaler, p_degree, X_train, X_test, y_train, y_test\n",
    "#     return scaler, p_degree, X_train, X_test, y_train, y_test, eval_X, eval_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. data 및 model selection, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMetricSave(model_list,X_train,X_test,y_train,y_test,eval_X=None,eval_y=None):\n",
    "    time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    for model in model_list:\n",
    "        start_model_time = time.time()\n",
    "        model_name = model.__class__.__name__\n",
    "        param_grid=None\n",
    "        best_params=None\n",
    "        if(grid == 1 or grid == 4): # GridSerchCV 없이 초기 2. ~ 3. model 훈련, metric 리턴받아 처리 \n",
    "            model, metrics_dict = \\\n",
    "                get_model_train_eval(model,model_name,data_name,time_name,start_model_time,X_train,X_test,y_train,y_test)\n",
    "        elif(grid == 2): # RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "            model, best_params, metrics_dict =\\\n",
    "                        get_model_train_eval(model,model_name,data_name,time_name,start_model_time,X_train,X_test,y_train,y_test)\n",
    "#                         get_model_train_eval(model,model_name,data_name,time_name,start_model_time,X_train,X_test,y_train,y_test,eval_X,eval_y)\n",
    "        elif(grid == 3): # GridSerchCV, RandomSearchCV 수행 ==> 2. data split ~ 3. model 훈련, metric 리턴 ####\n",
    "            model, param_grid, best_params, metrics_dict =\\\n",
    "                        get_model_train_eval(model,model_name,data_name,time_name,start_model_time,X_train,X_test,y_train,y_test)\n",
    "#                         get_model_train_eval(model,model_name,data_name,time_name,start_model_time,X_train,X_test,y_train,y_test,eval_X,eval_y)\n",
    "        ## metrics json 저장 \n",
    "        metrics_total_dict = {}\n",
    "        spend_model_time=(time.time() - start_model_time)\n",
    "        metrics_total_dict['Experiment_date_time'] = time_name\n",
    "        metrics_total_dict['data_name'] = data_name\n",
    "        metrics_total_dict['model_name'] = model_name\n",
    "        if(param_grid):        \n",
    "            metrics_total_dict['param_grid'] = param_grid \n",
    "        if(best_params):        \n",
    "            metrics_total_dict['best_params'] = best_params\n",
    "        metrics_total_dict['model_params'] = model.get_params()\n",
    "        metrics_total_dict['metrics'] =metrics_dict\n",
    "        metrics_total_dict['Execution_time'] = spend_model_time\n",
    "        with open(f'./json/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "            json.dump(metrics_total_dict, f, indent=4)            \n",
    "        print(f'{model_name} 수행시간 : {int(spend_model_time//60)}분 {spend_model_time%60:.2f}초\\n')  \n",
    "        if(best_params):\n",
    "            params = 'random_state=1'\n",
    "            for key, value in best_params.items():\n",
    "                params = params + ',' + str(key) + '=' + str(value)\n",
    "    #             print(f'{model_name} params : {params}\\n')\n",
    "            params_dict[f'{model_name}']=params\n",
    "    spend_time=(time.time() - start_time)\n",
    "    print(f'전체수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LogisticRegression ###\n",
      " 정확도: 0.6700, 정밀도: 0.6429, 재현율: 0.7347, f1_score: 0.6857, auc_score:0.7462\n",
      "LogisticRegression metric 수행시간 : 0분 3.64초\n",
      "\n",
      "LogisticRegression 수행시간 : 0분 3.64초\n",
      "\n",
      "### LinearSVC ###\n",
      " 정확도: 0.5200, 정밀도: 0.5052, 재현율: 1.0000, f1_score: 0.6712, auc_score:0.7501\n",
      "LinearSVC metric 수행시간 : 0분 0.09초\n",
      "\n",
      "LinearSVC 수행시간 : 0분 0.09초\n",
      "\n",
      "### SGDClassifier ###\n",
      " 정확도: 0.5300, 정밀도: 0.8333, 재현율: 0.0510, f1_score: 0.0962, auc_score:0.5206\n",
      "SGDClassifier metric 수행시간 : 0분 0.02초\n",
      "\n",
      "SGDClassifier 수행시간 : 0분 0.02초\n",
      "\n",
      "### DecisionTreeClassifier ###\n",
      " 정확도: 0.5750, 정밀도: 0.5657, 재현율: 0.5714, f1_score: 0.5685, auc_score:0.5749\n",
      "DecisionTreeClassifier metric 수행시간 : 0분 0.02초\n",
      "\n",
      "DecisionTreeClassifier 수행시간 : 0분 0.02초\n",
      "\n",
      "### RandomForestClassifier ###\n",
      " 정확도: 0.6850, 정밀도: 0.6636, 재현율: 0.7245, f1_score: 0.6927, auc_score:0.7501\n",
      "RandomForestClassifier metric 수행시간 : 0분 0.56초\n",
      "\n",
      "RandomForestClassifier 수행시간 : 0분 0.56초\n",
      "\n",
      "### GradientBoostingClassifier ###\n",
      " 정확도: 0.6500, 정밀도: 0.6296, 재현율: 0.6939, f1_score: 0.6602, auc_score:0.7260\n",
      "GradientBoostingClassifier metric 수행시간 : 0분 0.11초\n",
      "\n",
      "GradientBoostingClassifier 수행시간 : 0분 0.11초\n",
      "\n",
      "### XGBClassifier ###\n",
      " 정확도: 0.6900, 정밀도: 0.6667, 재현율: 0.7347, f1_score: 0.6990, auc_score:0.7666\n",
      "XGBClassifier metric 수행시간 : 0분 0.40초\n",
      "\n",
      "XGBClassifier 수행시간 : 0분 0.40초\n",
      "\n",
      "### LGBMClassifier ###\n",
      " 정확도: 0.7000, 정밀도: 0.6827, 재현율: 0.7245, f1_score: 0.7030, auc_score:0.7572\n",
      "LGBMClassifier metric 수행시간 : 0분 0.19초\n",
      "\n",
      "LGBMClassifier 수행시간 : 0분 0.19초\n",
      "\n",
      "전체수행시간 : 0분 6.92초\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 수행 시간 측정을 위함. 시작 시간 설정.\n",
    "start_time = time.time()\n",
    "\n",
    "# data_name = \"bostonHouse\"\n",
    "# data_name = \"bike\"\n",
    "# data_name = \"AmesHouse\"\n",
    "data_name = 'project'\n",
    "\n",
    "grid = 1      # 초기모델링    # 초기 model_list, # target scaler했다면 is_expm1=False, True 확인\n",
    "# grid = 2     # RandomSearchCV 수행    # params 조정\n",
    "# grid = 3     # GridSearchCV 수행     # params 조정\n",
    "# grid = 4    # best params 적용  # 해당 model_list\n",
    "\n",
    "plot = 0    # plot  disable 수행시간때문에\n",
    "plot = 1  # plot enable \n",
    "\n",
    "est = \"clf\" \n",
    "# est = \"reg\" \n",
    "\n",
    "def run():    \n",
    "    ## data 데이터 선택 ########################################\n",
    "#     df = dataset()\n",
    "#     df = bike_df\n",
    "#     df = house_df_ohe\n",
    "    # project\n",
    "#     df = pd.read_csv('./data/NHIS_total_model.csv')\n",
    "    df = pd.read_csv('./data/NHIS_model_1.csv')\n",
    "#     df = pd.read_csv('./data/NHIS_model_reg_1.csv')\n",
    "    df = df.iloc[:1000, :]\n",
    "    # clf\n",
    "#     target_count = df[df['target'] == 1].shape[0]\n",
    "#     target_ex_count = df[df['target'] != 1].shape[0]\n",
    "#     print(f\"target   count : {target_count} ({ round( target_count/df.shape[0], 4)*100 })%\")\n",
    "#     print(f\"target외 count : {target_ex_count} ({ round( target_ex_count/df.shape[0], 4)*100 })%\")\n",
    "\n",
    "    ## 1. 데이터 전처리(preprocessing), 분리(split) ##########################################\n",
    "    X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n",
    "#     scaler, p_degree, X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n",
    "#     X_train, X_test, y_train, y_test, eval_X, eval_y = get_train_test_dataset(df)\n",
    "    \n",
    "    ## model 선택 ##################################### 'squared_loss’, 'huber’,'epsilon_insensitive’,squared_epsilon_insensitive’\n",
    "    if (est=='clf'):\n",
    "        model_list= [LogisticRegression(random_state=1, n_jobs=-1),\n",
    "                 LinearSVC(random_state=1, loss='hinge'),\n",
    "                 SGDClassifier(random_state=1,n_jobs=-1, loss='modified_huber'), \n",
    "                 DecisionTreeClassifier(random_state=1),\n",
    "                 RandomForestClassifier(random_state=1,n_jobs=-1),\n",
    "        GradientBoostingClassifier(random_state=1,validation_fraction=0.1,n_iter_no_change=20,subsample=0.25), # xgb,lgbm:fit조기중단가능\n",
    "                 XGBClassifier(random_state=1,n_jobs=-1,silent=True,device='gpu'),\n",
    "                 LGBMClassifier(random_state=1,n_jobs=-1) ]\n",
    "    elif (est=='reg'):\n",
    "        model_list = [LinearRegression(n_jobs=-1), \n",
    "                      LinearSVR(random_state=1,verbose=0), # 수행시간 느림 metric 65분\n",
    "    #                   SGDRegressor(random_state=1),\n",
    "                      Ridge(random_state=1), Lasso(random_state=1), ElasticNet(random_state=1),\n",
    "                      DecisionTreeRegressor(random_state=1), RandomForestRegressor(random_state=1,n_jobs=-1),\n",
    "                      GradientBoostingRegressor(random_state=1), # 수행시간 느림 metric 56분\n",
    "                      XGBRegressor(random_state=1, n_jobs=-1, silent=True),\n",
    "                      LGBMRegressor(random_state=1, n_jobs=-1) ] \n",
    "    \n",
    "#     model_list= [\n",
    "#         LinearRegression(n_jobs=-1,fit_intercept=True), # random_state 없음\n",
    "#         LinearSVR(random_state=1, C=1,max_iter=1000),\n",
    "#         Ridge(random_state=1, alpha=10,solver='auto'),\n",
    "#         Lasso(random_state=1, alpha=100,max_iter=1000),\n",
    "#         ElasticNet(random_state=1, alpha=0.1,l1_ratio=0.9),\n",
    "#         DecisionTreeRegressor(random_state=1, max_depth=10,min_samples_leaf=10,min_samples_split=5),\n",
    "#         RandomForestRegressor(n_jobs=-1, random_state=1, max_depth=20,min_samples_leaf=5,min_samples_split=5),\n",
    "#         GradientBoostingRegressor(random_state=1, max_depth=10,min_samples_leaf=20,min_samples_split=10),\n",
    "#         XGBRegressor(n_jobs=-1, random_state=1, silent=True, max_depth=5,min_child_weight=0.1,subsample=0.8),\n",
    "#         LGBMRegressor( n_jobs=-1,random_state=1,max_depth=64,min_child_samples=20,num_leaves=64,subsample=0.6)\n",
    "#     ] \n",
    "\n",
    "    ##  2. ~ 3. 훈련, metric  ###################################################\n",
    "    trainMetricSave(model_list, X_train, X_test, y_train, y_test, eval_X=None, eval_y=None)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list= [\n",
    "    LinearRegression(n_jobs=-1,fit_intercept=True), # random_state 없음\n",
    "    LinearSVR(random_state=1, C=1,max_iter=1000),\n",
    "    SGDRegressor(random_state=1),\n",
    "    Ridge(random_state=1, alpha=10,solver='auto'),\n",
    "    Lasso(random_state=1, alpha=100,max_iter=1000),\n",
    "    ElasticNet(random_state=1, alpha=0.1,l1_ratio=0.9),\n",
    "    DecisionTreeRegressor(random_state=1, max_depth=10,min_samples_leaf=10,min_samples_split=5),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=1, max_depth=20,min_samples_leaf=5,min_samples_split=5),\n",
    "    GradientBoostingRegressor(random_state=1, max_depth=10,min_samples_leaf=20,min_samples_split=10),\n",
    "    XGBRegressor(n_jobs=-1, random_state=1, silent=True, max_depth=5,min_child_weight=0.1,subsample=0.8),\n",
    "    LGBMRegressor( n_jobs=-1,random_state=1,max_depth=64,min_child_samples=20,num_leaves=64,subsample=0.6)\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개별 model별 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# data_name = \"bostonHouse\"\n",
    "# data_name = \"bike\"\n",
    "# data_name = \"project\"\n",
    "data_name = \"project\"\n",
    "\n",
    "grid = 1      # 초기모델링    # 초기 model_list, # target scaler했다면 is_expm1=False, True 확인\n",
    "# grid = 2     # RandomSearchCV 수행    # params 조정\n",
    "# grid = 3     # GridSearchCV 수행     # params 조정\n",
    "# grid = 4    # best params 적용  # 해당 model_list\n",
    "\n",
    "plot = 0    # plot  disable 수행시간때문에\n",
    "# plot = 1  # plot enable \n",
    "\n",
    "# est = \"clf\" \n",
    "est = \"reg\" \n",
    "\n",
    "# plot = 1  # plot enable \n",
    "# df = dataset()\n",
    "df = bike_df\n",
    "\n",
    "## project\n",
    "# df = pd.read_csv('data/NHIS_total_model.csv')\n",
    "# df = df.iloc[:10000, :]\n",
    "\n",
    "## 아래 params에서 n_jobs=-1이 있는 곳은 그대로 추가 # xgb는 silent=True까지 추가\n",
    "# model = LinearRegression(n_jobs=-1,fit_intercept=True)\n",
    "model = LinearSVR(random_state=1, C=1,max_iter=100)\n",
    "# model = SGDRegressor(random_state=1)\n",
    "# model = Ridge(random_state=1, alpha=10,solver='auto')\n",
    "# model = Lasso(random_state=1, alpha=0.1,max_iter=100)\n",
    "# model = ElasticNet(random_state=1, alpha=0.1,l1_ratio=0.9)\n",
    "\n",
    "# model = DecisionTreeRegressor(random_state=1, max_depth=13,min_samples_leaf=5,min_samples_split=5)\n",
    "# model = RandomForestRegressor(n_jobs=-1, random_state=1, max_depth=20,min_samples_leaf=5,min_samples_split=5)\n",
    "# model = GradientBoostingRegressor(random_state=1, max_depth=13,min_samples_leaf=10,min_samples_split=50)\n",
    "# model = XGBRegressor(n_jobs=-1, random_state=1, silent=True, max_depth=20,min_child_weight=7,subsample=0.6)\n",
    "# model = LGBMRegressor( n_jobs=-1,random_state=1,max_depth=64,min_child_samples=20,num_leaves=128,subsample=0.6)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n",
    "\n",
    "time_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = model.__class__.__name__\n",
    "model, metrics_dict = \\\n",
    "            get_model_train_eval(model, model_name,data_name,time_name, X_train, X_test, y_train, y_test)\n",
    "# model, metrics_dict = get_model_train_eval(model, X_train, X_test, y_train, y_test,eval_X, eval_y)\n",
    "\n",
    "## model 저장\n",
    "# pickle.dump(model, open(f'./modeling/model_{model_name}_{time_name}.sav', 'wb'))\n",
    "\n",
    "## plot : learning_curve, 회귀 ==> coef_, 회귀트리는 => feature_importance_\n",
    "Plot_learning_curve(data_name, model, model_name, time_name,  X_train, y_train, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "Plot_coef_(data_name, model, X_train.columns, model_name, time_name) # 회귀계수  # regressionTree x\n",
    "# Plot_feature_importance(data_name, model, X_train.columns, model_name, time_name) # 회귀트리 # lr x\n",
    "# Plot_tree_(data_name, model, model_name,time_name) # 수행시간이 많이 걸림\n",
    "\n",
    "## metrics json 저장 \n",
    "metrics_total_dict = {}\n",
    "spend_time=(time.time() - start_time)\n",
    "metrics_total_dict['data_name'] = data_name\n",
    "metrics_total_dict['model_name'] = model_name\n",
    "model_params = model.get_params() # dict\n",
    "metrics_total_dict['model_params'] = model_params\n",
    "# metrics_total_dict['scaler'] = scaler\n",
    "# metrics_total_dict['p_degree'] = p_degree\n",
    "metrics_total_dict['metrics'] =metrics_dict\n",
    "metrics_total_dict['Execution_time'] = spend_time\n",
    "with open(f'./json/{data_name}_{model_name}_{time_name}.json', 'w', encoding='UTF-8-sig') as f:\n",
    "    json.dump(metrics_total_dict, f, indent=4)            \n",
    "print(f'수행시간 : {int(spend_time//60)}분 {spend_time%60:.2f}초')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0. data_name, df 선택 ==> target 분리, 전처리영역필요여부\n",
    "1. 초기모델은 grid = 1 선택, 전체 model_list for문, 과적합 learning_curve plot 확인\n",
    "    model_list,  Plot learning_curve\n",
    "    \n",
    "2. GridSearchCV 사용 grid = 2 선택, 전체 model_list for문, parameter별 영향력 확인 grid_curve\n",
    "    model_list,  plot grid_curve\n",
    "    ==> 1항에서 parameter tunning 반복가능\n",
    "    \n",
    "3. grid = 3 best param model 선택, 과적합 learning_curve plot 확인\n",
    "    model_list, Plot learning_curve\n",
    "    \n",
    "4. model과 최적 param 찾았다면, model\n",
    "    model, Plot (coeff_, feature_importance_ )\n",
    "    \n",
    "5. feature engineering : scaler, p_degree(다항식), \n",
    "    scaler log1p 변환시 is_expm1=True\n",
    "    1. 3.항 실행\n",
    "6. xgb, lgbm의 early기능 사용시 eval_X, eval_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./static/json/{model_name}_{time_name}.json', 'r', encoding='UTF-8-sig') as json_file:\n",
    "    #     data = json.load(json_file)\n",
    "    #     print(data)\n",
    "def dictParse(data):\n",
    "    time_name = data['Experiment_date_time']\n",
    "    data_name= data['data_name']\n",
    "    model_name= data['model_name']\n",
    "    model_params = data['model_params']\n",
    "    if 'scaler' in data.keys():\n",
    "        scaler = data['scaler']\n",
    "    if 'p_degree' in data.keys():\n",
    "        p_degree = data['p_degree']\n",
    "    mae_v = data['metrics']['MAE']\n",
    "    mse_v = data['metrics']['MSE']\n",
    "    rmse_v = data['metrics']['RMSE']\n",
    "    rmsle_v = data['metrics']['RMSLE']\n",
    "    Execution_time = data['Execution_time'] \n",
    "    return data_name, model_name, model_params, rmsle_v, rmse_v, mse_v, Execution_time\n",
    "metrics_total_dict = run()\n",
    "data_name, model_name, model_params, rmsle_v, rmse_v, mse_v, Execution_time = dictParse(metrics_total_dict)\n",
    "# model_n, scaler, p_degree, rmsle_v, rmse_v, mse_v, Execution_time = jsonParse(metrics_total_dict)\n",
    "print(f'Experiment_date_time:{time_name}\\n data_name : {data_name}\\n model_name : {model_name}\\n model_params : {model_params}\\n MAE {mae_v}\\n MSE {mse_v}\\n RMSE {rmse_v}\\n  RMSLE {rmsle_v}\\n Execution_time {Execution_time}')\n",
    "# print(f'model name : {model_n}\\n model_params : {model_parma}\\n scaler {scaler}\\n p_degree {p_degree}\\n RMSLE {rmsle_v}\\n RMSE {rmse_v}\\n MAE {mae_v}\\n Execution_time {Execution_time}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.datasets import load_breast_cancer # xgb 유방암, voting사용\n",
    "    from sklearn.datasets import load_diabetes  # pima아니고 \n",
    "    from sklearn.datasets import load_boston # 회귀\n",
    "   \n",
    "    # 보스턴 데이터 세트 로드\n",
    "    boston = load_boston()\n",
    "    df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "    df['PRICE'] = boston.target\n",
    "#     print(df.info()) # 506\n",
    "    return df\n",
    "# dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = pd.read_csv('data/bike_train.csv')\n",
    "# 문자열을 datetime 타입으로 변경. \n",
    "bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)\n",
    "# datetime 타입에서 년, 월, 일, 시간 추출\n",
    "bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)\n",
    "bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)\n",
    "bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)\n",
    "bike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour)\n",
    "drop_columns = ['datetime','casual','registered'] # registered와 count 거의 동일해서\n",
    "bike_df.drop(drop_columns, axis=1,inplace=True)\n",
    "# print(bike_df.info()) # 10886\n",
    "# df = bike_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AmesHouse\n",
    "house_df = pd.read_csv('data/house_price.csv')\n",
    "# Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제\n",
    "house_df.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu'], axis=1 , inplace=True)\n",
    "# Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체\n",
    "house_df.fillna(house_df.mean(),inplace=True)\n",
    "null_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0]\n",
    "# print('## Null 피처의 Type :\\n', house_df.dtypes[null_column_count.index])\n",
    "# 문자형(범주형) feature one-hot 인코딩으로 shape 변환됨 (1460, 75) ==> (1460, 271) \n",
    "house_df_ohe = pd.get_dummies(house_df)\n",
    "null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]\n",
    "# print('## Null 피처의 Type :\\n', house_df_ohe.dtypes[null_column_count.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AmesHouse scaler2 ==> X_features skew큰것 log1p변환\n",
    "from scipy.stats import skew\n",
    "# object가 아닌 숫자형 피쳐의 컬럼 index 객체 추출.\n",
    "features_index = house_df.dtypes[house_df.dtypes != 'object'].index\n",
    "# house_df에 컬럼 index를 [ ]로 입력하면 해당하는 컬럼 데이터 셋 반환. apply lambda로 skew( )호출 \n",
    "skew_features = house_df[features_index].apply(lambda x : skew(x))\n",
    "# skew 정도가 1 이상인 컬럼들만 추출. \n",
    "skew_features_top = skew_features[skew_features > 1]\n",
    "# print(skew_features_top.sort_values(ascending=False))\n",
    "house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index])\n",
    "house_df_ohe = pd.get_dummies(house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset(df=None):\n",
    "    ## 전체 df 전처리 필요시\n",
    "    df = get_preprocessed_df(df)\n",
    "\n",
    "    ## target과 train 데이터 분리\n",
    "    ## clf\n",
    "    # pima\n",
    "#     y_target = df['Outcome']\n",
    "#     X_features = df.drop('Outcome', axis=1)\n",
    "    # customer(bank)\n",
    "#     y_target = df['TARGET']\n",
    "#     X_features = df.drop('TARGET', axis=1)    \n",
    "\n",
    "    # credit\n",
    "#     y_target = df['Class']\n",
    "#     X_features = df.drop('Class', axis=1)    \n",
    "#     scaler = StandardScaler()\n",
    "#     amount_n = scaler.fit_transform(X_features['Amount'].values.reshape(-1, 1))\n",
    "#     amount_n = np.log1p(X_features['Amount'])\n",
    "#     X_features.insert(0, 'Amount_Scaled', amount_n)\n",
    "#     X_features.drop(['Amount'], axis=1, inplace=True)\n",
    "\n",
    "    ## reg\n",
    "    #bostonHouse\n",
    "#     y_target = df['PRICE']\n",
    "#     X_features = df.drop(['PRICE'], axis=1,inplace=False)\n",
    "    # bike\n",
    "#     y_target = df['count']\n",
    "#     X_features = df.drop(['count'],axis=1,inplace=False)\n",
    "    # Ameshouse\n",
    "#     y_target = house_df_ohe['SalePrice']\n",
    "#     X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metric 함수 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score이용한  5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시  RMSE 구함. \n",
    "def get_model_cv_prediction(model, X_data, y_target):\n",
    "    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "    rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print('##### ',model.__class__.__name__ , ' #####')\n",
    "    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_error_data(y_test, pred, n_tops = 5):\n",
    "    # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. \n",
    "    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\n",
    "    result_df['predicted_count']= np.round(pred)\n",
    "    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])\n",
    "    # 예측값과 실제값이 가장 큰 데이터 순으로 출력. \n",
    "    print(result_df.sort_values('diff', ascending=False)[:n_tops])\n",
    "    \n",
    "# get_top_error_data(y_test,pred,n_tops=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
